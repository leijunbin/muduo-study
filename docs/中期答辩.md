# 毕业设计中期答辩

## 核心架构设计

### 历史发展

如果要让服务器服务多个客户端，那么最直接的方式就是为每一条连接创建线程。

处理完业务逻辑后，随着连接关闭后线程也同样要销毁了，但是这样不停地创建和销毁线程，不仅会带来性能开销，也会造成浪费资源，而且如果要连接几万条连接，创建几万个线程去应对也是不现实的。

我们可以使用资源复用的方式。也就是不用再为每个连接创建线程，而是创建一个线程池，将连接分配给线程，然后一个线程可以处理多个连接的业务。

不过，这样又引来一个新的问题，线程怎样才能高效地处理多个连接的业务？

当一个连接对应一个线程时，线程一般采用 read -> 业务处理 -> send 的处理流程，如果当前连接没有数据可读，那么线程会阻塞在 read 操作上（socket 默认情况是阻塞 I/O），不过这种阻塞方式并不影响其他线程。

但是引入了线程池，那么一个线程要处理多个连接的业务，线程在处理某个连接的 read 操作时，如果遇到没有数据可读，就会发生阻塞，那么线程就没办法继续处理其他连接的业务。

要解决这一个问题，最简单的方式就是将 socket 改成非阻塞，然后线程不断地轮询调用 read 操作来判断是否有数据，这种方式虽然该能够解决阻塞的问题，但是解决的方式比较粗暴，因为轮询是要消耗 CPU 的，而且随着一个线程处理的连接越多，轮询的效率就会越低。

上面的问题在于，线程并不知道当前连接是否有数据可读，从而需要每次通过 read 去试探。

那有没有办法在只有当连接上有数据的时候，线程才去发起读请求呢？答案是有的，实现这一技术的就是 I/O 多路复用。

I/O 多路复用技术会用一个系统调用函数来监听我们所有关心的连接，也就说可以在一个监控线程里面监控很多的连接。

![](https://picx.zhimg.com/80/v2-0a86ab90d8167860dec5c695064648f3_720w.webp?source=1940ef5c)

我们熟悉的 select/poll/epoll 就是内核提供给用户态的多路复用系统调用，线程可以通过一个系统调用函数从内核中获取多个事件。

在获取事件时，先把我们要关心的连接传给内核，再由内核检测：

* 如果没有事件发生，线程只需阻塞在这个系统调用，而无需像前面的线程池方案那样轮训调用 read 操作来判断是否有数据。
* 如果有事件发生，内核会返回产生了事件的连接，线程就会从阻塞状态返回，然后在用户态中再处理这些连接对应的业务即可。

#### I/O 复用系统调用 —— select 和 poll

在使用的时候，首先需要把关注的 Socket 集合通过 select/poll 系统调用从用户态拷贝到内核态，然后由内核检测事件，当有网络事件产生时，内核需要遍历进程关注 Socket 集合，找到对应的 Socket，并设置其状态为可读/可写，然后把整个 Socket 集合从内核态拷贝到用户态，用户态还要继续遍历整个 Socket 集合找到可读/可写的 Socket，然后对其处理。

很明显发现，select 和 poll 的缺陷在于，当客户端越多，也就是 Socket 集合越大，Socket 集合的遍历和拷贝会带来很大的开销。

#### I/O 复用系统调用 —— epoll

- epoll 在内核里使用「红黑树」来关注进程所有待检测的 Socket，红黑树是个高效的数据结构，增删查一般时间复杂度是 O(logn)，通过对这棵黑红树的管理，不需要像 select/poll 在每次操作时都传入整个 Socket 集合，减少了内核和用户空间大量的数据拷贝和内存分配。
- epoll 使用事件驱动的机制，内核里维护了一个「链表」来记录就绪事件，只将有事件发生的 Socket 集合传递给应用程序，不需要像 select/poll 那样轮询扫描整个集合（包含有和无事件的 Socket ），大大提高了检测的效率。

+ epoll 支持边缘触发和水平触发的方式，而 select/poll 只支持水平触发。

综合考虑程序效率和开发成本，最后选用 epoll 系统调用来实现 I/O 多路复用。

### 事件驱动模型

从上面的历史沿革可以看出，不管是接受客户端连接还是处理客户端事件，都是围绕 I/O 复用的系统调用来编程，可以说其是整个服务器系统的核心，服务器做的事情就是监听 I/O 复用的系统调用上的事件，然后对不同事件类型进行不同的处理。这种以事件为核心的模式又叫事件驱动，事实上几乎所有的现代服务器都是事件驱动的。而其中就有两种经典的事件驱动模式—— Reactor 和 Proactor 模式。

#### reactor

![](https://pic1.zhimg.com/80/v2-4da008d8b7f55a0c18bef0e87c5c5bb1_720w.webp?source=1940ef5c)

#### proactor

![](https://picx.zhimg.com/80/v2-35bd4bdf3b12246fb005415d3a29ecc0_720w.webp?source=1940ef5c)

无论是 Reactor，还是 Proactor，都是一种基于事件分发的网络编程模式，区别在于  **Reactor 模式是基于待完成的 I/O 事件，而 Proactor 模式则是基于已完成的 I/O 事件** 。

但是在 Linux 下的异步 I/O 是不完善的， aio 系列函数是由 POSIX 定义的异步操作接口，不是真正的操作系统级别支持的，而是在用户空间模拟出来的异步，并且仅仅支持基于本地文件的 aio 异步操作，网络编程中的 socket 是不支持的（Linux 5.1 实验支持异步 I/O），所以最终综合实现成本和兼容性等方面的考量，本次毕业设计使用 Reactor 方案。

## 日志系统

### 异步日志机制

异步日志，用一个线程负责收集日志消息，并写入日志文件。其他业务线程只管往这个日志线程发送日志消息。

整个框架如图所示，该框架有一个日志缓冲队列来将日志前端的数据发送到后端（日志线程），这是一个典型的生产者与消费者模型。生产者前端不是将日志消息逐条分别传送给后端，而是将多条日志信息缓存拼成1个大的 buffer 发送给后端。同时，对于消费者后端线程来说，并不是每条日志消息写入都将其唤醒，而是当前端写满1个 buffer 的时候，才唤醒后端日志线程批量写入磁盘文件。

![img](https://pic4.zhimg.com/80/v2-10762f49ca12648e287023fd60678d27_720w.webp)

因此，后端日志线程的唤醒有两个条件

- buffer 写满唤醒：批量写入写满1个 buffer 后，唤醒后端日志线程，减少线程被唤醒的频率，降低系统开销。
- 超时被唤醒：为了及时将日志消息写入文件，防止系统故障导致内存中的日志消息丢失，超过规定的时间阈值，即使 buffer 未满，也会立即将 buffer 中的数据写入。

### 双缓冲机制

muduo 库采用的是双缓冲机制（Google C++日志库也是如此）。其基本思路是准备两个缓冲区，bufferA 和 bufferB。前端负责向 A 中写入日志消息，后端负责将 B 中的数据写入文件。当 bufferA 写满后，交换 A 和 B，此时让后端将 A 的数据写入文件，前端向 B 中写入新的日志消息，如此往复。这样在追加日志消息的时候不必等待磁盘 IO 操作，同时也避免了每条新日志消息都触发唤醒后端日志线程。

![img](https://pic2.zhimg.com/80/v2-b12d975c6a31ac1fe309077f8664fa9d_720w.webp)

缓冲区 A 和 B，实际上采用的是缓冲区队列，分别对应前台日志缓冲队列 buffers 和后台日志缓冲队列 buffersToWrite。交换 A 和 B 时，只需要交换其指针的指向即可。

另外，采用队列可以提升缓冲区的容错性。考虑下面这种情况：前端写满 buffer，触发交换，后端还未将数据完全写入磁盘，此时前端又写满交换后的 buffer 了，再次触发交换机制，无 buffer 可用，造成阻塞。

### 前端日志写入

AsyncLogging::append()：前端生成一条日志消息。

前端准备一个前台缓冲区队列 buffers_和两个 buffer。前台缓冲队列 buffers_ 用来存放积攒的日志消息。两个 buffer，一个是当前缓冲区 currentBuffer，追加的日志消息存放于此；另一个作为当前缓冲区的备份，即预备缓冲区 nextBuffer，减少内存的开销。

函数执行逻辑如下：

判断当前缓冲区 currentBuffer_ 是否已经写满

- 若当前缓冲区未满，追加日志消息到当前缓冲，这是最常见的情况
- 若当前缓冲区写满，首先，把它移入前台缓冲队列 `buffers_`。其次，尝试把预备缓冲区 `nextBuffer_`移用为当前缓冲，若失败则创建新的缓冲区作为当前缓冲。最后，追加日志消息并唤醒后端日志线程开始写入日志数据。

### 后端日志落盘

AsyncLogging::threadFunc()：后端日志落盘线程的执行函数。

后端同样也准备了一个后台缓冲区队列 buffersToWrite 和两个备用 buffer。后台缓冲区队列 buffersToWrite 存放待写入磁盘的数据。两个备用 buffer，newBuffer1 和 newBuffer2，分别用来替换前台的当前缓冲和预备缓冲，而这两个备用 buffer 最后会被buffersToWrite  内的两个 buffer 重新填充，减少了内存的开销。

函数执行逻辑如下，注意思考如何锁的粒度如何减小，起到了什么作用。

- 唤醒日志落盘线程（超时或写满buffer），交换前台缓冲队列和后台缓冲队列。加锁
- 日志落盘，将后台缓冲队列的所有 buffer 写入文件。不加锁，这样做的好处是日志落盘不影响前台缓冲队列的插入，不会出现阻塞问题，极大提升了系统性能。

接下来，用图表示前端和后端的具体交互情况，注意配合源码认真分析。

一开始，分配好四个缓冲区，前端和后端各持有其中的两个。同时，前端和后端各有一个缓冲队列，初始时都是空的。结合源码分析以下过程：

case 1: 超时唤醒后端线程将当前缓冲区写入文件，此时前端写日志的频率不高。

![img](https://pic2.zhimg.com/80/v2-fa5d82320ff077979b9a17f3246031a5_720w.webp)

超时唤醒后端线程，先把 currentBuffer_ 送入 buffers_，再把 newbuffer1 移用为 currentBuffer_。随后，交换 buffers_ 和 buffersToWrite。离开临界区，后端开始将 buffersToWrite 中的 bufferA 写入文件。写完后（write done）再重新填充  newbuffer1，等待下一次 cond_.waitForSeconds() 返回。

case 2：超时前写满当前缓冲唤醒后端线程写入文件

![img](https://pic4.zhimg.com/80/v2-630ae9e51d985ccdff985a4f81210403_720w.webp)

写满 currentBuffer_ 唤醒后端线程，把 currentBuffer_ 送入 buffers_，再把 newbuffer1 移用为 currentBuffer_。随后，交换 buffers_ 和  buffersToWrite，最后用 newbuffer2 替换 nextBuffer_，始终保证前端有两个空缓冲可用。离开临界区，后端开始将 buffersToWrite 中的 bufferA 和 bufferB 写入文件。写完后再重新填充 newbuffer1和 newbuffer2，等待下一次 cond_.waitForSeconds() 返回。

上述这两种情况是最常见的。

case3: 用完了两个缓冲，需要重新分配新 buffer。可能原因：前端短时间内密集写入日志，或者后端文件写入速度较慢，导致前端耗尽了两个缓冲，并分配了新缓冲。

![img](https://pic3.zhimg.com/80/v2-66fab597a2efd22f505969711dba58c2_720w.webp)

写满 currentBuffer_ 唤醒后端线程，但出于种种原因，后端线程并没有立刻开始工作，接下来预备缓冲 nextBuffer_ 也写满了，前端线程新分配了缓冲区 E。当后端线程终于获取控制权后，将缓冲 C、D 交给前端，并开始将缓冲 A, B, E 依次写入文件。写完后再用缓冲 A、B 重新填充两块空闲缓冲，从而释放了缓冲 E。

### 高性能原因总结

如何实现高性能的日志

- 批量写入：攒够数据，一次写入。
- 唤醒机制：通知唤醒 notify + 超时唤醒 wait_timeout。
- 锁的粒度：刷新磁盘时，日志接口不会阻塞。这是通过双队列实现的，前台队列实现日志接口，后台队列实现刷新磁盘。

## C++ 工程化

### RAII 思想

RAII 是 C++ 的发明者 Bjarne Stroustrup 提出的概念，RAII 全称是“Resource Acquisition is Initialization”，直译过来是“资源获取即初始化”，也就是说在构造函数中申请分配资源，在析构函数中释放资源。因为C++的语言机制保证了，当一个对象创建的时候，自动调用构造函数，当对象超出作用域的时候会自动调用析构函数。所以，在RAII的指导下，我们应该使用类来管理资源，将资源和对象的生命周期绑定。

### CMake 编译

我们应当使用CMake来管理我们的项目，CMake的使用非常简单、功能强大，会帮我们自动生成Makefile文件，使项目的编译链接更加容易，程序员可以将更多的精力放在写代码上。

### 代码风格

作为一个大型C++项目，可能有许多程序员共同开发，每个人的编码习惯风格都不同，整个项目可能风格杂乱，可读性差，不利于项目维护。所以在写C++代码时应该遵守一些约定，使代码的风格统一。目前比较流行的C++代码风格有google、llvm等，本项目采用google风格。

### cpplint

基于google C++编码规范的静态代码分析工具，可以查找代码中错误、违反约定、建议修改的地方。

### clang-tidy

clang编译器的代码分析工具，功能十分强大。既可以查找代码中的各种静态错误，还可以提示可能会在运行时发生的问题。不仅如此，还可以通过代码分析给出可以提升程序性能的建议。

### 单元测试

通过对软件中的最小可测试单元进行检查和验证，以提升代码稳定性和质量。

## 成果展示—— echo 服务器演示

https://116.205.243.116:8086/

```bash
curl --location --request POST -d '123' 'localhost:8086/post'
curl --location --request POST -d '{"uid":"123"}' 'localhost:8086/post' 
curl --location --request POST -d '{ "social": [ { "weibo": "https://weibo.com/leiqikui" }, { "github": "https://github.com/leiqikui" } ] }' 'localhost:8086/post'
```

## 未来工作

+ 定时器
+ http 支持
+ 剩余部分的单元测试
+ 性能测试
+ 华为云部署
+ 对应文档的写作和毕业论文的写作
