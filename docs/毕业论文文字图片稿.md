# 毕业论文文字图片稿

## 摘要和关键词

### 中文

随着近十年来互联网用户的高速增长，互联网领域对服务器软件这样的软件基础设施提出了更高的性能和稳定性要求，同时 Linux 凭借其高度的可定制性和可拓展性迅速占领了服务器操作系统市场。基于这样的背景，本文设计并实现了一个 Linux 下的高性能网络服务器软件。

本文首先介绍了 Linux 下高性能网络服务器的课题背景并介绍了实现所需的相关技术，包括了事件驱动模式、多路 I/O 复用、线程池等。

随后基于以上技术和背景，本文分析了网络服务器的需求并设计了软件的整体架构，介绍了各个架构模块之间的功能分工，并对网络请求、异步日志、定时器三个主要处理流程进行了详尽的阐述；随后介绍了各个架构模块的内部设计与实现，阐述了重要成员函数的功能。

本文对网络服务器软件设计了详细的单元测试用例和集成测试流程，并提供了相关演示截图，验证了系统功能达到了设计的要求；同时设计了性能测试实验流程，与开源网络服务器软件 muduo 进行了对比，证明了本网络服务器软件在中低负载下拥有与 muduo 相当的性能，但在高负载下略逊于 muduo，结合 strace 工具统计的系统调用使用情况，尝试分析了高负载情况下的性能瓶颈和可能的优化方向。

本文最后对全文做出了总结，描述了软件存在的不足和下一步的工作。

### 英文

With the rapid growth of internet users over the past decade, the internet industry has raised higher demands for software infrastructure, such as high performance and stability of server software. Linux has quickly dominated the server operating system market due to its high customizability and scalability. Against this background, this paper designs and implements a high-performance network server software on Linux.

Firstly, this paper introduces the background of high-performance network servers on Linux and the related technologies required for implementation, including event-driven model, multiplexing I/O, and thread pool.

Subsequently, based on the above technologies and background, this paper analyzes the requirements of network servers and designs the overall architecture of the software, introducing the functional division of each architecture module and providing a detailed explanation of the three main processing flows: network requests, asynchronous logging, and timers. The internal design and implementation of each architecture module are then introduced, and the functions of important member functions are explained.

This paper designs detailed unit test cases and integration test processes for network server software, provides relevant demonstration screenshots, and verifies that the system's functionality meets the design requirements. Meanwhile, a performance testing experimental process is designed, and a comparison is made with the open-source network server software muduo. The results demonstrate that the performance of this network server software is equivalent to muduo under medium and low loads, but slightly inferior to muduo under high loads. Furthermore, combined with the system call usage statistics from the strace tool, this paper attempts to analyze the performance bottlenecks and possible optimization directions under high loads.

Finally, this paper summarizes the entire article, describes the software's shortcomings, and outlines the next steps of work.

### 关键词

网络服务器、多路 I/O 复用、事件驱动模式、异步日志

## 绪论

### 课题背景

随着近十年来互联网用户的高速增长，互联网应用需求也在成倍增长，同时也对应用后端硬件和软件的基础设施提出了更高的要求——即在保证通信质量和应用数据安全的前提下，快速、准确、高效地响应应用请求。而作为后端重要组成部分之一的服务器是进行海量数据通讯的基础，是互联网应用软件开发中的重中之重，从用户能够直观看到的几千万人同时在线的大型网络游戏，到用户看不到的大量数据库信息通信，都能够看到高性能服务器架构的身影。可以说高性能服务器已经成为了如今互联网时代的软件基础设施之一。

在众多互联网应用中，12306 后台架构的改变和众多铁路乘客在春运高峰期逐渐改善的订票体验完美体现了服务器架构设计的趋势和对于广大用户的意义。多中心、重内存缓存、读写分离、业务解耦等方法都是服务器架构优化所能够考虑的方案，同时也体现了将单一应用程序划分成一组小的服务，服务之间互相协调、互相配合，最终为用户提供稳定、高效的应用服务的微服务架构思想。[27]

在服务器的操作系统方面，Linux 已经迅速占领了网络服务器市场，成为了主流的网络服务器操作系统。这是因为 Linux 具有高度的可定制性和可扩展性，同时也具备了良好的安全性能和出色的网络性能。Linux 操作系统的免费开放源代码特性也为开发者提供了极大的灵活性，可以根据实际需要进行自定义配置和二次开发，满足不同的应用需求。

基于以上背景，本文将设计并实现一个 Linux 下的高性能网络服务器软件，并通过与现有不同的开源服务器框架的性能测试对比，尝试找出本课题设计的服务器架构的性能瓶颈部分。

### 主要工作内容

在本课题的研究过程中，设计作者调研了网络服务器软件的设计和实现的相关技术，对开源社区相关讨论都进行了比较细致的研究，设计并实现了一个高性能网络服务器软件。在项目进行过程中，本人完成了高性能网络服务器软件的详细设计、编码实现、测试的全过程。

文章介绍了作者如下几个方面的工作：

+ 调研了主流网络服务器软件框架的设计和实现，研究了开源社区对于网络服务器需求的相关讨论。
+ 根据调研结果，设计并实现网络服务器软件，完成系统原型。
+ 结合个人实现的单元测试代码测试关键模块功能的正确性，并结合开源静态文件实现一个简单效果的网页来进行系统的集成测试，测试整体功能的正确性。
+ 利用 Webbench 性能测试工具与开源网络服务器软件框架进行对比性能测试。

### 论文结构

本文的结构和主要内容安排如下：

+ 第一章：绪论，介绍本毕业设计的课题背景、主要工作内容等。
+ 第二章：介绍了网络服务器软件设计与开发所需要使用到的架构、技术、系统调用等。
+ 第三章：详细介绍了网络服务器软件的整体架构和各个模块的功能，介绍了三个主要部分——网络连接、异步日志、定时器的处理流程。
+ 第四章：详细介绍各个模块的内部设计和实现，阐述了主要成员函数的功能。
+ 第五章：搭建测试环境，根据测试结果分析网络服务器软件的实现情况。
+ 第六章：对本文进行总结，提出本毕业设计未来的开发展望。

## 网络服务器软件相关技术

### TCP 协议

TCP（Transmission Control Protocol）是一种面向连接的、可靠的、基于字节流的传输协议。其提供了一种端到端的数据传输机制，可以保证数据的可靠性、有序性和完整性。其优势也在于此，但在一些场景下的缺点也较为明显，比如连接建立需要时间、传输过程中需要占用一定的带宽等。

TCP 协议的工作原理是通过三次握手建立连接，然后通过数据包的确认和重传机制来保证数据的可靠性。当一个应用程序需要发送数据时，它会创建一个 TCP 连接，并将数据发送到连接中。TCP 协议将数据分成多个数据包，并将它们发送到网络上。接收方收到数据包后，会发送确认消息给发送方，以确认数据包已经收到。如果发送方没有收到确认消息，它会重新发送数据包，直到接收方确认收到数据包为止。[22]

### HTTP 协议

HTTP（Hypertext Transfer Protocol）是一种用于传输超文本的协议，它是互联网上应用最广泛的协议之一。HTTP 协议使用 TCP 协议作为传输层协议，它使用 80 端口进行通信。

HTTP 协议的请求和响应都是通过 HTTP 报文进行传输的。HTTP 报文分为请求报文和响应报文两种类型。请求报文由请求行、请求头和请求体三部分组成，响应报文由状态行、响应头和响应体三部分组成。

请求报文的请求行包含了请求方法、请求 URL 和 HTTP 协议版本三个部分。请求方法指定了客户端希望服务器执行的操作，常见的请求方法有 GET、POST、PUT、DELETE 等。请求 URL 指定了客户端希望访问的资源的位置。HTTP 协议版本指定了客户端使用的 HTTP 协议版本。请求头包含了一些附加的信息，比如客户端的 User-Agent、Accept-Language、Cookie 等。请求体包含了客户端向服务器发送的数据，比如表单数据、上传的文件等。

响应报文的状态行包含了 HTTP 协议版本、状态码和状态描述三个部分。HTTP 协议版本指定了服务器使用的 HTTP 协议版本。状态码是一个三位数字，用于表示服务器对请求的处理结果。常见的状态码有 200（成功）、404（未找到）、500（服务器内部错误）等。状态描述是对状态码的简短描述。响应头包含了一些附加的信息，比如服务器的 Server、Content-Type、Content-Length 等。响应体包含了服务器向客户端返回的数据，比如 HTML 页面、图片、视频等。[23]

### Socket

Socket 套接字是通信的基石，是支持 TCP/IP 协议的路通信的基本操作单元。可以将套接字看作不同主机间的进程进行双间通信的端点，它构成了单个主机内及整个网络间的编程界面。套接字存在于通信域中，通信域是为了处理一般的线程通过套接字通信而引进的一种抽象概念。套接字通常和同一个域中的套接字交换数据(数据交换也可能穿越域的界限，但这时一定要执行某种解释程序)，各种进程使用这个相同的域互相之间用 Internet 协议簇来进行通信。[18]

Socket 套接字的工作原理是通过网络连接来传输数据。当一个应用程序需要发送数据时，它会创建一个 Socket 套接字，并将数据发送到套接字。套接字将数据传输到网络上，然后将数据传输到目标计算机的套接字。目标计算机的套接字将数据传输到目标应用程序中。[20]

Socket 套接字有三种类型：流套接字、数据报套接字、原始套接字。流套接字是一种面向连接的套接字，它提供了可靠的、有序的、基于字节流的数据传输；数据报套接字是一种无连接的套接字，它提供了不可靠的、无序的、固定大小的数据传输；原始套接字可以直接接收本机网卡上的数据帧或者数据包。[19]

### 事件驱动模式

事件驱动模式是一种围绕事件的发布、捕获、处理和存储(或持久化)而设计软件架构和模式。事件驱动模式在网络服务器领域支持了应用和服务之间的松散耦合，它们可以通过发布和消费事件相互通信。[7]而其中应用最为广泛的是以下两种事件驱动模式：Reactor 模式和 proactor 模式。

#### Reactor 模式

Reactor 模式是非阻塞同步网络模式，捕获就绪的可读写事件。由于 Reactor 模式的灵活多变，使得其存在多种方案可供选择。[8]这里将着重介绍单 Reactor 多线程方案，其方案流程图如图 2-1 所示：

图 2-1 Reactor 模式流程图

![img](./image/Reactor%E6%A8%A1%E5%BC%8F.png)

+ Reactor 监听事件，收到事件后通过 dispatch 进行分发——若是建立连接事件，则交由 Acceptor 获取连接并创建 Handle 对象分发至子线程中来处理后续响应事件；如果不是连接建立事件，则交由当前连接对应的 Handler 对象来进行响应。
+ Handler 对象负责 I/O 操作和业务处理。

#### Proactor 模式

Proactor 模式是异步网络模式，捕获已完成的读写事件，其方案流程图如图 2-2 所示：

图 2-2 Proactor 模式流程图

![img](./image/Proactor%E6%A8%A1%E5%BC%8F.png)

+ Proactor Initiator 负责创建 Proactor 和 Handler 对象，并将 Proactor 和 Handler 都通过 Asynchronous Operation Processor 注册到内核。
+ Asynchronous Operation Processor 负责处理注册请求，并处理 I/O 操作。
+ Asynchronous Operation Processor 完成 I/O 操作后通知 Proactor。
+ Proactor 根据不同的事件类型回调不同的 Handler 进行业务处理。
+ Handler 完成业务处理。

### 多路 I/O 复用

传统的网络编程模型是阻塞式的，每个连接需要创建一个线程或进程来处理请求，这种方式在连接数量较少的情况下还可以接受，但是当连接数量增多时，线程或进程的创建和切换会占用大量的 CPU 资源，导致服务器性能下降。为解决传统网络编程模型的弊病，多路 I/O 复用技术诞生了——其可以在单线程或少数线程的情况下监听多个网络请求，并等待这些请求中的任何一个有数据到达。一旦有数据到达，线程或进程就会被唤醒，可以立即处理该请求并返回数据。这种方式可以有效地降低服务器的 CPU 开销，同时可以处理更多的连接请求。

在 Linux 中，实现多路 I/O 复用的系统调用有 select[13]、poll[14]、epoll[15]，它们的区别在于实现方式和适用场景不同，但对于网络服务器场景，epoll 更加适合，占据了几乎所有 Linux 高性能网络服务器的多路 I/O 复用部分。下面将详细分析 select、poll 在网络服务器场景下的缺点和 epoll 的优势。

#### select 和 poll 的缺点

select 在网络服务器场景下的缺点如下：

+ 单个进程能够监听的文件描述符数量存在最大限制，通常是1024[13]。虽然可以更改数量，但由于 select 采用轮询的方式扫描文件描述符，文件描述符数量越多，性能越差。
+ 内核、用户空间内存拷贝问题，select 需要复制大量的句柄数据结构（文件描述符），产生巨大的开销。
+ select 返回的是含有整个句柄的数组，应用程序需要遍历整个数组才能发现哪些句柄发生了事件。

相比 select，poll 使用链表保存文件描述符，因此没有监听文件描述符数量的限制，但其他两个缺点依然存在。[16]

#### epoll 的优势

epoll 的设计和实现与 select、poll 完全不同。epoll 把原先的 select、poll 调用分成以下 3 个部分：

+ 调用 epoll_create() 建立一个 epoll 对象。
+ 调用 epoll_ctl() 向 epoll 对象中添加连接的 Socket。
+ 调用 epoll_wait() 收集发生的事件的文件描述符资源。

而 epoll 对象中对于大量 Socket 采用红黑树的数据结构进行管理，使得其查找、插入、删除 Socket 的效率显著高于 select 和 poll 的遍历方式。
同时，epoll_wait 的效率也非常高——在调用 epoll_wait 时，不会将全部 Socket 再从内核态复制到用户态，而是仅将活跃的 Socket 复制到用户态，显著减少了复制的开销。[16]

#### 水平触发和边缘触发

多路 I/O 复用的系统调用存在两种触发方式——边缘触发和水平触发。下面将介绍这两种触发方式：

+ 水平触发：当被监控的文件描述符上有可读写事件发生时，会通知用户程序去读写，如果用户一次读写没取完数据，他会一直通知用户。
+ 边缘触发：当被监控的文件描述符上有可读写事件发生时，会通知用户程序去读写，它只会通知用户进程一次，这需要用户一次性将内容读写完成。如果用户未能一次性将数据读完，再次请求时，不会立即返回，需要等待下一次的新数据到来时才会返回，这次返回的内容包括上次未读完的数据。

而在现实的网络编程中，水平触发的使用更多，主要有以下三点原因：

+ 高可靠性：其不会丢失数据或消息，应用没有读取完数据，内核会不断上报。
+ 低延时处理：每次读数据只需要一次系统调用，照顾了多个连接的公平性，不会因为某个连接上的数据量过大而影响其他连接处理消息。
+ 跨平台处理：其像 select 一样可以跨平台使用。[16]

### timerfd

timerfd 是 Linux 为用户程序提供的一个定时器接口。这个接口基于文件描述符，通过文件描述符的可读事件进行超时通知，因此可以配合多路 I/O 复用系统调用使用。其主要有以下几个接口：	

+ timerfd_create() 创建一个定时器对象，同时返回一个与之关联的文件描述符。
+ timerfd_settime() 用于设置新的超时时间，并开始计时，能够启动和停止定时器。
+ timerfd_gettime() 函数获取距离下次超时剩余的时间。

### eventfd

eventfd 是 Linux 特有的专用于事件通知的文件描述符。利用 eventfd() 创建 eventfd 后，可用 read(2) 读取 eventfd，若 eventfd 是阻塞的，read(2) 可能阻塞线程；若 eventfd 设置了 EFD_NONBLOCK，read(2) 返回 EAGIAN 错误。直到另外一个线程对 eventfd 进行 write(2) 后才可正常返回。[12]

### 线程池技术

在并发环境下，系统不能够确定在任意时刻中，有多少任务需要执行，有多少资源需要投入。这种不确定性将带来以下若干问题：

+ 频繁申请、销毁资源和调度资源，将带来额外的消耗，可能会非常巨大。
+ 对资源无限申请缺少抑制手段，易引发系统资源耗尽的风险。
+ 系统无法合理管理内部的资源分布，会降低系统的稳定性。

为解决以上问题，线程池应运而生。线程池维护多个线程，等待监督管理者分配可并发执行的任务。这种做法，相比于一般的线程管理方式有以下四点优势：

+ 降低资源消耗：通过池化技术重复利用已创建的线程，降低线程创建和销毁造成的损耗。
+ 提高响应速度：任务到达时，无需等待线程创建即可立即执行。
+ 提高线程的可管理性：线程是稀缺资源，如果无限制创建，不仅会消耗系统资源，还会因为线程的不合理分布导致资源调度失衡，降低系统的稳定性。使用线程池可以进行统一的分配、调优和监控，保证了对内核的充分有效的利用。
+ 提供更多更强大的功能：线程池具备可拓展性，允许开发人员向其中增加更多的功能。如延时定时线程池，就允许任务延期执行或定期执行。[11]

## 整体架构设计

### 网络服务器软件需求分析

结合网络服务器软件的现实需求、行业标准、开源社区相关讨论等，一个优秀的网络服务器软件应该满足以下需求[1]：

+ 网络服务器软件能够在操作系统的底层能力支持下，完成以下两个主要任务：
  + 接收并管理来自于不同网络地址，且携带不同网络信息的网络连接，同时为它们分配操作系统资源并协调它们之间的工作；
  + 最大限度地满足各类用户的网络服务要求，及时的响应用户服务请求，为互联网服务用户及时的提供所需要的服务信息，包括但不限于请求的回复报文、静态存储的文件、操作系统的状态等。
+ 网络服务器软件在架构上应该足够抽象，以达成可以利用配置文件、中间件或者插件、软件提供相关配置选项的 API 、甚至于将网络服务器软件作为底层，在上层编写相关代码来实现完成特定网络服务的网络服务器软件等不同方式来适应不同的操作系统、多样的网络协议和网络服务需求、日益增长的网络服务用户数量等任务。
+ 网络服务器软件应该为软件开发者提供友好的编程接口和符合人体工程学的编程范式，同时提供详细且易于理解的使用样例及相关文档。
+ 网络服务器软件在现实中可能出现的各种异常情况下应该通过良好的异常处理机制——包括但不限于日志记录、告警处理、自动恢复等方式来保证网络服务不会被可恢复异常中断，且在发生不可恢复异常时能够提供相关信息以帮助软件开发者快速定位问题，降低不可恢复异常所造成的损失。

### 系统架构设计

基于 3.1 节的需求分析，同时结合了其他优秀的网络服务器软件开源项目的架构（Netpoll[2]、Netty[3]、muduo[4] 等），设计了本网络服务器软件的架构。网络服务器软件整体架构图如图 3-1 所示。网络服务器软件整体架构分为操作系统、公共组件、事件分发、网络协议、用户五层，下面将介绍每一层和相关模块的主要功能。

图 3-1 网络服务器软件整体架构图

![img](./image/%E6%9E%B6%E6%9E%84%E5%9B%BE.png)

#### 操作系统

操作系统为计算机硬件提供了抽象和管理，为用户和应用程序提供了接口与服务[5]，但在不同类型的操作系统上相同功能的实现细节、所需的系统调用以及编程上的最佳实践都千差万别，这种差异使得软件的跨平台适配大多都极其繁琐。考虑到网络服务器领域的现实需求和个人能力，本网络服务器软件在架构和实现上都为类 UNIX 系统的适配留下相应接口，但只对 Linux 系统实现完整的网络服务器软件功能。

具体到本服务器软件，类 UNIX 操作系统应当能够为其提供以下接口与服务支持：

+ 网络通信的基础支持，包括网络协议栈、Socket 接口等功能。
+ I/O 多路复用机制。
+ 线程和进程管理机制。
+ 文件系统能够访问并修改磁盘或其他存储设备上的数据。
+ 接收时钟的定时中断，获取时钟的时间信息。

#### 公共组件

公共组件将操作系统提供的接口与服务封装成功能模块，以便于网络服务器软件上层能够忽略底层操作系统的系统调用细节，通过更友好的编程接口来使用操作系统所提供的接口和服务支持，同时为上层应用提供如异步日志等通用功能组件的接口支持。

公共组件各个功能模块的具体功能介绍如下：

+ 网络：对常用网络编程系统接口的封装，包括 Socket、InetAddress、Buffer 部分。

  + Socket 部分是对 Socket 文件描述符及其各种常用操作的封装，如创建 Socket、绑定地址、监听、接受连接等。
  + InetAddress 部分是对网络地址及其相关操作的封装，可以获取本端和对端的 ip 地址和端口信息，也可以传入 ip 地址和端口信息用来设置 Socket 文件描述符所绑定的地址。
  + Buffer 部分是网络服务器软件数据缓冲区的实现。由于网络服务器软件所使用到的网络协议（主要是 TCP 协议）大多是无边界的字节流协议[6]，可能会发生读取或写入数据时不能通过一次系统调用完成的情况，这时就需要缓冲区来承接这部分数据以保证程序不会因此阻塞——例如服务器发送 100 字节的数据，但使用系统调用后只写入 80 字节，在缓冲区存在的情况下，服务器软件可以将剩余数据放入缓冲区，并将 Socket 是否可写列入文件描述符监听队列，交出线程控制权，直至 Socket 可写后激活线程并发送缓冲区数据。
+ I/O 复用：对多路 I/O 复用系统调用的封装。基于 2.4 节的对于多路 I/O 复用的介绍，本网络服务器软件最终选用了水平触发的 epoll 系统调用来实现多路 I/O 复用。包括 Channel、Poller、EpollPoller 部分。

  + Channel 部分是对文件描述符的封装，每个 Channel 对象与一个文件描述符对应，其中定义了该文件描述符的可监听的事件、事件的回调函数等。
  + Poller 部分抽象了 I/O 多路复用的操作，如注册事件、删除事件等；并提供实现接口，为在其它类 UNIX 系统下实现本网络服务器软件可用的 I/O 多路复用功能给出了符合人体工程学的编程范式。
  + EpollPoller 部分是对 Linux 下 epoll 相关的多路 I/O 复用系统调用的封装，继承自 Poller 类，补全了接口函数，实现了 Linux 下本网络服务器软件可用的 epoll 多路 I/O 复用功能。
+ Timestamp：本网络服务器软件的微秒级时间戳部分，具有以下功能：

  + 通过操作系统获取当前微秒级时间戳。
  + 可比较时间戳先后顺序、计算时间戳之间的差值、计算某个时间戳在一定时间后的另一个时间戳等。
  + 将时间戳转换成为形如“2023/04/24 23:41:31”格式的时间字符串。
+ Thread：对操作系统线程管理能力的封装，有创建、销毁线程等功能。
+ 定时器：对操作系统接收定时中断能力的封装，且能够在收到定时中断后处理对应事件。包括 Timer、TimerQueue 部分。

  + Timer 部分定义了定时任务的属性以及对其进行读取和修改的函数方法。
  + TimerQueue 部分是定时器队列类，实现了对定时器队列的管理，如添加、删除定时事件等。
+ 异步日志：将代码运行时的重要信息进行保存，方便故障诊断和追踪，包括日志前端和日志后端。

  + 日志前端主要为开发人员提供异步日志接口，能够使其按照等级写入日志并在控制台输出日志信息。包括 Logger、LogStream 部分。
    + Logger 部分为用户提供日志库的接口，能够设置日志等级、控制日志输出流等。
    + LogStream 部分主要提供日志流操作，将用户提供的整型数、浮点数、字符、字符串、字符数组、二进制内存、另一个日志缓冲区，格式化为字符串，并加入当前流的日志缓冲区。
  + 日志后端将前端写下的日志写入文件，放入计算机存储设备中使其持久化。包括 FileUtil、LogFile、AsyncLogging 部分。
    + FileUtil 部分封装操作系统提供的底层的创建、打开文件，写文件，关闭文件等操作接口。
    + LogFile 部分提供对日志文件的操作，包括滚动日志文件、将日志数据写到当前日志文件、刷新日志数据到当前日志文件；同时保证底层操作的线程安全。
    + AsynccLogging 部分提供缓冲存放多条日志信息，为前端线程提供线程安全的写日志操作；提供专门的后端线程，用于定时或缓冲非空时，将缓冲中日志信息逐个写到磁盘上。

#### 事件分发

Linux 在 5.1 版本才支持可应用于网络编程的异步 I/O 系统调用—— io_uring，此前仅支持同步阻塞或非阻塞的网络 I/O 系统调用[9]。故基于 2.4 节的介绍和 3.1.1 节的操作系统取舍，本网络服务器软件将采用 Reactor 模式作为事件分发部分的事件驱动模式。

事件分发部分作为网络服务器软件的核心，其在内部实现了 Reactor 模式，向上层代码提供了一组符合直觉的事件驱动调用接口，开发人员可以基于该接口适配其他网络协议。事件分发包括 EventLoop、EventLoopThread、EventLoopThreadPool 三部分。

+ EventLoop 部分是事件循环类，负责接收 Channel 对象上的事件并将其分发到对应的事件处理函数中。
+ EventLoopThread 部分是封装了一个新线程，并在该线程上创建一个 EventLoop 对象，用于处理事件。
+ EventLoopThreadPool 部分是封装了一个线程池，用于创建一组 EventLoopThread 对象，以此提高并发处理能力和操作系统资源利用率。

#### 网络协议

网络协议基于事件分发部分的接口，实现特定网络通信协议的特性，为应用程序提供网络协议支持。开发人员也可以在本部分适配其他的网络协议或特性。本网络服务器软件实现了 TCP 协议和 HTTP 协议。

+ TCP 部分实现对 TCP 连接的管理。包括 TcpConnection、TcpServer、Acceptor 部分。

  + TcpConnection 部分是对一条 TCP 连接的封装，包括读写数据、关闭连接等。
  + TcpServer 部分是对 TCP 服务器的封装，负责监听并处理新的 TCP 连接。
  + Acceptor 部分用于接收来自其他网络地址的 TCP 网络连接并将其分发至线程池中经过分配策略所决定的工作线程。
+ HTTP 部分实现 HTTP 协议中的 GET 和 POST 请求， 实现了静态文件传输和下载的功能。包括 HttpServer、HttpContext、HttpResponse、HttpRequest 部分。

  + HttpServer 部分是对 HTTP 服务器的封装，负责监听并处理新的 HTTP 连接。
  + HttpContext 部分是对 HTTP 报文解析的封装，负责解析收到的 HTTP 报文信息。
  + HttpResponse 部分是对 HTTP 请求类的封装，负责管理 HttpContext 解析完成的报文信息。
  + HttpRequest 部分是对 HTTP 响应类的封装，负责管理服务器响应报文的数据并将其格式化为字符串报文。

#### 用户

用户部分使得使用该系统的各类用户，包括软件开发人员和网络服务的使用者，它们可以通过软件开发人员在这一部分实现的业务代码，从而使网络服务的使用者得到其所需要的服务数据。

### 处理流程

本网络服务器软件存在三个业务处理流程：网络连接处理流程、异步日志处理流程、定时器处理流程。

#### 网络连接处理流程

网络服务器软件的本质是处理四个事件[4]，即：

+ 建立连接。
+ 断开连接。
+ 读取网络信息。
+ 发送网络消息。

故网络协议之间细节可能不同，但流程都大致相似，故下文将以 TCP 协议为例介绍本网络服务器软件的网络连接处理流程。图 3-2 为本网络服务器软件在 TCP 协议下的处理流程图。

图 3-2 网络服务器软件处理 TCP 网络连接的处理流程图

![img](./image/TCP%E7%BD%91%E7%BB%9C%E8%BF%9E%E6%8E%A5%E6%B5%81%E7%A8%8B%E5%9B%BE.png)

##### 建立连接

使用 Socket 来创建一个简单的 TCP 服务器时，建立一个连接通常需要四个步骤：

+ 调用 socket 函数建立监听 socket。
+ 调用 bind 函数绑定需要监听的地址和端口。
+ 调用 listen 函数监听端口。
+ 调用 accept 函数返回新建立连接的文件描述符。

而在本网络服务器软件中，也基本遵循了以上步骤：

+ 构造一个事件循环器 EventLoop。
+ 建立对应的业务服务器 TcpServer。在构造 TcpServer 时，会创建 Acceptor 对象，在 Acceptor 对象的构造函数中，会执行 socket 函数和 bind 函数。
+ 然后 main 函数执行 TcpServer 的 TcpServer::start() 方法完成以下两个任务——启动线程池；调用 listen 函数，并将 listen 所监听的文件描述符添加到可读事件的监听队列中。
+ 设置 TcpServer 的对应的回调函数。
+ main 函数会执行 EventLoop::loop() 方法来启动事件循环。在事件循环中，当有连接到来后，会执行 Acceptor 中 handleRead 回调函数，进而调用 TcpServer 中的 newConnection 回调函数，执行 accept 函数并生成对应的文件描述符并将其绑定至一个 TcpConnection 实例中，并注册相应的回调函数。随后将文件描述符分发至线程池中的特定子线程的事件循环中。

##### 读取网络信息

读取网络信息主要有以下两个步骤：

+ 在子线程的事件循环中，当连接中有数据送达时，就会调用 TcpConnection 中的 handleRead 回调函数，将连接送达的数据放入请求数据缓冲区（inputBuffer）中。
+ 调用用户注册的 onMessage 回调函数，执行网络服务的业务逻辑。

##### 发送网络信息

在用户注册的 onMessage 回调中会调用 TcpConnection::send() 方法来发送网络信息，这个方法会调用 TcpConnection::sendInLoop() ，其处理过程如下：

+ 若响应数据缓冲区（outputBuffer） 为空，则直接向网络连接文件描述符写数据。
+ 若向网络连接文件描述符写的数据未写完，则记录剩余的字节数。
+ 若此时响应数据缓冲区（outputBuffer）中的旧数据的个数和未写完字节个数和大于高水位表记值（highWaterMark） ，则调用回调函数 highWaterMarkCallback。否则将剩余数据写入响应数据缓冲区（outputBuffer）。
+ 若向网络连接文件描述符写的数据没有写完，则最后需要为其注册可写事件。当其可写事件发生时，调用对应的回调 TcpConnection::handleWrite()，尽可能将数据从响应数据缓冲区（outputBuffer）中向网络连接文件描述符中写数据。
+ 如果响应数据缓冲区（outputBuffer）中的数据都写完了，将网络连接文件描述符的写事件移除，并调用写完成回调 writeCompleteCallback。

##### 断开连接

断开连接分为被动断开和主动断开。主动断开和被动断开的处理流程基本一致，所以被动断开为例介绍断开连接的流程。

被动断开即客户端断开了连接，服务端需要感知到这个断开的状态，然后进行的相关的处理。其中感知远程断开这一步是在 TCP 连接的可读事件处理函数 TcpConnection::handleRead() 中进行的：当对网络连接文件描述符进行 read 操作时，返回值为 0，则说明此时连接已断开。然后调用 TcpConnection::handleClose() 函数。其执行过程如下：

+ 将网络连接文件描述符从监听队列中移除。
+ 调用用户的设置的 onConnection() 函数。
+ 将网络连接文件描述符对应的 TcpConnection 对象从 TcpServer 中移除。
+ 关闭网络连接文件描述符。

#### 异步日志处理流程

异步日志的处理流程如下图 3-3。

图 3-3 异步日志处理流程图

![img](./image/%E5%BC%82%E6%AD%A5%E6%97%A5%E5%BF%97%E6%B5%81%E7%A8%8B%E5%9B%BE.png)

异步日志整个流程采用双缓冲区的思路——准备两个日志缓冲区：A 和 B，前端线程从各自的日志缓冲区往 Buffer A 填入数据（日志消息），后端线程负责将 Buffer B 的数据（日志消息）写入日志文件。当 A 写满时，交换 A 和 B。在后端设置一个已满缓冲队列，用于缓存一个周期内临时要写的日志消息，这样两个 Buffer 在前端写日志时，不必等待磁盘文件操作，也避免每写一条日志消息都触发后端线程。

#### 定时器处理流程

不同定时器接口的处理流程一致，故将以 EventLoop 中 runAt() 接口流程为例介绍定时器处理流程，图 4-4 为 runAt() 的处理流程图。

图 3-4 定时器 runAt() 函数处理流程图

![img](./image/%E5%AE%9A%E6%97%B6%E5%99%A8%E6%B5%81%E7%A8%8B%E5%9B%BE.png)

+ 当 EventLoop 被创建时，与其同生命周期的 TimerQueue 定时器队列类和其管理的 timerfd 也一并被创建。
+ 执行 runAt() 接口创建单次定时事件时，调用 TimerQueue::addTimer() 向 TimerQueue 的定时器队列中添加定时任务，同时向 EventLoop 中的 Poller 注册监听 timerfd 事件。
+ 当 timerfd 到达预定时间被唤醒时，执行其回调函数 TimerQueue::handleRead()，执行定时任务。若其为循环任务，则重新添加定时任务，注册监听 timerfd；反之则删除定时任务，为下一个定时任务注册监听 timerfd。

## 各模块的设计与实现

### 网络

网络部分类图如下图 4-1 所示：

图 4-1 网络部分类图

![img](./image/%E7%BD%91%E7%BB%9C%E7%B1%BB%E5%9B%BE.png)

#### InetAddress

InetAddress 类对地址信息进行了包装，是 sockaddr_in 的包装类，提供 sockaddr_in 的常用方法。

+ toIpPort(), toIp(), toPort()：将 IP 地址、Port 信息由 sockaddr 对象，转换为字符串或整形。
+ getLocalAddr()：从连接获取本地ip地址（包括端口号）。
+ getPeerAddr()：获取连接对端的ip地址（包括端口号）。

#### Socket

Socket 类是 Socket 文件描述符的一个轻量级封装，提供操作系统 Socket 文件描述符的常用方法。

+ setReuseAddr(), setReusePort()：设置是否开启复用 IP 地址和端口的特性。
+ setKeepAlive()：是否设置心跳检测以保持 TCP 长连接。
+ setTcpNoDelay()：用于设置 TCP_NODELAY 选项，以禁用 Nagle 算法，从而不会等到收到 ACK 才进行下一次数据发送，而是 TCP 协议栈缓冲中有数据就立即发送。
+ createNonblockingOrDie()：创建非阻塞 Socket 文件描述符。
+ bindAddress()：绑定 Socket 文件描述符与本地地址。
+ listen()：监听本地 Socket 文件描述符。
+ accept()：接受连接请求。
+ shutdownWrite()：关闭连接写方向。
+ getSocketError()：获取tcp协议栈错误。通常在处理连接的读写事件时调用，检查是否发生错误。

* isSelfConnect()：检查是否为自连接。利用了 InetAddress 类的 getLocalAddr() 和 getPeerAddr() 函数，检查 IP 地址是否相同，来判断连接对端地址信息是否为本机。

#### Buffer

##### Buffer 数据结构

图 4-2 Buffer 数据结构示意图

![img](https://img2022.cnblogs.com/blog/741401/202204/741401-20220412171109103-1465773870.png)

readIndex、writeIndex 把 buffer_ 内容分为三部分：prependable、readable、writable。灰色部分是 Buffer 的有效载荷，prependable 能让程序以极低代价在数据前添加几个字节，从而简化客户代码。

Buffer中有两个个常数：kCheapPrepend，kInitialSize，分别定义了 prependable 初始大小，writable 的初始大小。readable 初始大小 0。

初始化完成后，Buffer 数据结构如下：

图 4-3 Buffer 初始化后数据结构示意图
![img](https://img2022.cnblogs.com/blog/741401/202204/741401-20220412173407579-1717880116.png)

##### 基本 I/O 操作

Buffer 初始化完成后，向 Buffer 写入 200 byte，其布局是：

图 4-4 Buffer 写入 200 byte 数据后数据结构示意图

![img](https://img2022.cnblogs.com/blog/741401/202204/741401-20220412173502458-783290308.png)

可以看到，writeIndex 向后移动了 200，readIndex 保持不变。readable、writable 都有变化。

如果从 Buffer 读入 50 byte，其布局是：

图 4-5 Buffer 读入 50 byte 数据后数据结构示意图
![img](https://img2022.cnblogs.com/blog/741401/202204/741401-20220412173532217-1169824755.png)

可以看到，readIndex 向后移动 50，writeIndex 保持不变，readable 和 writable 的值也有变化。

经过若干次读写，readIndex 移到较靠后的位置，留下了很大的 prependable 空间，如下图已由初始 8 byte，变成 532 byte。

图 4-6 Buffer 532 byte prependable 空间的数据结构示意图
![img](https://img2022.cnblogs.com/blog/741401/202204/741401-20220412173616065-1397427603.png)

此时，若想写入 300 byte，Buffer 不会重新分配内存，而是先把已有的数据移到前面去，减小多余 prependable 空间，为 writable 腾出空间。

图 4-7 Buffer 减小多余 prependable 空间后数据结构示意图
![img](https://img2022.cnblogs.com/blog/741401/202204/741401-20220412173620685-816517251.png)

这样，writable 变成 724 byte，就可以写入 300 byte 数据了。

Buffer 长度不是固定的，可以自动增长，因为底层存储利用的是 vector。

假设当前可写空间 writable 为 624 byte，现客户代码一次写入 1000 byte，那么 buffer 会自动增长，过程如图 4-8 和 4-9 所示。

增长前：

图 4-8 Buffer 写入 1000 byte 数据前数据结构示意图
![img](https://img2022.cnblogs.com/blog/741401/202204/741401-20220412173542958-1236803670.png)

增长后：

图 4-9 Buffer 写入 1000 byte 数据后数据结构示意图
![img](https://img2022.cnblogs.com/blog/741401/202204/741401-20220412173546598-1288273385.png)

readable 由 350 增长为 1350，writable 由 624 减为 0。另外，readIndex 由 58 回到了初始位置 8，保证prependable 等于 kCheapPrependable。Buffer 没有缩小功能，下次写入 1350 byte 就不会重新分配内存，一方面避免浪费内存，另一方面避免反复分配内存。

##### 重要函数

+ retrieve：retrieve 系列函数从 readable 空间取走数据，只关心移动 readableIndex_，改变 readable 空间大小，通常不关心读取数据具体内容，除非有指定具体的返回值，如 retrieveAllAsString()。其会改变 readable 空间大小，但通常不会改变 writable 空间大小，除非 retrieveAll() 取完所有 readable 空间的数据，readable 空间将会合并到 writable 空间。
+ peek：peek 系列函数只从 readable 空间头部读取数据，而不取走数据，不会导致 readable 空间变化。
+ makeSpace()：makeSpace() 生产足够大小的 writable 空间，以写入新的数据.
+ readFd()：readFd() 从指定连接对应文件描述符读取数据，当读取的数据超过内部缓冲区 writable 空间大小时，采用的策略是先用一个 64K 栈缓存 extrabuf 临时存储，然后根据需要合并 prependable 空间到 writable 空间，或者重新分配 buffer_ 大小。
+ writeFd(): readFd() 向指定连接对应文件描述符读取数据。
+ findCRLF()：对 readable 空间中的 CRLF 字符进行专门的查找。

### I/O 复用

I/O 复用部分类图如下图 4-10 所示：

图 4-10 I/O 复用部分类图

![img](./image/IO%E5%A4%8D%E7%94%A8%E7%B1%BB%E5%9B%BE.png)

#### Channel

Channel 成员函数主要包括以下八类：

+ 设置事件处理的回调函数，如 setReadCallback()。
+ 使能文件描述符关心的事件 events_ 会注册到 Poller 中进行监听，如 enableReading()。
+ 关闭文件描述符关心的事件 events_，更新该文件描述符在 Poller 中监听的事件，如 disableReading()。
+ 关闭文件描述符关心的所有事件 events_，更新该文件描述符在 Poller 中监听的事件，如 disableAll()。
+ 删除对文件描述符的监听，会将其从 Poller 的 ChannelMap 中移除,如 remove()。
+ handleEvent() 处理激活的 Channel 事件，由 Poller 更新激活的 Channel 列表，EventLoop::loop() 根据激活 Channel 列表，逐个执行 Channel 中已注册好的相应回调。实际事件处理工作，由 handleEventWithGuard 完成。其中 tie 变量保证了执行事件处理动作时，所需对象不会被释放。handleEventWithGuard() 根据不同的激活原因，调用不的回调函数。这些回调函数，需要进行事件监听的类中进行设置，比如 TcpConnection，EventLoop，Acceptor，TimerQueue 等。
+ update() 通过 EventLoop 对象，传递给 Poller 对象，然后更新其监听的通道列表中对应通道。支持 ADD、MOD 操作。
+ remove() 通过 EventLoop 传递给 Poller 对象，将当前通道从 Poller 的事件列表中删除。支持 DEL 操作。

#### Poller 和 EPollPoller

EPollPoller 以 epoll 为核心，实现了基类 Poller 的 virtual 函数，在其中调用了 epoll_create、epoll_ctl、epoll_wait 等接口。poll() 返回后，会将就绪的文件描述符添加到激活队列 activeChannels 中管理。

### 定时器

定时器部分类图如下图 4-11 所示：

图 4-11 定时器类图

![img](./image/%E5%AE%9A%E6%97%B6%E5%99%A8%E7%B1%BB%E5%9B%BE.png)

#### Timer 和 TimerId

Timer 类代表一个超时任务，但并不直接绑定 Channel。Timer 主要包含超时时刻（expiration_），超时回调（callback_），周期时间值（interval_），全局唯一id（sequence_）。每当创建一个新 Timer 对象时，原子变量 s_numCreated_ 就会自增一，作为全剧唯一序列号 sequence_，用来标识该 Timer 对象。

在创建 Timer 时，超时时刻 when 决定了回调超时事件时间点，而 interva 决定了 Timer 是一次性的，还是周期性的。如果是周期性的，会在 TimerQueue::reset() 中，调用 Timer::restart()，在当前时间点基础上，重启定时器。

restart() 重启 Timer，根据 Timer 是否为周期类型，分为两种情况：

+ 周期 Timer，restart() 将重置超时时刻 expiration_ 为当前时间加上周期间隔时间。
+ 非周期 Timer，即一次性 Timer，restart() 将 expiration_ 置为无效时间，默认为 0。

TimerId 类来主要用来作为 Timer 的唯一标识，用于取消 Timer。

#### TimerQueue

定时器队列 TimerQueue 类是定时功能的核心，其包含 3 个 Timer 集合：

+ timers_ 定时器集合：包含用户添加的所有 Timer 对象，std::set 会用 AVL 搜索树，对集合元素按时间戳（Timestamp）从小到大顺序。
+ activeTimers_ 激活定时器集合：包含激活的 Timer 对象，与 timers_ 包含的 Timer 对象相同，个数也相同，std::set 会根据 Timer 指针大小，对元素进行排序。
+ cancelingTimers_ 取消定时器集合：包含所有取消的 Timer 对象，与 activeTimers_ 相对。

构造 TimerQueue 对象时，就会绑定 TimerQueue 创建 TimerQueue 的 EventLoop 对象。同时调用 Channel::enableReading() 将 timerfd 加入 Poller 的监听列表中。

在析构 TimerQueue 对象有两点需要注意：

+ 在 remove() 绑定的 timerfd 前，要先 disableAll() 停止监听所有 timerfd 事件。
+ timers_ 中 Timer 对象是在 TimerQueue::addTimer() 中使用 new 关键字创建出来的，需要手动 delete。

addTimer() 会在构造一个 Timer 对象后，将其添加到 timers_ 的工作转交给 addTimerInLoop() 完成调用——调用定时器函数的线程，可能并非在 TimerQueue 所在的 loop 线程，而修改 TimerQueue 数据成员时，必须在所属 loop 线程中进行，因此需要通过 loop_->runInLoop() 将工作转交给所属 loop 线程。

addTimerInLoop() 的主要工作由两个函数来完成：insert 将 Timer 插入 timers_ 和 activeTimers_，resetTimerfd 重置 timerfd 到下一次定时事件的触发时间。

TimerQueue::cancel() 可以清除一个尚未到期的定时器。其类似于 addTimer()，cancel() 也可能在别的线程被调用，因此需要将其转交给 cancelInLoop() 执行。

timerfd 的可读事件回调函数 handleRead() 实现有三个要点：

+ 其必须在所在 loop_ 线程运行。
+ 可能不止一个定时任务超时，可用 getExpired() 获取。
+ 所有超时任务执行完后，重置周期定时任务，释放一次性定时任务。

getExpired() 以参数时间点 now 为界限，查找 set timers_ 中所有超时定时任务。set 会对 timers_ 元素进行排序，std::set::lower_bound() 会二分查找到第一个小于 now 时间点的定时任务，同时调用 reset 重置所有超时的周期定时任务，释放超时的一次性任务。

### 异步日志

#### FixedBuffer

FixedBuffer 类图如下图 4-12 所示：

图 4-12 FixedBuffer 类图

![img](./image/FixedBuffer%E7%B1%BB%E5%9B%BE.png)

FixedBuffer 模板类内部是用数组 data_[SIZE] 存储，用指针 cur_ 表示当前待写数据的位置。对FixedBuffer 的各种操作，实际上是对 data_ 数组和 cur_ 指针的操作。

在异步日志系统中，其有以下两种具体实现：

+ Small Buffer，默认大小 4 KB，用于存放前端日志消息，为前端类 LogStream 持有。
+ Large Buffer，默认大小 4 MB，用于存放大量后端日志消息。为后端类 AsyncLogging 持有。

#### 日志前端

日志前端主要包括：Logger, LogStream，SourceFile。其类图如下图 4-13 所示：

图 4-13 日志前端部分类图
![img](./image/%E6%97%A5%E5%BF%97%E5%89%8D%E7%AB%AF%E7%B1%BB%E5%9B%BE.png)

##### Logger

Logger 支持以下六种日志等级：

+ TRACE：指出比 DEBUG 粒度更细的一些信息事件。
+ DEBUG：指出细粒度信息事件对调试应用程序是非常有帮助的。
+ INFO：表明消息在粗粒度级别上突出强调应用程序的运行过程。
+ WARN：系统能正常运行，但可能会出现潜在错误的情形。
+ ERROR：指出虽然发生错误事件，但仍然不影响系统的继续运行。
+ FATAL：指出每个严重的错误事件将会导致应用程序的退出。

本网络服务器软件默认级别为 INFO，开发过程中可以选择 TRACE 或 DEBUG。

日志信息是由 Logger 的内部类 Logger::Impl 所提供，其包含了一条完整日志信息的所有组成部分，并通过 Logstream::operator<< 将日志信息加入到 Small Buffer 中。

Logger 的用户接口则通过定义了一系列 LOG_ 开头的宏，每个宏定义都构造了一个 Logger 临时对象，然后通过 stream() 函数来达到以 C++ 风格写日志的能力。同时定义全局变量存储当前日志等级（g_logLevel），通过宏定义的 if 语句，使得日志仅记录不低于当前日志等级的日志；低于当前日志等级的不会有任何操作，几乎 0 开销。而且用户还可以调用 setOutput()、setFlush()、setLogLevel() 接口来调整日志的输出位置（g_output），日志冲刷方式（g_flush）,当前日志等级（g_logLevel）等日志系统基本配置。

而在其析构函数中，其主要工作是为 LogStream 对象 stream_ 中的日志消息加上后缀（如文件名、行号，换行符等），将 stream_ 缓存的 log 消息通过 g_output 回调写入指定文件流。另外，如果有致命错误（FATAL级别log），就终止程序。

##### LogStream

针对不同类型数据，LogStream 重载了一系列 operator<< 操作符，用于将数据格式化为字符串，并存入LogStream::buffer_ （Small Buffer）中。

+ 对于字符串类型参数，operator<< 本质上是调用 buffer_ 对应的FixedBuffer::append()，将其存放当到 Small Buffer 中。
+ 对于字符类型，跟参数是字符串类型区别是长度只有一，并且无需判断指针是否为空。
+ 对于十进制整型，如 int/long，则是通过模板函数 formatInteger()，将转换为字符串并直接填入Small Buffer 尾部。formatInteger() 并没有用 snprintf 对整型数据进行格式转换，而是用到了 Matthew Wilson 提出的高效的转换方法 convert()[21]。基本思想是：从末尾开始，对待转换的整型数，由十进制位逐位转换为 char 类型，然后填入缓存，直到剩余待转数值已为 0。
+ 对于double类型，使用库函数 snprintf 转换为 const char*，并直接填入 Small Buffer 尾部。
+ 对于其他类型，都是转换为以上基本类型，然后再转换为字符串，添加到 Small Buffer 末尾。

#### 日志后端

日志后端主要包括：AsyncLogging, LogFile, FileUtil。其类图如下图 4-14 所示：

图 4-14 日志后端部分类图

![img](./image/%E6%97%A5%E5%BF%97%E5%90%8E%E7%AB%AF%E7%B1%BB%E5%9B%BE.png)

##### AsyncLogging

AsyncLogging 类提供 Large Buffer 存放多条日志消息，缓冲队列 BufferVector 用于存放多个 Large Buffer，为前端线程提供线程安全的写 Large Buffer 操作；提供专门的后端线程，用于定时或缓冲队列非空时，将缓冲队列中的 Large Buffer 通过 LogFile 提供的日志文件操作接口，逐个写到磁盘上。

提供线程安全的写 Large Buffer 操作的函数是 append()，其基本思路是当前缓冲（currentBuffer_）剩余空间足够存放新日志消息大小时，就直接存放到当前缓冲；当前缓冲剩余空间不够时，说明当前缓冲已满（或者接近已满），就将当前缓冲 move 到已满缓冲队列（buffers_），将空闲缓冲 move 到当前缓冲，再把新日志消息存放到当前缓冲中，最后唤醒等待中的后端线程。同时 append() 可能会被多个前端线程调用，因此必须考虑线程安全。这里用 mutex_ 加锁来保证数据的线程安全。

后端线程的启动是在 start() 中，通过调用 Thread::start() 完成。而 stop() 用于关闭后端线程，通常是在析构函数中调用其停止后端线程。

后端线程函数 threadFunc()，会构建一个 LogFile 对象，用于控制日志文件创建和日志数据的写入；创建两个空闲缓冲区 buffer1、buffer2，和一个待写缓冲队列 buffersToWrite，分别用于替换当前缓冲 currentBuffer_、空闲缓冲 nextBuffer_、已满缓冲队列 buffers_，避免在写文件过程中，锁住缓冲和队列，导致前端无法写数据到后端缓冲。

threadFunc() 中，提供了一个执行循环，其基本流程如下：

+ 每次当已满缓冲队列中有数据时，或者即使没有数据但3秒超时，就将当前缓冲加入到已满缓冲队列（即使当前缓冲没满），将 buffer1 移动给当前缓冲，buffer2 移动给空闲缓冲（如果空闲缓冲已移动的话）。
+ 再交换已满缓冲队列和待写缓冲队列。
+ 将待写缓冲队列的所有缓冲通过 LogFile 对象，写入日志文件。
+ 待写缓冲队列中的缓冲，已全部写到 LogFile 指定的文件中（也可能在内核缓冲中）后，擦除多余缓冲，只用保留两个，归还给 buffer1 和 buffer2。
+ 清空待写缓冲队列中的缓冲。
+ 将内核高速缓存中的数据 flush 到磁盘，防止意外情况造成数据丢失。

##### LogFile

当日志文件接近指定的滚动限值（rollSize_）时，需要换一个新文件写数据，便于后续归档、查看。调用 rollFile() 函数可以实现文件滚动，其基本流程如下：

+ 利用 getLogFileName() 根据调用者提供的基础名，以及当前时间，得到一个全新的、唯一的日志文件名。
+ 创建并打开一个新日志文件，用指向 LogUtil 对象的指针 file_ 表示。

而为避免频繁创建新文件，rollFile() 会确保上次滚动时间到现在如果不超过一秒，就不会滚动。

LogFile 提供了两个个接口，用于向当前日志文件 file_ 写入数据—— append() 本质上是通过 appendInLock() 完成对日志文件写操作，但使用互斥锁 mutex_ 来保证线程安全；appendInLock() 会先将日志信息写入 file_ 文件，之后再判断是否需要滚动日志文件；如果不滚动，就根据 append_unlocked 的调用次数和时间，确保一个日志文件超时（默认1天），就创建一个新日志文件，同时调用 FileUtil::flush() 函数刷新文件操作（默认间隔3秒）。

flush() 函数实际上是通过 AppendFile::flush()，完成对日志文件的冲刷，也使用互斥锁 mutex_ 来保证线程安全。

##### FileUtil

FileUtil 类提供主要功能的有三个函数：append()、write()、flush()。其中，append() 是将指定数据附加到文件末尾，但实际的写文件操作是通过 write() 来完成的；write() 出于性能考虑通过非线程安全的 glibc 库函数 fwrite_unlocked() 来完成写文件操作，而线程安全将由上层调用 LogFile 来保证。

### Thread

Thread 类图如下图 4-15 所示：

图 4-15 Thread 类图
![img](./image/Thread%E7%B1%BB%E5%9B%BE.png)

+ start() 函数用来初始化子线程，获取子线程操作系统编号，执行子线程函数。
+ join() 函数阻塞主线程，等待子线程终止后回收相关资源。

### Timestamp

Timestamp 类图如下图 4-16 所示：

图 4-16 Timestamp 类图

![img](./image/Timestamp%E7%B1%BB%E5%9B%BE.png)

+ 成员变量 microSecondsSinceEpoch_ 用来来表示从 Epoch 时间到目前为止的微妙数。
+ valid() 函数判断 microSecondsSinceEpoch_ 有效性。
+ now() 函数用 gettimeofday() 获取当前时刻，转化为微秒，并构造一个 Timestamp 对象。
+ toString() 将秒数、微秒数转换为形如“2023/04/24 23:41:31”格式的时间字符串。
+ operator<() 比较两个个时间戳先后顺序。
+ addTime() 利用一个基准时间戳 timestamp 加上一段时间的秒数，得到新的 Timestamp 对象。

### 事件分发

事件分发部分类图如下图 4-17 所示：

图 4-17 事件分发部分类图

![img](./image/%E4%BA%8B%E4%BB%B6%E5%88%86%E5%8F%91%E7%B1%BB%E5%9B%BE.png)

#### EventLoop

基于 3.2.3 节中关于 EventLoop 功能的讨论和图 4-17 中 EventLoop 类图，其设计和实现可分为四个部分：

+ 提供事件循环。
+ 运行定时任务。
+ 处理激活通道事件。
+ 线程安全。

##### 事件循环

事件循环主要有以下四个接口，loop()、quit()、wakeup()、handleRead()。

loop() 是EventLoop 实现事件循环的核心函数。其利用 Poller::poll() 函数将激活事件填入 Channel 队列 activeChannels_，然后排队执行每个 Channel 的 handleEvent，从而调用对应激活事件的回调函数。

quit() 函数通过修改 quit_ 的值提供了退出事件循环的接口。

wakeup() 函数用于通过向类内自创建的 wakeupFd_ 文件描述符写入信息来唤醒被阻塞的事件循环。

handleRead() 函数用于读取 wakeupFd_ 文件描述符中的信息来完成可读事件。

##### 处理激活通道事件

处理激活通道事件的接口有以下三个：updateChannel()、removeChannel()、hasChannel()。

updateChannel()、removeChannel() 利用 Poller::updateChannel 和 Poller::removeChannel 更新或移除 Poller 监听的事件。

hasChannel() 来判断 Poller 是否正在监听某个 Channel。

##### 定时任务

定时任务的接口有以下四个：runAt()、runAfter()、runEvery()、cancel()。其中前三个接口利用 TimerQueue::addTimer() 函数添加一次性定时任务或周期定时任务，最后一个接口调用 TimerQueue::cancel() 函数取消定时任务。

##### 线程安全

一个 EventLoop 的实例的接口函数可能会出现多个线程并发执行的情况，这使得保证类私有数据的线程安全就格外重要。所以 EventLoop 在设计上采用“one loop per thread”的设计思想——即一个事件循环（EventLoop）对应一个线程，这在大部分情况是一个非常好的线程模型，大大减少了因数据竞争而产生的并发问题。[10]

同时在具体实现和使用时，本软件使用了以下四种种方法来实现以上的设计思想：

+ 在每个子线程中设置其自己拥有的全局 EventLoop 变量 t_loopInThisThread，用以保证每个子线程中都运行唯一的 EventLoop 实例。
+ 实现 isInLoopThread、assertInLoopThread 函数用以判断调用函数线程和 EventLoop 事件循环线程是否是同一线程，以保证事件循环函数始终在同一线程中执行。
+ 实现 runInLoop、queueInLoop 函数，用以处理来自不同线程的用户任务，具体分为以下两种情况：
  + 若当前线程是 EventLoop 事件循环线程，则立即执行用户任务。
  + 反之，则加入用户任务队列 pendingFunctors_ 中，唤醒事件循环后排队执行。
+ 用户任务队列 pendingFunctors_ 使用互斥锁保证不同线程之间使用用户任务队列的线程安全。

#### EventLoopThread

EventLoopThread 的结构比较简单，对外只提供启动 I/O 线程的接口 startLoop()。startLoop() 接口过程就是启动在初始化时创建的 I/O 线程，然后等待 I/O 线程函数执行 threadFunc() 函数完成初始化，并将 loop_ 同步值返回给函数调用者。其中 threadFunc() 函数也叫做 I/O 线程函数 主要工作是创建 EventLoop 局部对象，并将其值同步至 EventLoop 的 loop_，然后运行事件循环。

##### 线程安全

从上文介绍可以看出 EventLoopThread 中的 loop_ 指针在创建时（startLoop()中），会存在调用线程和子线程函数 threadFunc 并发读、写 loop_ 的情况。为保证 loop_ 的数据线程安全，在实现时采用了以下两种方式：

+ 采用互斥锁在读写时保护 loop_，同时使用条件变量来同步 loop_ 的值。
+ I/O 线程函数退出时，线程中事件循环已不在运行，这时应清空 loop_ 的值，否则可能会导致析构函数重复调用 loop_ 的 quit() 函数，让 I/O 线程循环重复退出。

#### EventLoopThreadPool

EventLoopThreadPool 线程池类通常由 main 函数线程创建，绑定 main 函数线程创建的 EventLoop 到 baseLoop_ 中。

线程池在创建后，通过调用 start() 接口来启动线程池。其主要工作如下：

+ 保证 baseLoop_ 所属线程调用 start() 接口。
+ 创建用户指定数量的线程（其数量由用户调用 setThreadNum() 接口确定），并启动线程，记录子线程对应 EventLoop。
+ 若没有指定线程数量（或为指定 0），调用用户指定的线程初始化回调函数。

getNextLoop() 接口从 I/O 线程池维护的 EventLoop 数组 loops_ 中轮询取得一个 EventLoop 对象，每次调用取下一个 EventLoop 对象（若取到最后一个，则下一个是数组中第一个 EventLoop 对象），利用此分配策略以达到负载均衡的目的。

### TCP

TCP 部分类图如下图 4-18 所示：

图 4-18 TCP部分类图

![img](./image/TCP%E7%B1%BB%E5%9B%BE.png)

#### Acceptor

Acceptor 是 TcpServer 的一个内部类，Acceptor 内部有一个 Channel 成员，当 Poller 监听到有 TCP 连接请求时，就通过 Channel 的可读事件，在 loop 线程，来回调 Acceptor::handleRead()。从而将连接文件描述符和 IP 地址传递给上一层 TcpServer，用于创建 TcpConnection 对象管理 TCP 连接。

#### TcpServer

TcpServer 类管理 TcpConnection，供用户直接使用，生命周期由用户控制。ConnectionMap 保存有服务器所有以连接的 TCP 连接，而每个 TCP 连接用的是一个 TcpConnection 对象来管理的。

setThreadNum() 用于设置 TcpServer 中线程池的初始线程数量。

start() 用于启动 TcpServer 的事件循环。

#### TcpConnection

TcpConnection 类是本网络服务器最核心的类，唯一默认用 shared_ptr 来管理的类，唯一继承自 enable_shared_from_this 的类。这是因为其生命周期模糊——可能在连接断开时，还有其他地方持有它的引用，贸然 delete 会造成空悬指针。只有确保其他地方没有持有该对象的引用的时候，才能安全地销毁对象。TcpConnection 的状态有四个：kDisconnected, kConnecting, kConnected, kDisconnecting。这些状态也在实现中帮助本网络服务器软件更好的确定 TcpConnection 实例的生命周期状态，减少错误释放资源所导致的相关问题。

TcpConnection 为满足网络用户需求，创建了大量的回调函数，如 TcpConnection::handleError()—— TcpConnection 在处理事件发生错误时，则会借由 Channel::errorCallback_ 回调到 TcpConnection::handleError() 函数进行相关的错误处理。

TcpConnection 不仅为错误事件准备了回调函数，也为应用程序提供了几个用于处理连接及数据的回调接口，主要包括 ConnectionCallback、MessageCallback、WriteCompleteCallback、HighWaterMarkCallback、CloseCallback。

connectionCallback_ 存放用户注册的连接回调， 会在以下几种情形回调：

+ TCP 连接确认建立时：TcpServer 在 TCP 连接建立时，通过 newConenction() 新建 TcpConnection 对象时，回调TcpConnection::connectEstablished()，进而回调 connectionCallback_。
+ TCP 连接确认销毁时：TcpServer 在移除 TCP 连接时，通过 removeConnectionInLoop() 销毁连接，回调 connectionCallback_。
+ 强制关闭 TCP 连接时：从 socket 文件描述符读到数据量为 0（对端关闭连接），进而调用 handleClose()，回调 connectionCallback_。

messageCallback_ 存放用户注册的接收到消息回调，会在 Channel 调用 handleRead 处理读事件时，调用 TcpConnection::handleRead()，从 socket 文件描述符读到数据量大于 0，并存放到 inputBuffer_ 中，然后回调 messageCallback_，交给用户处理从对端接收到的数据。

writeCompleteCallback_ 存放用户注册的写完成回调，将要发送的数据写到内核缓冲区后，然后回调 writeCompleteCallback_。

highWaterMarkCallback_ 存放用户注册的高水位回调，用户send数据时，会在应用发送缓存堆积数据过多大于高水位标记值（highWaterMark_）时调用。

而这些回调都可以使用 Set 系列函数来设置，如 TcpConnection::setMessageCallback() 函数。

### HTTP

HTTP 部分类图如下图 4-19 所示：

图 4-19 HTTP 部分类图

![img](./image/HTTP%E7%B1%BB%E5%9B%BE.png)

#### HttpServer

HttpServer 将收到的数据存放于 TcpConnection::inputbuffer_ 中，之后利用 HttpContext 解析成 HttpRequest 请求对象，再创建一个 HttpResponse 响应对象并将其内容格式化为字节数据放入 TcpConnection::outputbuffer_，随后发送返回给用户。

根据 HTTP 协议的规定，HttpServer 也为用户提供了以下四个接口能够更快、更符合编程直觉的方式处理连接请求，返回相应报文：

+ Get() 函数定义收到特定 URL 的 GET 请求时的处理流程。
+ Post() 函数定义收到特定 URL 的 POST 请求时的处理流程。
+ StaticFile() 函数定义了收到特定 URL 的 GET 请求时其所加载到用户浏览器所对应的静态文件。
+ DownloadFile() 函数定义了收到特定 URL 的 GET 请求时其所对应下载到用户本地的静态文件。

#### HttpRequest

在解析 HTTP 请求报文时会创建 HttpRequest 实例，用来将字节序信息转化为结构化信息，包括请求头、请求体等。其中成员函数也大多是设置或获取请求报文信息，如 setPath() 等；剩余部分包含了一些字符解析的辅助函数和对于 HttpRequest 操作如交换的辅助函数，如 swap()、ConverHex() 等。

#### HttpResponse

HttpResponse 主要给用户以编程的方式编辑 HTTP 的响应报文，并按照 HTTP 响应报文格式将结构化数据转化为字节序信息。其中成员函数大多是设置或获取响应报文信息，如 setStatusCode() 等；剩余部分函数是将结构化数据转化为字节数据的，如 appendStringToBuffer() 等。

#### HttpContext

HttpContext 主要将 HTTP 请求报文的字节序信息解析为结构化信息并存储至 HttpRequest 中。其采用的实现思想是有限状态机（Finite State Machine，FSM）——其是一种数学模型，它由一组状态、一组输入符号和一组转移函数组成。在 HTTP 协议中，有限状态机用于解析请求报文，它将请求报文分成多个部分，并根据当前状态和输入符号来决定下一步的操作。

HTTP 协议中的有限状态机包含了以下几个状态：

+ 正在解析请求行。
+ 正在解析请求头。
+ 正在解析请求体。
+ 已经解析完成。

而其有限状态机的输入符号包括了请求行、请求头、请求体等。根据当前状态和输入符号，有限状态机会执行相应的操作，比如解析请求行、解析请求头、解析请求体等。

使用有限状态机解析 HTTP 请求报文可以提高解析效率和减少内存占用，因为有限状态机只需要保存当前状态和一些必要的信息，而不需要保存整个请求报文。此外，有限状态机还可以检测请求报文的格式是否正确，从而提高请求的可靠性。

parseRequest() 就是实现有限状态机解析 HTTP 请求报文的函数过程的主要函数；processRequestLine() 函数是单独解析请求行的函数，其余函数大多是类辅助函数，如 reset() 等。

## 系统测试

### 测试环境

测试环境为一台笔记本电脑，处理器为 Intel Core i5-9300H，内存为 16GB，使用 Windows 11 操作系统，在其之上运行 Windows Subsystem for Linux（WSL）Ubuntu 22.04.2 TLS，编译选择的 C++ 标准为 C++11，编译器版本为 Ubuntu clang 14.0.0-1ubuntu1。

### 单元测试

单元测试部分将测试主要模块的功能是否能够按照预期正常运行。这里将测试三个主要模块：网络连接模块、异步日志模块、定时器模块，测试用例和测试结果图如下所示：

表 5-1 网络连接模块的测试用例

| 流程名称 | 网络连接流程                                              |
| -------- | --------------------------------------------------------- |
| 测试类型 | 功能、正确性                                              |
| 用例描述 | 验证网络连接能否正常接收并返回对应信息                    |
| 输入数据 | 连接服务器成功后，客户端任意输入一串字符串。              |
| 预计结果 | 服务端日志显示输入字符串，客户端在新一行显示输入字符串。  |
| 实际结果 | 与预计结果相符，图 5-1 展示了网络连接模块的终端显示结果。 |

图 5-1 网络连接模块的终端显示结果

![img](./image/%E6%B5%8B%E8%AF%95%E7%BB%93%E6%9E%9C%E6%88%AA%E5%9B%BE/%E7%BD%91%E7%BB%9C%E8%BF%9E%E6%8E%A5%E6%B5%8B%E8%AF%95.png)

表 5-2 异步日志模块的测试用例

| 流程名称 | 异步日志流程                                                                                             |
| -------- | -------------------------------------------------------------------------------------------------------- |
| 测试类型 | 功能、正确性                                                                                             |
| 用例描述 | 验证异步日志能否正确写入文件且单文件日志数量达到一定数量时是否会主动创建并切换日志文件                   |
| 输入数据 | 利用程序启动异步日志后端，写入 102420 条各种类型（不包括 FATAL 级别）日志。                              |
| 预计结果 | 所有日志完整的写入到两个文件中，且两个文件的命名有所区别。                                               |
| 实际结果 | 与预计结果相符，图 5-2 展示了终端显示的日志内容和生成的两个日志文件，图 5-3 展现了文件内写入的日志信息。 |

图 5-2 终端显示的日志内容和生成的两个日志文件

![img](./image/%E6%B5%8B%E8%AF%95%E7%BB%93%E6%9E%9C%E6%88%AA%E5%9B%BE/%E5%BC%82%E6%AD%A5%E6%97%A5%E5%BF%97%E6%B5%8B%E8%AF%95.png)

图 5-3 文件内写入的日志信息

![img](./image/%E6%B5%8B%E8%AF%95%E7%BB%93%E6%9E%9C%E6%88%AA%E5%9B%BE/%E5%BC%82%E6%AD%A5%E6%97%A5%E5%BF%97%E6%96%87%E4%BB%B6.png)

表 5-3 定时器模块的测试用例

| 流程名称 | 定时器流程                                                                |
| -------- | ------------------------------------------------------------------------- |
| 测试类型 | 功能、正确性                                                              |
| 用例描述 | 验证定时器任务能否按照输入时间定时触发                                    |
| 输入数据 | 设置两个定时任务，第一个是 1 秒打印一次值，第二个是 5 秒后结束程序。      |
| 预计结果 | 每秒在一行上打印一个数字，按顺序递增，最后打印 ”timer finish“程序结束。 |
| 实际结果 | 与预计结果相符，图 5-4 展示了定时器测试的终端显示结果。                   |

图 5-4 定时器测试的终端显示结果
![img](./image/%E6%B5%8B%E8%AF%95%E7%BB%93%E6%9E%9C%E6%88%AA%E5%9B%BE/%E5%AE%9A%E6%97%B6%E5%99%A8%E6%B5%8B%E8%AF%95.png)

### 集成测试

集成测试将整体测试整个网络服务器软件是否能够按照预期返回结果。为测试流程的统一和方便叙述，这里使用了一个开源的网页代码[28]，并由论文作者在网络服务器软件上实现相关的业务代码，以能够完整请求所有页面和静态资源，包括图片和视频，并完成指定文件的下载为正常运行的标准，来执行网络服务器软件的集成测试。

在浏览器地址栏输入 localhost:8086 进入测试网页主页，如下图所示：

图 5-5 网页主页

![img](./image/%E6%B5%8B%E8%AF%95%E7%BB%93%E6%9E%9C%E6%88%AA%E5%9B%BE/%E7%BD%91%E9%A1%B5%E4%B8%BB%E9%A1%B5.png)

页面和首页头像图片都能够正常请求和加载。接下来点击右上角 "图片" 导航按钮，进入图片加载测试页面，如下图所示：

图 5-6 图片页面

![img](./image/%E6%B5%8B%E8%AF%95%E7%BB%93%E6%9E%9C%E6%88%AA%E5%9B%BE/%E7%BD%91%E9%A1%B5%E5%9B%BE%E7%89%87%E6%B5%8B%E8%AF%95.png)

图片可以正常加载并浏览。然后点击右上角 ”视频“ 导航按钮，进入视频加载测试页面，如下图所示：

图 5-7 视频页面

![img](./image/%E6%B5%8B%E8%AF%95%E7%BB%93%E6%9E%9C%E6%88%AA%E5%9B%BE/%E7%BD%91%E9%A1%B5%E8%A7%86%E9%A2%91%E6%B5%8B%E8%AF%95.png)

视频可以正常播放并暂停。然后点击右上角 "登录" 导航按钮，进入登录测试页面，如下图所示：

图 5-8 登录页面

![img](./image/%E6%B5%8B%E8%AF%95%E7%BB%93%E6%9E%9C%E6%88%AA%E5%9B%BE/%E7%99%BB%E5%BD%95%E6%B5%8B%E8%AF%95.png)

登录页面正常加载，然后输入账号和密码，点击 ”确认“ 按钮。若账号密码正确，页面将重定向至欢迎页面，如图 5-9 所示；若账号密码错误，将重定向至错误页面，如图 5-10 所示。

图 5-9 欢迎页面

![img](./image/%E6%B5%8B%E8%AF%95%E7%BB%93%E6%9E%9C%E6%88%AA%E5%9B%BE/%E7%99%BB%E5%BD%95%E6%88%90%E5%8A%9F%E9%87%8D%E5%AE%9A%E5%90%91%E9%A1%B5%E9%9D%A2.png)

图 5-10 错误页面

![img](./image/%E6%B5%8B%E8%AF%95%E7%BB%93%E6%9E%9C%E6%88%AA%E5%9B%BE/%E7%99%BB%E5%BD%95%E5%A4%B1%E8%B4%A5%E9%87%8D%E5%AE%9A%E5%90%91%E9%A1%B5%E9%9D%A2.png)

接下来点击右上角 ”注册“ 导航按钮，进入注册测试页面，如下图所示：

图 5-11 注册页面

![img](./image/%E6%B5%8B%E8%AF%95%E7%BB%93%E6%9E%9C%E6%88%AA%E5%9B%BE/%E6%B3%A8%E5%86%8C%E9%A1%B5%E9%9D%A2.png)

注册页面正常加载，然后输入注册账号和密码，点击 ”确认“ 按钮，然后将页面重定向至欢迎页面，如下图所示：

图 5-12 注册成功页面

![img](./image/%E6%B5%8B%E8%AF%95%E7%BB%93%E6%9E%9C%E6%88%AA%E5%9B%BE/%E6%B3%A8%E5%86%8C%E6%88%90%E5%8A%9F%E9%87%8D%E5%AE%9A%E5%90%91%E9%A1%B5%E9%9D%A2.png)

接着在浏览器地址栏输入 localhost:8086/filed 可以下载一张图片，结果如下图所示：

图 5-13 下载成功结果图片

![img](./image/%E6%B5%8B%E8%AF%95%E7%BB%93%E6%9E%9C%E6%88%AA%E5%9B%BE/%E4%B8%8B%E8%BD%BD%E6%B5%8B%E8%AF%95.png)

图片成功完整的被下载到本地并能够在本地进行预览。最后若输入的 URL 未在网络服务器软件的用户代码中被定义，则将页面重定向至 404 页面，如下图所示：

图 5-14 404 页面

![img](./image/%E6%B5%8B%E8%AF%95%E7%BB%93%E6%9E%9C%E6%88%AA%E5%9B%BE/404%20%E9%A1%B5%E9%9D%A2.png)

### 性能测试

性能测试将与开源网络服务器软件 muduo 进行对比。测试流程为在本网络服务器软件和 muduo 上编写相同的业务代码，利用 Webbench[29] 分别创建 100、1000、5000、10000 个线程请求同一地址页面运行 10 秒，记录它们请求成功的连接数量和传输数据的速度，借以评价两个网络服务器软件的性能。根据网络常识可知，请求成功的连接数量越多、传输数据的速度越快，网络服务器软件的性能越好。下图 5-15 是本网络服务器软件单次 Webbench 测试终端显示结果，表 5-4 是性能测试的原始数据结果表格：

图 5-15 本网络服务器单次 Webbench 测试终端显示结果

![img](./image/%E6%B5%8B%E8%AF%95%E7%BB%93%E6%9E%9C%E6%88%AA%E5%9B%BE/webbench%20%E6%B5%8B%E8%AF%95.png)

表 5-4 性能测试原始数据结果表格

| 服务器软件类型   | 创建线程数 | 请求成功的连接数量 | 传输数据的速度    |
| ---------------- | ---------- | ------------------ | ----------------- |
| 本网络服务器软件 | 100        | 10319              | 224954 bytes/sec  |
|                  | 1000       | 9408               | 205094 bytes/sec  |
|                  | 5000       | 9130               | 199034 bytes/sec  |
|                  | 10000      | 9372               | 204309 bytes/sec |
| muduo            | 100        | 10317              | 224910 bytes/sec  |
|                  | 1000       | 9412               | 205181 bytes/sec |
|                  | 5000       | 9145               | 199361 bytes/sec |
|                  | 10000      | 9403               | 204984 bytes/sec |

### 测试结果分析

根据单元测试和集成测试结果，本网络服务器软件基本完成了 3.1 节需求分析中的大部分功能，实现了数据报文和静态文件的传输、下载，定时器和异步日志系统的正常工作也为定时任务和异常处理提供了足够的实现空间。而根据性能测试结果，本网络服务器软件在中低负载下拥有接近或超过 muduo 的处理性能，在高负载时本网络服务软件性能略低于 muduo，表明本网络服务器软件在高负载情况的性能还有待改进。利用 strace 命令统计网络服务器软件各个系统调用的使用次数和执行时间，如下图 5-16 所示，可以发现 read 和 write 系统调用占用了大约 56.53% 的系统调用时间。因此优化的方向可以考虑合并同类同文件描述符的系统调用，减少系统调用的使用次数来达到高负载情况下的性能优化。

图 5-16 网络服务器软件使用各系统调用的次数和时间的统计结果
![img](./image/%E6%B5%8B%E8%AF%95%E7%BB%93%E6%9E%9C%E6%88%AA%E5%9B%BE/strace%20%E7%BB%93%E6%9E%9C.png)

## 总结

### 全文总结

本文参考了大量成熟的网络服务器框架，如 Netpoll、Netty、muduo 等，设计并实现了 Reactor 事件驱动模式为基础的高性能网络服务器软件，同时为 HTTP 协议提供了简单支持，能够完成 GET、POST 类型请求的完整网络流程并实现静态文件的传输和下载。

本文首先介绍了 Linux 下高性能网络服务器的课题背景并介绍了实现所需的相关技术；接着介绍了本网络服务器软件的整体架构和各个架构模块之间的功能分工，并对网络请求、异步日志、定时器三个主要处理流程进行了详细的阐述；随后部分介绍了各个架构模块的内部设计与实现，阐述了重要成员函数的功能；最后，通过对系统的单元测试和集成测试，验证了系统功能的正确实现，证明了系统的功能达到了设计的要求；而与成熟的网络服务器软件 muduo 之间的性能对比测试也证明了本网络服务器软件在中低负载下拥有相当于 muduo 的性能，但在高负载下仍有进一步提升性能的空间，为未来的本网络服务器软件的性能改进指明了方向。

### 未来工作展望

结合未来网络服务器软件框架的发展趋势、5.5 节测试结果分析和开源社区相关讨论，对于本软件未来的开发工作有以下展望：

+ 目前网络缓冲区的实现虽然尽力减少复制次数，但仍存在复制移动占用部分 CPU 周期和操作系统资源。参考 libevent 2.0 版本对于网络缓冲区的设计[24]，不要求数据在内存中连续，而是用链表把数据块链接到一起，以实现 zero copy buffer 的特性。
+ 目前网络服务器软件的操作系统调用仍然存在内核态到用户态的数据复制过程，在网络压力较高时会导致占用 CPU 周期，减缓网络连接处理流程。而 Linux 内核在 4.14 以后支持 send，5.4 以后支持 receive 系统调用的 ZeroCopy 特性[25]。支持在版本较高的操作系统上可选启动 ZeroCopy 系统调用特性，也是未来性能改进的一个方向。
+ 在网络层支持更多的网络协议特性，比如微服务和分布式领域常用的 Protocol Buffers 协议[26]。

## 致谢

经过几个月的忙碌与学习，本次毕业设计伴随着大学生活已经接近尾声。虽然由于个人能力和时间的关系，毕业设计还有很多不尽如人意的地方，如代码结构不合理、实现方式过于粗糙缺少对未来维护和性能的考量……但这个玩具式的毕业设计仍然填补了我对于 Web 编程框架到 Socket 套接字之间网络处理流程认识的缺失，实现了我对于相关知识进行系统学习的愿望。

在这个过程中，我的导师汪茂老师对我给予了极大的帮助。从小学期到毕业设计，我在本人感兴趣的领域进行学习探索，无不都受到了汪老师的支持。作为本科生，相关经验的匮乏导致有许多考虑不够周全的地方，在毕业设计论文方面尤其如此，而其中大多数问题都是由汪老师所指出并指导我进行修正，极大提高了学习和工作的效率。而在我对于工作焦虑和迷茫之时，也是汪老师给予我指导和帮助。感谢汪老师对于我个人兴趣的尊重和不厌其烦的指导与帮助。

其次感谢舍友们在毕业设计过程中所提供的通知信息和 word 操作方法的指导，没有这些帮助这篇毕业设计也不能准时的以学校规定的标准格式完成。

最后感谢父母、我的弟弟、廖登彬同学以及游戏《原神》和《塞尔达传说：王国之泪》，人的部分是因为在我离开学校走向社会这段最迷茫和焦虑的时候他们充分倾听了我无理由的情绪，给予了我鼓励并肯定了我的个人价值；而游戏的部分是因为在我疲劳的时候它们一直和最近给予了我充分的放松和游玩的快乐，能够让我以更饱满的状态投入到工作和学习之中。

没有以上种种因素，这篇毕业设计都不能够很好的完成，在这里再次感谢以上所有提到的人和文化产品，同时也感谢能够让我遇到这一切因素的中国地质大学（北京）的平台和自 1840 年以来为国内革命和工业化建设添砖加瓦的所有人。

## 参考文献

[1] Yeager N J, McGrath R E. Web server technology[M]. Morgan Kaufmann, 1996.

[2] [字节跳动在 Go 网络库上的实践](https://www.cloudwego.io/zh/blog/2020/05/24/%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8%E5%9C%A8-go-%E7%BD%91%E7%BB%9C%E5%BA%93%E4%B8%8A%E7%9A%84%E5%AE%9E%E8%B7%B5/)

[3] [Netty Project. Netty Project Community. [2019-01-31].](https://web.archive.org/web/20190130065314/https://netty.io/)

[4] 陈硕. Linux 多线程服务端编程: 使用 muduo C++ 网络库[M]. 电子工业出版社, 2013.

[5] Silberschatz A, Galvin G. Operating systems concepts[J]. 2018.

[6 - 22] [RFC 793](https://datatracker.ietf.org/doc/html/rfc793)

[7] [什么是事件驱动的架构？](https://www.ibm.com/cn-zh/topics/event-driven-architecture)

[8] [如何深刻理解Reactor和Proactor？ - 小林coding的回答 - 知乎](https://www.zhihu.com/question/26943938/answer/1856426252)

[9] [Efficient IO with io_uring](https://kernel.dk/io_uring.pdf)

[10] [INTERACTION WITH OTHER PROGRAMS, LIBRARIES OR THE ENVIRONMENT](http://pod.tst.eu/http://cvs.schmorp.de/libev/ev.pod#THREADS_AND_COROUTINES)

[11] [Java线程池实现原理及其在美团业务中的实践](https://tech.meituan.com/2020/04/02/java-pooling-pratice-in-meituan.html)

[12] [eventfd(2) — Linux manual page](https://man7.org/linux/man-pages/man2/eventfd.2.html)

[13] [select(2) — Linux manual page](https://man7.org/linux/man-pages/man2/select.2.html)

[14] [poll(2) — Linux manual page](https://man7.org/linux/man-pages/man2/poll.2.html)

[15] [epoll(7) — Linux manual page](https://man7.org/linux/man-pages/man7/epoll.7.html)

[16] 游双. Linux 高性能服务器编程[M]. 机械工业出版社, 2013.

[17] [timerfd_create(2) — Linux manual page](https://man7.org/linux/man-pages/man2/timerfd_create.2.html)

[18] 谢希仁. 计算机网络[M]. 电子工业出版社, 2003.

[19] Stevens W R, Narten T. UNIX network programming[J]. ACM SIGCOMM Computer Communication Review, 1990, 20(2): 8-9.

[20] [getaddrinfo(3) — Linux manual page]( https://man7.org/linux/man-pages/man3/getaddrinfo.3.html)
[21] Wilson M. Efficient Variable Automatic Buffers[J]. CC PLUS PLUS USERS JOURNAL, 2003, 21(12): 30-35.

[22] RFC 9293: Transmission Control Protocol (TCP)[J]. 2022.

[23] Fielding R, Gettys J, Mogul J, et al. RFC2616: Hypertext Transfer Protocol--HTTP/1.1[J]. 1999.

[24] [Bufferevents: concepts and basics](https://libevent.org/libevent-book/Ref6_bufferevent.html)

[25] [Linux Zero-copy(零拷贝)机制](https://chinalhr.github.io/post/linux-zerocopy/)

[26] Currier C. Protocol buffers[M]//Mobile Forensics–The File Format Handbook: Common File Formats and File Systems Used in Mobile Devices. Cham: Springer International Publishing, 2022: 223-260.

[27] 朱建生, 王明哲, 杨立鹏, 等. 12306 互联网售票系统的架构优化及演进[J]. 铁路计算机应用, 2015, 24(11): 1-4.

[28] [WebServer](https://github.com/markparticle/WebServer)

[29] [Webbench](http://home.tiscali.cz/~cz210552/webbench.html)
