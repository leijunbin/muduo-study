# 毕业论文文字图片稿

## 摘要和关键词

### 中文

### 英文

### 关键词

## 绪论

### 课题背景

### 课题主要研究内容

### 主要工作内容

### 论文结构

## 网络服务器软件相关技术

### TCP 协议

### HTTP 协议

### Socket

### 事件驱动模式

事件驱动模式是一种围绕事件的发布、捕获、处理和存储(或持久化)而设计软件架构和模式。事件驱动模式在网络服务器领域支持了应用和服务之间的松散耦合，它们可以通过发布和消费事件相互通信。[7]而其中应用最为广泛的是以下两种事件驱动模式：Reactor 模式和 proactor 模式。

#### Reactor 模式

Reactor 模式是非阻塞同步网络模式，捕获就绪的可读写事件。由于 Reactor 模式的灵活多变，使得其存在多种方案可供选择。[8]这里将着重介绍单 Reactor 多线程方案，其方案流程图如图 2-1 所示：

![img](./image/Reactor%E6%A8%A1%E5%BC%8F.png)

+ Reactor 监听事件，收到事件后通过 dispatch 进行分发——若是建立连接事件，则交由 Acceptor 获取连接并创建 Handle 对象分发至子线程中来处理后续响应事件；如果不是连接建立事件，则交由当前连接对应的 Handler 对象来进行响应。
+ Handler 对象负责 I/O 操作和业务处理。

#### Proactor 模式

Proactor 模式是异步网络模式，捕获已完成的读写事件，其方案流程图如图 2-2 所示：

![img](./image/Proactor%E6%A8%A1%E5%BC%8F.png)

+ Proactor Initiator 负责创建 Proactor 和 Handler 对象，并将 Proactor 和 Handler 都通过 Asynchronous Operation Processor 注册到内核。
+ Asynchronous Operation Processor 负责处理注册请求，并处理 I/O 操作。
+ Asynchronous Operation Processor 完成 I/O 操作后通知 Proactor。
+ Proactor 根据不同的事件类型回调不同的 Handler 进行业务处理。
+ Handler 完成业务处理。

### 多路 I/O 复用

I/O复用是一种高效的网络编程技术，可以在单线程或少数线程的情况下处理多个客户端的请求。在网络服务器中，I/O复用技术通常用于监听和处理连接请求，以及处理客户端发送的数据。

传统的网络编程模型是阻塞式的，每个连接需要创建一个线程或进程来处理请求，这种方式在连接数量较少的情况下还可以接受，但是当连接数量增多时，线程或进程的创建和切换会占用大量的 CPU 资源，导致服务器性能下降。

而使用 I/O 复用技术，服务器只需要创建一个线程或进程来监听所有连接请求，并等待这些请求中的任何一个有数据到达。一旦有数据到达，线程或进程就会被唤醒，可以立即处理该请求并返回数据给客户端。这种方式可以有效地降低服务器的 CPU 开销，并且可以处理更多的连接请求。

在 I/O 复用中，常用的技术包括 select、poll、epoll 等，这些技术都是基于操作系统提供的多路复用机制实现的。它们的区别在于实现方式和适用场景不同，但本质上都是通过监听多个文件描述符的可读状态，来实现同时处理多个连接请求的功能。

总的来说，I/O 复用技术是网络服务器中非常重要的一种技术，可以提高服务器的性能和可扩展性，降低服务器的开销和复杂性。

#### select 和 poll 的缺点

select 的缺点:

1. 单个进程能够监视的文件描述符数量存在最大限制，通常是1024（`#define __FD_SETSIZE 1024`）。当然可以更改数量，但由于 select 采用轮询的方式扫描文件描述符，文件描述符数量越多，性能越差;
2. 内核/用户空间内存拷贝问题，select 需要复制大量的句柄数据结构（文件描述符），产生巨大的开销;
3. select 返回的是含有整个句柄的数组，应用程序需要遍历整个数组才能发现哪些句柄发生了事件;
4. select 的触发方式是水平触发，应用程序如果没有完成对一个已经就绪的文件描述符进程IO操作，那么之后每次 select 调用还是会将这些文件描述符通知进程。

相比 select 模型，poll 使用链表保存文件描述符，因此没有了监视文件数量的限制，但其他三个缺点依然存在。

> 以 select 模型为例，假设我们的服务器需要支持100万的并发连接，则在__FD_SETSIZE 为 1024 的情况下，则我们至少需要开辟1k个进程才能实现100万的并发连接。除了进程间上下文切换的时间消耗外，从内核/用户空间大量的句柄结构内存拷贝、数组轮询等，是系统难以承受的。因此，基于select模型的服务器程序，要达到100万级别的并发访问，是一个很难完成的任务。

#### epoll 的优势

epoll的实现机制与select/poll机制完全不同，它们的缺点在epoll上不复存在。

设想一下如下场景：有100万个客户端同时与一个服务器进程保持着TCP连接。而每一时刻，通常只有 几百上千个TCP连接是活跃的(事实上大部分场景都是这种情况)。如何实现这样的高并发？

在select/poll时代，服务器进程每次都把这100万个连接告诉操作系统（从用户态复制句柄数据结构到 内核态），让操作系统内核去查询这些套接字上是否有事件发生，轮询完成后，再将句柄数据复制到用 户态，让服务器应用程序轮询处理已发生的网络事件，这一过程资源消耗较大，因此，select/poll一般 只能处理几千的并发连接。

epoll的设计和实现与select完全不同。epoll通过在Linux内核中申请一个简易的文件系统（文件系统一 般用什么数据结构实现？B+树，磁盘IO消耗低，效率很高）。把原先的select/poll调用分成以下3个部 分：

1. 调用epoll_create()建立一个epoll对象（在epoll文件系统中为这个句柄对象分配资源）
2. 调用epoll_ctl向epoll对象中添加这100万个连接的套接字
3. 调用epoll_wait收集发生的事件的fd资源

如此一来，要实现上面说是的场景，只需要在进程启动时建立一个epoll对象，然后在需要的时候向这 个epoll对象中添加或者删除事件。同时，epoll_wait的效率也非常高，因为调用epoll_wait时，并没有 向操作系统复制这100万个连接的句柄数据，内核也不需要去遍历全部的连接。

epoll_create在内核上创建的eventpoll结构如下：

```c++
struct eventpoll{
....
/*红黑树的根节点，这颗树中存储着所有添加到epoll中的需要监控的事件*/
struct rb_root rbr;
/*双链表中则存放着将要通过epoll_wait返回给用户的满足条件的事件*/
struct list_head rdlist;
....
};
```

#### 边缘触发和水平触发

水平触发：当被监控的文件描述符上有可读写事件发生时，会通知用户程序去读写，如果用户一次读写没取完数据，他会一直通知用户，如果这个描述符是用户不关心的，它每次都返回通知用户，则会导致用户对于关心的描述符的处理效率降低。

边缘触发：当被监控的文件描述符上有可读写事件发生时，会通知用户程序去读写，它只会通知用户进程一次，这需要用户一次把内容读取玩，相对于水平触发，效率更高。如果用户一次没有读完数据，再次请求时，不会立即返回，需要等待下一次的新的数据到来时才会返回，这次返回的内容包括上次未取完的数据。

 **muduo 采用的是水平触发** ：

1. 不会丢失数据或消息
   * 应用没有读取完数据，内核会不断上报
2. 低延迟处理
   * 每次读数据只需要一次系统调用，照顾了多个连接的公平性，不会因为某个连接上的数据量过大而影响其他连接处理消息
3. 跨平台处理
   * 像 select 一样可以跨平台使用

要想客户端和服务器能在网络中通信，那必须得使用 Socket  编程，它是进程间通信里比较特别的方式，特别之处在于它是可以跨主机间通信。

Socket  的中文名叫作插口，咋一看还挺迷惑的。事实上，双方要进行网络通信前，各自得创建一个 Socket，这相当于客户端和服务器都开了一个“口子”，双方读取和发送数据的时候，都通过这个“口子”。这样一看，是不是觉得很像弄了一根网线，一头插在客户端，一头插在服务端，然后进行通信。

创建 Socket 的时候，可以指定网络层使用的是 IPv4 还是 IPv6，传输层使用的是 TCP 还是 UDP。

UDP 的 Socket 编程相对简单些，这里我们只介绍基于 TCP 的 Socket 编程。

服务器的程序要先跑起来，然后等待客户端的连接和数据，我们先来看看服务端的 Socket 编程过程是怎样的。

服务端首先调用 socket() 函数，创建网络协议为 IPv4，以及传输协议为 TCP 的 Socket ，接着调用 bind() 函数，给这个 Socket 绑定一个 IP 地址和端口，绑定这两个的目的是什么？

绑定端口的目的：当内核收到 TCP 报文，通过 TCP 头里面的端口号，来找到我们的应用程序，然后把数据传递给我们。

绑定 IP 地址的目的：一台机器是可以有多个网卡的，每个网卡都有对应的 IP 地址，当绑定一个网卡时，内核在收到该网卡上的包，才会发给我们；

绑定完 IP 地址和端口后，就可以调用 listen() 函数进行监听，此时对应 TCP 状态图中的 listen，如果我们要判定服务器中一个网络程序有没有启动，可以通过 netstate 命令查看对应的端口号是否有被监听。

服务端进入了监听状态后，通过调用 accept() 函数，来从内核获取客户端的连接，如果没有客户端连接，则会阻塞等待客户端连接的到来。

那客户端是怎么发起连接的呢？客户端在创建好 Socket 后，调用 connect() 函数发起连接，该函数的参数要指明服务端的 IP 地址和端口号，然后万众期待的 TCP 三次握手就开始了。

在  TCP 连接的过程中，服务器的内核实际上为每个 Socket 维护了两个队列：

一个是还没完全建立连接的队列，称为 TCP 半连接队列，这个队列都是没有完成三次握手的连接，此时服务端处于 syn_rcvd 的状态；

一个是一件建立连接的队列，称为 TCP 全连接队列，这个队列都是完成了三次握手的连接，此时服务端处于 established 状态；

当 TCP 全连接队列不为空后，服务端的 accept() 函数，就会从内核中的 TCP 全连接队列里拿出一个已经完成连接的  Socket 返回应用程序，后续数据传输都用这个 Socket。

注意，监听的 Socket 和真正用来传数据的 Socket 是两个：

一个叫作监听 Socket；

一个叫作已连接 Socket；

连接建立后，客户端和服务端就开始相互传输数据了，双方都可以通过 read() 和 write() 函数来读写数据。

至此， TCP 协议的 Socket 程序的调用过程就结束了，整个过程如下图：

图片
看到这，不知道你有没有觉得读写 Socket  的方式，好像读写文件一样。

是的，基于 Linux 一切皆文件的理念，在内核中 Socket 也是以「文件」的形式存在的，也是有对应的文件描述符。

PS : 下面会说到内核里的数据结构，不感兴趣的可以跳过这一部分，不会对后续的内容有影响。

文件描述符的作用是什么？每一个进程都有一个数据结构 task_struct，该结构体里有一个指向「文件描述符数组」的成员指针。该数组里列出这个进程打开的所有文件的文件描述符。数组的下标是文件描述符，是一个整数，而数组的内容是一个指针，指向内核中所有打开的文件的列表，也就是说内核可以通过文件描述符找到对应打开的文件。

然后每个文件都有一个 inode，Socket 文件的 inode 指向了内核中的 Socket 结构，在这个结构体里有两个队列，分别是发送队列和接收队列，这个两个队列里面保存的是一个个 struct sk_buff，用链表的组织形式串起来。

sk_buff 可以表示各个层的数据包，在应用层数据包叫 data，在 TCP 层我们称为 segment，在 IP 层我们叫 packet，在数据链路层称为 frame。

你可能会好奇，为什么全部数据包只用一个结构体来描述呢？协议栈采用的是分层结构，上层向下层传递数据时需要增加包头，下层向上层数据时又需要去掉包头，如果每一层都用一个结构体，那在层之间传递数据的时候，就要发生多次拷贝，这将大大降低 CPU 效率。

于是，为了在层级之间传递数据时，不发生拷贝，只用 sk_buff 一个结构体来描述所有的网络包，那它是如何做到的呢？是通过调整 sk_buff 中 data 的指针，比如：

当接收报文时，从网卡驱动开始，通过协议栈层层往上传送数据报，通过增加 skb->data 的值，来逐步剥离协议首部。

当要发送报文时，创建 sk_buff 结构体，数据缓存区的头部预留足够的空间，用来填充各层首部，在经过各下层协议时，通过减少 skb->data 的值来增加协议首部。

你可以从下面这张图看到，当发送报文时，data 指针的移动过程。

图片
如何服务更多的用户？
前面提到的 TCP Socket 调用流程是最简单、最基本的，它基本只能一对一通信，因为使用的是同步阻塞的方式，当服务端在还没处理完一个客户端的网络 I/O 时，或者 读写操作发生阻塞时，其他客户端是无法与服务端连接的。

可如果我们服务器只能服务一个客户，那这样就太浪费资源了，于是我们要改进这个网络 I/O 模型，以支持更多的客户端。

在改进网络 I/O 模型前，我先来提一个问题，你知道服务器单机理论最大能连接多少个客户端？

相信你知道 TCP 连接是由四元组唯一确认的，这个四元组就是：本机IP, 本机端口, 对端IP, 对端端口。

服务器作为服务方，通常会在本地固定监听一个端口，等待客户端的连接。因此服务器的本地 IP 和端口是固定的，于是对于服务端 TCP 连接的四元组只有对端 IP 和端口是会变化的，所以最大 TCP 连接数 = 客户端 IP 数×客户端端口数。

对于 IPv4，客户端的 IP 数最多为 2 的 32 次方，客户端的端口数最多为 2 的 16 次方，也就是服务端单机最大 TCP 连接数约为 2 的 48 次方。

这个理论值相当“丰满”，但是服务器肯定承载不了那么大的连接数，主要会受两个方面的限制：

文件描述符，Socket 实际上是一个文件，也就会对应一个文件描述符。在 Linux 下，单个进程打开的文件描述符数是有限制的，没有经过修改的值一般都是 1024，不过我们可以通过 ulimit 增大文件描述符的数目；

系统内存，每个 TCP 连接在内核中都有对应的数据结构，意味着每个连接都是会占用一定内存的；

那如果服务器的内存只有 2 GB，网卡是千兆的，能支持并发 1 万请求吗？

并发 1 万请求，也就是经典的 C10K 问题 ，C 是 Client 单词首字母缩写，C10K 就是单机同时处理 1 万个请求的问题。

从硬件资源角度看，对于 2GB 内存千兆网卡的服务器，如果每个请求处理占用不到 200KB 的内存和 100Kbit 的网络带宽就可以满足并发 1 万个请求。

不过，要想真正实现 C10K 的服务器，要考虑的地方在于服务器的网络 I/O 模型，效率低的模型，会加重系统开销，从而会离 C10K 的目标越来越远。

多进程模型
基于最原始的阻塞网络 I/O， 如果服务器要支持多个客户端，其中比较传统的方式，就是使用多进程模型，也就是为每个客户端分配一个进程来处理请求。

服务器的主进程负责监听客户的连接，一旦与客户端连接完成，accept() 函数就会返回一个「已连接 Socket」，这时就通过 fork() 函数创建一个子进程，实际上就把父进程所有相关的东西都复制一份，包括文件描述符、内存地址空间、程序计数器、执行的代码等。

这两个进程刚复制完的时候，几乎一摸一样。不过，会根据返回值来区分是父进程还是子进程，如果返回值是 0，则是子进程；如果返回值是其他的整数，就是父进程。

正因为子进程会复制父进程的文件描述符，于是就可以直接使用「已连接 Socket 」和客户端通信了，

可以发现，子进程不需要关心「监听 Socket」，只需要关心「已连接 Socket」；父进程则相反，将客户服务交给子进程来处理，因此父进程不需要关心「已连接 Socket」，只需要关心「监听 Socket」。

下面这张图描述了从连接请求到连接建立，父进程创建生子进程为客户服务。

图片
另外，当「子进程」退出时，实际上内核里还会保留该进程的一些信息，也是会占用内存的，如果不做好“回收”工作，就会变成僵尸进程，随着僵尸进程越多，会慢慢耗尽我们的系统资源。

因此，父进程要“善后”好自己的孩子，怎么善后呢？那么有两种方式可以在子进程退出后回收资源，分别是调用 wait() 和 waitpid() 函数。

这种用多个进程来应付多个客户端的方式，在应对 100 个客户端还是可行的，但是当客户端数量高达一万时，肯定扛不住的，因为每产生一个进程，必会占据一定的系统资源，而且进程间上下文切换的“包袱”是很重的，性能会大打折扣。

进程的上下文切换不仅包含了虚拟内存、栈、全局变量等用户空间的资源，还包括了内核堆栈、寄存器等内核空间的资源。

多线程模型
既然进程间上下文切换的“包袱”很重，那我们就搞个比较轻量级的模型来应对多用户的请求 —— 多线程模型。

线程是运行在进程中的一个“逻辑流”，单进程中可以运行多个线程，同进程里的线程可以共享进程的部分资源的，比如文件描述符列表、进程空间、代码、全局数据、堆、共享库等，这些共享些资源在上下文切换时是不需要切换，而只需要切换线程的私有数据、寄存器等不共享的数据，因此同一个进程下的线程上下文切换的开销要比进程小得多。

当服务器与客户端 TCP 完成连接后，通过 pthread_create() 函数创建线程，然后将「已连接 Socket」的文件描述符传递给线程函数，接着在线程里和客户端进行通信，从而达到并发处理的目的。

如果每来一个连接就创建一个线程，线程运行完后，还得操作系统还得销毁线程，虽说线程切换的上写文开销不大，但是如果频繁创建和销毁线程，系统开销也是不小的。

那么，我们可以使用线程池的方式来避免线程的频繁创建和销毁，所谓的线程池，就是提前创建若干个线程，这样当由新连接建立时，将这个已连接的 Socket 放入到一个队列里，然后线程池里的线程负责从队列中取出已连接 Socket 进程处理。

图片
需要注意的是，这个队列是全局的，每个线程都会操作，为了避免多线程竞争，线程在操作这个队列前要加锁。

上面基于进程或者线程模型的，其实还是有问题的。新到来一个 TCP 连接，就需要分配一个进程或者线程，那么如果要达到 C10K，意味着要一台机器维护 1 万个连接，相当于要维护 1 万个进程/线程，操作系统就算死扛也是扛不住的。

I/O 多路复用
既然为每个请求分配一个进程/线程的方式不合适，那有没有可能只使用一个进程来维护多个 Socket 呢？答案是有的，那就是 I/O 多路复用技术。

图片
一个进程虽然任一时刻只能处理一个请求，但是处理每个请求的事件时，耗时控制在 1 毫秒以内，这样 1 秒内就可以处理上千个请求，把时间拉长来看，多个请求复用了一个进程，这就是多路复用，这种思想很类似一个 CPU 并发多个进程，所以也叫做时分多路复用。

我们熟悉的 select/poll/epoll 内核提供给用户态的多路复用系统调用，进程可以通过一个系统调用函数从内核中获取多个事件。

select/poll/epoll 是如何获取网络事件的呢？在获取事件时，先把所有连接（文件描述符）传给内核，再由内核返回产生了事件的连接，然后在用户态中再处理这些连接对应的请求即可。

select/poll/epoll 这是三个多路复用接口，都能实现 C10K 吗？接下来，我们分别说说它们。

select/poll
select 实现多路复用的方式是，将已连接的 Socket 都放到一个文件描述符集合，然后调用 select 函数将文件描述符集合拷贝到内核里，让内核来检查是否有网络事件产生，检查的方式很粗暴，就是通过遍历文件描述符集合的方式，当检查到有事件产生后，将此 Socket 标记为可读或可写， 接着再把整个文件描述符集合拷贝回用户态里，然后用户态还需要再通过遍历的方法找到可读或可写的 Socket，然后再对其处理。

所以，对于 select 这种方式，需要进行 2 次「遍历」文件描述符集合，一次是在内核态里，一个次是在用户态里 ，而且还会发生 2 次「拷贝」文件描述符集合，先从用户空间传入内核空间，由内核修改后，再传出到用户空间中。

select 使用固定长度的 BitsMap，表示文件描述符集合，而且所支持的文件描述符的个数是有限制的，在 Linux 系统中，由内核中的 FD_SETSIZE 限制， 默认最大值为 1024，只能监听 0~1023 的文件描述符。

poll 不再用 BitsMap 来存储所关注的文件描述符，取而代之用动态数组，以链表形式来组织，突破了 select 的文件描述符个数限制，当然还会受到系统文件描述符限制。

但是 poll 和 select 并没有太大的本质区别，都是使用「线性结构」存储进程关注的 Socket 集合，因此都需要遍历文件描述符集合来找到可读或可写的 Socket，时间复杂度为 O(n)，而且也需要在用户态与内核态之间拷贝文件描述符集合，这种方式随着并发数上来，性能的损耗会呈指数级增长。

epoll
epoll 通过两个方面，很好解决了 select/poll 的问题。

第一点，epoll 在内核里使用红黑树来跟踪进程所有待检测的文件描述字，把需要监控的 socket 通过 epoll_ctl() 函数加入内核中的红黑树里，红黑树是个高效的数据结构，增删查一般时间复杂度是 O(logn)，通过对这棵黑红树进行操作，这样就不需要像 select/poll 每次操作时都传入整个 socket 集合，只需要传入一个待检测的 socket，减少了内核和用户空间大量的数据拷贝和内存分配。

第二点， epoll 使用事件驱动的机制，内核里维护了一个链表来记录就绪事件，当某个 socket 有事件发生时，通过回调函数内核会将其加入到这个就绪事件列表中，当用户调用 epoll_wait() 函数时，只会返回有事件发生的文件描述符的个数，不需要像 select/poll 那样轮询扫描整个 socket 集合，大大提高了检测的效率。

从下图你可以看到 epoll 相关的接口作用：

图片
epoll 的方式即使监听的 Socket 数量越多的时候，效率不会大幅度降低，能够同时监听的 Socket 的数目也非常的多了，上限就为系统定义的进程打开的最大文件描述符个数。因而，epoll 被称为解决 C10K 问题的利器。

插个题外话，网上文章不少说，epoll_wait 返回时，对于就绪的事件，epoll使用的是共享内存的方式，即用户态和内核态都指向了就绪链表，所以就避免了内存拷贝消耗。

这是错的！看过 epoll 内核源码的都知道，压根就没有使用共享内存这个玩意。你可以从下面这份代码看到， epoll_wait 实现的内核代码中调用了 __put_user 函数，这个函数就是将数据从内核拷贝到用户空间。

图片
好了，这个题外话就说到这了，我们继续！

epoll 支持两种事件触发模式，分别是边缘触发（edge-triggered，ET）和水平触发（level-triggered，LT）。

这两个术语还挺抽象的，其实它们的区别还是很好理解的。

使用边缘触发模式时，当被监控的 Socket 描述符上有可读事件发生时，服务器端只会从 epoll_wait 中苏醒一次，即使进程没有调用 read 函数从内核读取数据，也依然只苏醒一次，因此我们程序要保证一次性将内核缓冲区的数据读取完；

使用水平触发模式时，当被监控的 Socket 上有可读事件发生时，服务器端不断地从 epoll_wait 中苏醒，直到内核缓冲区数据被 read 函数读完才结束，目的是告诉我们有数据需要读取；

举个例子，你的快递被放到了一个快递箱里，如果快递箱只会通过短信通知你一次，即使你一直没有去取，它也不会再发送第二条短信提醒你，这个方式就是边缘触发；如果快递箱发现你的快递没有被取出，它就会不停地发短信通知你，直到你取出了快递，它才消停，这个就是水平触发的方式。

这就是两者的区别，水平触发的意思是只要满足事件的条件，比如内核中有数据需要读，就一直不断地把这个事件传递给用户；而边缘触发的意思是只有第一次满足条件的时候才触发，之后就不会再传递同样的事件了。

如果使用水平触发模式，当内核通知文件描述符可读写时，接下来还可以继续去检测它的状态，看它是否依然可读或可写。所以在收到通知后，没必要一次执行尽可能多的读写操作。

如果使用边缘触发模式，I/O 事件发生时只会通知一次，而且我们不知道到底能读写多少数据，所以在收到通知后应尽可能地读写数据，以免错失读写的机会。因此，我们会循环从文件描述符读写数据，那么如果文件描述符是阻塞的，没有数据可读写时，进程会阻塞在读写函数那里，程序就没办法继续往下执行。所以，边缘触发模式一般和非阻塞 I/O 搭配使用，程序会一直执行 I/O 操作，直到系统调用（如 read 和 write）返回错误，错误类型为 EAGAIN 或 EWOULDBLOCK。

一般来说，边缘触发的效率比水平触发的效率要高，因为边缘触发可以减少 epoll_wait 的系统调用次数，系统调用也是有一定的开销的的，毕竟也存在上下文的切换。

select/poll 只有水平触发模式，epoll 默认的触发模式是水平触发，但是可以根据应用场景设置为边缘触发模式。

另外，使用 I/O 多路复用时，最好搭配非阻塞 I/O 一起使用，Linux 手册关于 select 的内容中有如下说明：

Under Linux, select() may report a socket file descriptor as "ready for reading", while nevertheless a subsequent read blocks. This could for example happen when data has arrived but upon examination has wrong checksum and is discarded. There may be other circumstances in which a file descriptor is spuriously reported as ready. Thus it may be safer to use O_NONBLOCK on sockets that should not block.

我谷歌翻译的结果：

在Linux下，select() 可能会将一个 socket 文件描述符报告为 "准备读取"，而后续的读取块却没有。例如，当数据已经到达，但经检查后发现有错误的校验和而被丢弃时，就会发生这种情况。也有可能在其他情况下，文件描述符被错误地报告为就绪。因此，在不应该阻塞的 socket 上使用 O_NONBLOCK 可能更安全。

简单点理解，就是多路复用 API 返回的事件并不一定可读写的，如果使用阻塞 I/O， 那么在调用 read/write 时则会发生程序阻塞，因此最好搭配非阻塞 I/O，以便应对极少数的特殊情况。

总结
最基础的 TCP 的 Socket 编程，它是阻塞 I/O 模型，基本上只能一对一通信，那为了服务更多的客户端，我们需要改进网络 I/O 模型。

比较传统的方式是使用多进程/线程模型，每来一个客户端连接，就分配一个进程/线程，然后后续的读写都在对应的进程/线程，这种方式处理 100 个客户端没问题，但是当客户端增大到 10000  个时，10000 个进程/线程的调度、上下文切换以及它们占用的内存，都会成为瓶颈。

为了解决上面这个问题，就出现了 I/O 的多路复用，可以只在一个进程里处理多个文件的  I/O，Linux 下有三种提供 I/O 多路复用的 API，分别是：select、poll、epoll。

select 和 poll 并没有本质区别，它们内部都是使用「线性结构」来存储进程关注的 Socket 集合。

在使用的时候，首先需要把关注的 Socket 集合通过 select/poll 系统调用从用户态拷贝到内核态，然后由内核检测事件，当有网络事件产生时，内核需要遍历进程关注 Socket 集合，找到对应的 Socket，并设置其状态为可读/可写，然后把整个 Socket 集合从内核态拷贝到用户态，用户态还要继续遍历整个 Socket 集合找到可读/可写的 Socket，然后对其处理。

很明显发现，select 和 poll 的缺陷在于，当客户端越多，也就是 Socket 集合越大，Socket 集合的遍历和拷贝会带来很大的开销，因此也很难应对 C10K。

epoll 是解决 C10K 问题的利器，通过两个方面解决了 select/poll 的问题。

epoll 在内核里使用「红黑树」来关注进程所有待检测的 Socket，红黑树是个高效的数据结构，增删查一般时间复杂度是 O(logn)，通过对这棵黑红树的管理，不需要像 select/poll 在每次操作时都传入整个 Socket 集合，减少了内核和用户空间大量的数据拷贝和内存分配。

epoll 使用事件驱动的机制，内核里维护了一个「链表」来记录就绪事件，只将有事件发生的 Socket 集合传递给应用程序，不需要像 select/poll 那样轮询扫描整个集合（包含有和无事件的 Socket ），大大提高了检测的效率。

而且，epoll 支持边缘触发和水平触发的方式，而 select/poll 只支持水平触发，一般而言，边缘触发的方式会比水平触发的效率高。

### 操作系统定时时间描述符

#### timerfd简要介绍

timerfd的特点是有一个与之关联fd，可绑定Channel，交由Poller监听感兴趣的事件（读、写等）。
timerfd 3个接口： timerfd_create，timerfd_settime，timerfd_gettime。

```c++
#include <sys/timerfd.h>

/* 创建一个定时器对象, 返回与之关联的fd
* clockid 可指定为CLOCK_REALTIME（系统范围时钟）或CLOCK_MONOTONIC（不可设置的时钟，不能手动修改）
* flags 可指定为TFD_NONBLOCK（为fd设置O_NONBLOCK），TFD_CLOEXEC（为fd设置close-on-exec）
*/
int timerfd_create(int clockid, int flags);

/* 启动或停止绑定到fd的定时器
 * flags 指定0：启动一个相对定时器，由new_value->it_value指定相对定时值；TFD_TIMER_ABSTIME启动一个绝对定时器，由new_value->it_value指定定时值
 * old_value 保存旧定时值
 */
int timerfd_settime(int fd, int flags, const struct itimerspec *new_value, struct itimerspec *old_value);

/* 获取fd对应定时器的当前时间值  */
int timerfd_gettime(int fd, struct itimerspec *curr_value);
```

#### 如何度量程序在某一时刻的时间？

通常，我们用时刻来表示，比如"2022-02-26 23:43:00.000000"，这种方式便于人查看，但不便于程序中的比较和计算。比如有2个时刻A和B，计算哪个时刻在前，哪个在后，或者要计算时刻A和B的时间差时，这种字符串表示方式就很麻烦。
我们想到将字符串形式的时刻，用自纪元时间（Epoch时间，1970-01-01 00:00:00 +0000 (UTC)）以来的时间戳来表示，精度为1us（微秒）。

#### Linux中，如何获取这个时间呢？

使用gettimeofday(2)，分辨率1us，其实现也能达到毫秒级（当然分辨率不等于精度），再加上Linux是非实时任务系统，也能满足日常计时功能。
前面讲过，time(2)只能精确到1s，ftime(3)已被废弃，clock_gettime(2)精度高，但系统调用开销比gettimeofday(2)大，网络编程中，最适合用gettimeofday(2)来计时。muduo中也是这么做的。

有没有一种可能，两个线程，或者两段出现在1us内执行？
答案是有可能的，对于常规情况，即使时间戳相同，并不影响我们的日常计时功能；对于特殊需求，比如排序、查找，需要区分时间戳大小的，后面遇到具体情况具体分析。

### eventfd

eventfd 是 Linux 特有的专用于事件通知的文件描述符。创建后，可用 read(2) 读取 eventfd，若 eventfd 是阻塞的，read(2) 可能阻塞线程；若 eventfd 设置了 EFD_NONBLOCK，read(2) 返回 EAGIAN 错误。直到另外一个线程对 eventfd 进行 write(2) 后才可正常返回。[12]

### 线程池技术

在并发环境下，系统不能够确定在任意时刻中，有多少任务需要执行，有多少资源需要投入。这种不确定性将带来以下若干问题：

+ 频繁申请、销毁资源和调度资源，将带来额外的消耗，可能会非常巨大。
+ 对资源无限申请缺少抑制手段，易引发系统资源耗尽的风险。
+ 系统无法合理管理内部的资源分布，会降低系统的稳定性。

为解决以上问题，线程池应运而生。线程池维护多个线程，等待监督管理者分配可并发执行的任务。这种做法，相比于一般的线程管理方式有以下四点优势：

+ 降低资源消耗：通过池化技术重复利用已创建的线程，降低线程创建和销毁造成的损耗。
+ 提高响应速度：任务到达时，无需等待线程创建即可立即执行。
+ 提高线程的可管理性：线程是稀缺资源，如果无限制创建，不仅会消耗系统资源，还会因为线程的不合理分布导致资源调度失衡，降低系统的稳定性。使用线程池可以进行统一的分配、调优和监控，保证了对内核的充分有效的利用。
+ 提供更多更强大的功能：线程池具备可拓展性，允许开发人员向其中增加更多的功能。如延时定时线程池，就允许任务延期执行或定期执行。[11]

## 整体架构设计

### 网络服务器软件需求分析

结合网络服务器软件的现实需求、行业标准、开源社区相关讨论等，一个优秀的网络服务器软件应该满足以下需求[1]：

+ 网络服务器软件能够在操作系统的底层能力支持下，完成以下两个主要任务：
  + 接收并管理来自于不同网络地址，且携带不同网络信息的网络连接，同时为它们分配操作系统资源并协调它们之间的工作；
  + 最大限度地满足各类用户的网络服务要求，及时的响应用户服务请求，为互联网服务用户及时的提供所需要的服务信息，包括但不限于请求的回复报文、静态存储的文件、操作系统的状态等。
+ 网络服务器软件在架构上应该足够抽象，以达成可以利用配置文件、中间件或者插件、软件提供相关配置选项的 API 、甚至于将网络服务器软件作为底层，在上层编写相关代码来实现完成特定网络服务的网络服务器软件等不同方式来适应不同的操作系统、多样的网络协议和网络服务需求、日益增长的网络服务用户数量等任务。
+ 网络服务器软件应该为软件开发者提供友好的编程接口和符合人体工程学的编程范式，同时提供详细且易于理解的使用样例及相关文档。
+ 网络服务器软件在现实中可能出现的各种异常情况下应该通过良好的异常处理机制——包括但不限于日志记录、告警处理、自动恢复等方式来保证网络服务不会被可恢复异常中断，且在发生不可恢复异常时能够提供相关信息以帮助软件开发者快速定位问题，降低不可恢复异常所造成的损失。

### 系统架构设计

基于 3.1 节的需求分析，同时结合了其他优秀的网络服务器软件开源项目的架构（Netpoll[2]、Netty[3]、muduo[4] 等），设计了本网络服务器软件的架构。网络服务器软件整体架构图如图 4-1 所示。网络服务器软件整体架构分为操作系统、公共组件、事件分发、网络协议、用户五层，下面将介绍每一层和相关模块的主要功能。

![img](./image/%E6%9E%B6%E6%9E%84%E5%9B%BE.png)

#### 操作系统

操作系统为计算机硬件提供了抽象和管理，为用户和应用程序提供了接口与服务[5]，但在不同类型的操作系统上相同功能的实现细节、所需的系统调用以及编程上的最佳实践都千差万别，这种差异使得软件的跨平台适配大多都极其繁琐。考虑到网络服务器领域的现实需求和个人能力，本网络服务器软件在架构和实现上都为类 UNIX 系统的适配留下相应接口，但只对 Linux 系统实现完整的网络服务器软件功能。

具体到本服务器软件，类 UNIX 操作系统应当能够为其提供以下接口与服务支持：

+ 网络通信的基础支持，包括网络协议栈、Socket 接口等功能。
+ I/O 多路复用机制。
+ 线程和进程管理机制。
+ 文件系统能够访问并修改磁盘或其他存储设备上的数据。
+ 接收时钟的定时中断，获取时钟的时间信息。

#### 公共组件

公共组件将操作系统提供的接口与服务封装成功能模块，以便于网络服务器软件上层能够忽略底层操作系统的系统调用细节，通过更友好的编程接口来使用操作系统所提供的接口和服务支持，同时为上层应用提供如异步日志等通用功能组件的接口支持。

公共组件各个功能模块的具体功能介绍如下：

+ 网络：对常用网络编程系统接口的封装，包括 Socket、InetAddress、Buffer 部分。

  + Socket 部分是对 Socket 文件描述符及其各种常用操作的封装，如创建 Socket、绑定地址、监听、接受连接等。
  + InetAddress 部分是对网络地址及其相关操作的封装，可以获取本端和对端的 ip 地址和端口信息，也可以传入 ip 地址和端口信息用来设置 Socket 文件描述符所绑定的地址。
  + Buffer 部分是网络服务器软件数据缓冲区的实现。由于网络服务器软件所使用到的网络协议（主要是 TCP 协议）大多是无边界的字节流协议[6]，可能会发生读取或写入数据时不能通过一次系统调用完成的情况，这时就需要缓冲区来承接这部分数据以保证程序不会因此阻塞——例如服务器发送 100 字节的数据，但使用系统调用后只写入 80 字节，在缓冲区存在的情况下，服务器软件可以将剩余数据放入缓冲区，并将 Socket 是否可写列入文件描述符监听队列，交出线程控制权，直至 Socket 可写后激活线程并发送缓冲区数据。
+ I/O 复用：对 I/O 复用系统接口的封装，包括 Channel、Poller、EpollPoller 部分。

  + Channel 部分是对文件描述符的封装，每个 Channel 对象与一个文件描述符对应，其中定义了该文件描述符的可监听的事件、事件的回调函数等。
  + Poller 部分抽象了 I/O 多路复用的操作，如注册事件、删除事件等；并提供实现接口，为在其它类 UNIX 系统下实现本网络服务器软件可用的 I/O 多路复用功能给出了符合人体工程学的编程范式。
  + EpollPoller 部分是对 Linux 下 epoll 相关的 I/O 多路复用接口的封装，继承自 Poller 类，补全了接口函数，实现了 Linux 下本网络服务器软件可用的 epoll I/O多路复用功能。
+ Timestamp：本网络服务器软件的微秒级时间戳部分，具有以下功能：

  + 通过操作系统获取当前微秒级时间戳。
  + 可比较时间戳先后顺序、计算时间戳之间的差值、计算某个时间戳在一定时间后的另一个时间戳等。
  + 将时间戳转换成为形如“2023/04/24 23:41:31”格式的时间字符串。
+ Thread：对操作系统线程管理能力的封装，有创建、销毁线程等功能。
+ 定时器：对操作系统接收定时中断能力的封装，且能够在收到定时中断后处理对应事件。包括 Timer、TimerQueue 部分。

  + Timer 部分定义了定时任务的属性以及对其进行读取和修改的函数方法。
  + TimerQueue 部分是定时器队列类，实现了对定时器队列的管理，如添加、删除定时事件等。
+ 异步日志：将代码运行时的重要信息进行保存，方便故障诊断和追踪，包括日志前端和日志后端。

  + 日志前端主要为开发人员提供异步日志接口，能够使其按照等级写入日志并在控制台输出日志信息。包括 Logger、LogStream 部分。
    + Logger 部分为用户提供日志库的接口，能够设置日志等级、控制日志输出流等。
    + LogStream 部分主要提供日志流操作，将用户提供的整型数、浮点数、字符、字符串、字符数组、二进制内存、另一个日志缓冲区，格式化为字符串，并加入当前流的日志缓冲区。
  + 日志后端将前端写下的日志写入文件，放入计算机存储设备中使其持久化。包括 FileUtil、LogFile、AsyncLogging 部分。
    + FileUtil 部分封装操作系统提供的底层的创建、打开文件，写文件，关闭文件等操作接口。
    + LogFile 部分提供对日志文件的操作，包括滚动日志文件、将日志数据写到当前日志文件、刷新日志数据到当前日志文件。
    + AsynccLogging 部分提供缓冲存放多条日志信息，为前端线程提供线程安全的写日志操作；提供专门的后端线程，用于定时或缓冲非空时，将缓冲中日志信息逐个写到磁盘上。

#### 事件分发

Linux 在 5.1 版本才支持可应用于网络编程的异步 I/O 系统调用—— io_uring，此前仅支持同步阻塞或非阻塞的网络 I/O 系统调用[8]。故基于 2.4 节的介绍和 3.1.1 节的操作系统取舍，本网络服务器软件将采用 Reactor 模式作为事件分发部分的事件驱动模式。

事件分发部分作为网络服务器软件的核心，其在内部实现了 Reactor 模式，向上层代码提供了一组符合直觉的事件驱动调用接口，开发人员可以基于该接口适配其他网络协议。事件分发包括 EventLoop、EventLoopThread、EventLoopThreadPool 三部分。

+ EventLoop 部分是事件循环类，负责接收 Channel 对象上的事件并将其分发到对应的事件处理函数中。
+ EventLoopThread 部分是封装了一个新线程，并在该线程上创建一个 EventLoop 对象，用于处理事件。
+ EventLoopThreadPool 部分是封装了一个线程池，用于创建一组 EventLoopThread 对象，以此提高并发处理能力和操作系统资源利用率。

#### 网络协议

网络协议基于事件分发部分的接口，实现特定网络通信协议的特性，为应用程序提供网络协议支持。开发人员也可以在本部分适配其他的网络协议或特性。本网络服务器软件实现了 TCP 协议和 HTTP 协议。

+ TCP 部分实现对 TCP 连接的管理。包括 TcpConnection、TcpServer、Acceptor 部分。

  + TcpConnection 部分是对一条 TCP 连接的封装，包括读写数据、关闭连接等。
  + TcpServer 部分是对 TCP 服务器的封装，负责监听并处理新的 TCP 连接。
  + Acceptor 部分用于接收来自其他网络地址的 TCP 网络连接并将其分发至线程池中经过分配策略所决定的工作线程。
+ HTTP 部分实现 HTTP 协议中的 GET 和 POST 请求， 实现了静态文件传输和下载的功能。包括 HttpServer、HttpContext、HttpResponse、HttpRequest 部分。

  + HttpServer 部分是对 HTTP 服务器的封装，负责监听并处理新的 HTTP 连接。
  + HttpContext 部分是对 HTTP 报文解析的封装，负责解析收到的 HTTP 报文信息。
  + HttpResponse 部分是对 HTTP 请求类的封装，负责管理 HttpContext 解析完成的报文信息。
  + HttpRequest 部分是对 HTTP 响应类的封装，负责管理服务器响应报文的数据并将其格式化为字符串报文。

#### 用户

用户部分使得使用该系统的各类用户，包括软件开发人员和网络服务的使用者，它们可以通过软件开发人员在这一部分实现的业务代码，从而使网络服务的使用者得到其所需要的服务数据。

### 处理流程

本网络服务器软件存在三个业务处理流程：网络连接处理流程、异步日志处理流程、定时器处理流程。

#### 网络连接处理流程

网络服务器软件的本质是处理四个事件[4]，即：

+ 建立连接。
+ 断开连接。
+ 读取网络信息。
+ 发送网络消息。

故网络协议之间细节可能不同，但流程都大致相似，故下文将以 TCP 协议为例介绍本网络服务器软件的网络连接处理流程。图 4-2 为本网络服务器软件在 TCP 协议下的处理流程图。

![img](./image/TCP%E7%BD%91%E7%BB%9C%E8%BF%9E%E6%8E%A5%E6%B5%81%E7%A8%8B%E5%9B%BE.png)

##### 建立连接

使用 Socket 来创建一个简单的 TCP 服务器时，建立一个连接通常需要四个步骤：

+ 调用 socket 函数建立监听 socket。
+ 调用 bind 函数绑定需要监听的地址和端口。
+ 调用 listen 函数监听端口。
+ 调用 accept 函数返回新建立连接的文件描述符。

而在本网络服务器软件中，也基本遵循了以上步骤：

+ 构造一个事件循环器 EventLoop。
+ 建立对应的业务服务器 TcpServer。在构造 TcpServer 时，会创建 Acceptor 对象，在 Acceptor 对象的构造函数中，会执行 socket 函数和 bind 函数。
+ 然后 main 函数执行 TcpServer 的 TcpServer::start() 方法完成以下两个任务——启动线程池；调用 listen 函数，并将 listen 所监听的文件描述符添加到可读事件的监听队列中。
+ 设置 TcpServer 的对应的回调函数。
+ main 函数会执行 EventLoop::loop() 方法来启动事件循环。在事件循环中，当有连接到来后，会执行 Acceptor 中 handleRead 回调函数，进而调用 TcpServer 中的 newConnection 回调函数，执行 accept 函数并生成对应的文件描述符并将其绑定至一个 TcpConnection 实例中，并注册相应的回调函数。随后将文件描述符分发至线程池中的特定子线程的事件循环中。

##### 读取网络信息

读取网络信息主要有以下两个步骤：

+ 在子线程的事件循环中，当连接中有数据送达时，就会调用 TcpConnection 中的 handleRead 回调函数，将连接送达的数据放入请求数据缓冲区（inputBuffer）中。
+ 调用用户注册的 onMessage 回调函数，执行网络服务的业务逻辑。

##### 发送网络信息

在用户注册的 onMessage 回调中会调用 TcpConnection::send() 方法来发送网络信息，这个方法会调用 TcpConnection::sendInLoop() ，其处理过程如下：

+ 若响应数据缓冲区（outputBuffer） 为空，则直接向网络连接文件描述符写数据。
+ 若向网络连接文件描述符写的数据未写完，则记录剩余的字节数。
+ 若此时响应数据缓冲区（outputBuffer）中的旧数据的个数和未写完字节个数和大于高水位表记值（highWaterMark） ，则调用回调函数 highWaterMarkCallback。否则将剩余数据写入响应数据缓冲区（outputBuffer）。
+ 若向网络连接文件描述符写的数据没有写完，则最后需要为其注册可写事件。当其可写事件发生时，调用对应的回调 TcpConnection::handleWrite()，尽可能将数据从响应数据缓冲区（outputBuffer）中向网络连接文件描述符中写数据。
+ 如果响应数据缓冲区（outputBuffer）中的数据都写完了，将网络连接文件描述符的写事件移除，并调用写完成回调 writeCompleteCallback。

##### 断开连接

断开连接分为被动断开和主动断开。主动断开和被动断开的处理流程基本一致，所以被动断开为例介绍断开连接的流程。

被动断开即客户端断开了连接，服务端需要感知到这个断开的状态，然后进行的相关的处理。其中感知远程断开这一步是在 TCP 连接的可读事件处理函数 TcpConnection::handleRead() 中进行的：当对网络连接文件描述符进行 read 操作时，返回值为 0，则说明此时连接已断开。然后调用 TcpConnection::handleClose() 函数。其执行过程如下：

+ 将网络连接文件描述符从监听队列中移除。
+ 调用用户的设置的 onConnection() 函数。
+ 将网络连接文件描述符对应的 TcpConnection 对象从 TcpServer 中移除。
+ 关闭网络连接文件描述符。

#### 异步日志处理流程

#### 定时器处理流程

## 主要模块的设计与实现

### 公共组件

#### 网络

##### Socket类

Socket类是socket 文件描述符（sock fd）的一个轻量级封装，提供操作底层sock fd的常用方法。采用RTII方式管理sock fd，但本身并不创建sock fd，也不打开它，只负责关闭。

提供的public方法主要包括：获取tcp协议栈信息（tcp_info）；绑定ip地址（bind）；监听套接字（listen）；接收连接请求（accept）；关闭连接写方向（shutdown），等等。
值得注意的是，Socket并不提供close sock fd的public方法，因为析构时，调用close关闭套接字。

##### Socket的构造与析构

Socket构造和析构很简单：

Socket类不创建sockfd，其含义取决于构造Socket对象的调用者。如果是由调用socket(2)创建的sockfd，那就是本地套接字；如果是由accept(2)返回的sockfd，那就是accepted socket，代表一个连接。

例如，Acceptor持有的Socket对象，是由socket(2)创建的，代表一个套接字；
TcpConnection只有一个Socket对象，是由TcpServer在新建TcpConnection对象时传入，而由Acceptor::handleRead()中通过Socket::accept()创建的sockfd参数的实参。

##### Socket获取Tcp协议栈信息

利用getsockopt + TCP_INFO选项，获取tcp协议栈信息。
方法一：getTcpInfo 获取tcp协议栈信息，存放到tcp_info结构对象；
方法二：getTcpInfoString 获取tcp协议栈字符串形式，存放到数组buf[len]；

##### 其他常用接口

比如bindAddress、listen、accept等，与基础网络编程接口bind、listen、accept类似，不过由SocketOps进行轻度包裹。

值得一提的是setTcpNoDelay()，用于设置TCP_NODELAY选项，以禁用Nagle算法，从而不会等到收到ACK才进行下一次数据发送，而是tcp协议栈缓冲存中有数据就立即发送。

##### InetAddress类

InetAddress类对地址信息进行了包装，是sockaddr_in的包装类。

**既然是表示ip地址，可以直接用sockaddr_in，为什么要用InetAddress重新包装一下？**
因为表示IPv4地址的sockaddr_in是C语言数据类型，并不包含对数据类型的操作，另外，支持IPv6的地址结构是sockaddr_in6。
如果直接使用C风格的sockaddr_in，那么其他类要用来表示地址，不得不使用大量底层C接口。而使用C++ 类InetAddress包装sockaddr_in/sockaddr_in6，提供必要的C++接口，可以有效解决参数兼容问题。

InetAddress 声明如下：

InetAddress是值语义的，便于在传递时拷贝。数据成员是一个union，对于IPv4，使用addr_；对于IPv6，则使用addr6_。

5个静态断言（static_assert）确保数据成员大小及联合体内部位段偏移，因为后面会直接将sockaddr_in6转换为sockaddr_in。

##### InetAddress构造

```c++
static const in_addr_t kInaddrAny = INADDR_ANY;
static const in_addr_t kInaddrLoopback = INADDR_LOOPBACK;

/**
* 构造InetAddress对象
* @param portArg 端口号
* @param loopbackOnly　决定是否为回环地址
* @param ipv6　决定是否为ipv6地址
* @note 注意addr_/addr6_中存放的是网络字节序
*/
InetAddress::InetAddress(uint16_t portArg, bool loopbackOnly, bool ipv6)
{
    // 确保addr6_/addr_ 在InetAddress class内存中的偏移
    static_assert(offsetof(InetAddress, addr6_) == 0, "addr6_ offset 0");
    static_assert(offsetof(InetAddress, addr_) == 0, "addr_ offset 0");
    if (ipv6)
    { // ipv6地址
        memZero(&addr6_, sizeof(addr6_));
        addr6_.sin6_family = AF_INET6;
        // in6addr_loopback: 回环地址; in6addr_any： 任意地址
        in6_addr ip = loopbackOnly ? in6addr_loopback : in6addr_any;
        addr6_.sin6_addr = ip; // ip地址
        addr6_.sin6_port = sockets::hostToNetwork16(portArg); // 端口号
    }
    else
    { // ipv4地址
        memZero(&addr_, sizeof(addr_));
        addr_.sin_family = AF_INET;
        // kInaddrLoopback: 回环地址; kInaddrAny： 任意地址
        in_addr_t ip = loopbackOnly ? kInaddrLoopback : kInaddrAny;
        addr_.sin_addr.s_addr = sockets::hostToNetwork32(ip); // ip地址
        addr_.sin_port = sockets::hostToNetwork16(portArg);   // 端口号
    }
}

/*
* 根据ip地址字符串形式 + 端口号, 构造InetAddress对象
* IPv6: 2409:8a4c:662f:2900:9b2:63:9618:56c0
* IPv4: 127.0.0.1
*/
InetAddress::InetAddress(StringArg ip, uint16_t portArg, bool ipv6)
{
    if (ipv6 || strchr(ip.c_str(), ':'))
    { // 指定为ipv6地址类型, 或ip地址字符串中包含':'
        memZero(&addr6_, sizeof(addr6_));
        sockets::fromIpPort(ip.c_str(), portArg, &addr6_); // 将冒号16进制表示的ip地址+port, 转化为addr6_结构
    }
    else
    {
        memZero(&addr_, sizeof(addr_));
        sockets::fromIpPort(ip.c_str(), portArg, &addr_); // 将冒号16进制表示的ip地址+port, 转化为addr6_结构
    }
}

/* 根据sockaddr_in构造InetAddress对象 */
explicit InetAddress(const struct sockaddr_in& addr)
        : addr_(addr)
{ }

/* 根据sockaddr_in6构造InetAddress对象 */
explicit InetAddress(const struct sockaddr_in6& addr)
        : addr6_(addr)
{ }
```

##### 将IP地址信息转换为字符串

将IP地址、端口号转换为字符串形式，这种打印log、debug的时候，是需要常用的方法，可以调用toIp(), toIpPort()。

##### 将主机名或IPv4地址转换为InetAddress结构对象

可以用InetAddress::resolve

##### SocketsOps模块

SocketsOps准确来说是一个模块，而不是一个class，在sockets命名空间封装了系统底层提供的socket操作，比如socket(), bind(), listen(), accept(), connect(), close(), read(), readv(), write(), close(), shutdown()等等。

包裹函数的主要意义是为函数提供基本的出错处理，避免每次调用都要重写一次异常处理，使之更容易融入程序的框架。

我把包裹的函数分为4类：
1）基础的sock fd操作
2）便于交互的转换操作
3）地址类型转型
4）协议栈信息

```c++
// 基础的sock fd操作

/* 包裹socket(2), 创建非阻塞sockfd. 失败终止程序(LOG_SYSFATAL) */
int createNonblockingOrDie(sa_family_t family);

/* 包裹connect(2), 连接指定对端地址(addr) */
int connect(int sockfd, const struct sockaddr* addr);
/* 包裹bind(2), 绑定本地sockfd与本地ip地址addr. 失败终止程序(LOG_SYSFATAL) */
void bindOrDie(int sockfd, const struct sockaddr* addr);
/* 包裹listen(2), 监听本地sockfd. 失败终止程序(LOG_SYSFATAL) */
void listenOrDie(int sockfd);
/* 包裹accept(2)/accept4(2), 接受客户端请求连接, 返回连接sockfd */
int accept(int sockfd, struct sockaddr_in6* addr);
/* 包裹read(2), 从sockfd读取数据, 存放到数组buf[count] */
ssize_t read(int sockfd, void* buf, size_t count);
/* 包裹readv(2), 从sockfd读取数据, 存放到不连续内存iov[iovcnt]中 */
ssize_t readv(int sockfd, const struct iovec* iov, int iovcnt);
/* 包裹write(2), 将buf[count]中的数据写到sockfd连接 */
ssize_t write(int sockfd, const void* buf, size_t count);
/* 包裹close(2), 关闭sockfd */
void close(int sockfd);
/* 包裹shutdown(8), 关闭连接写方向 */
void shutdownWrite(int sockfd);

// 便于交互的转换操作

/* 将地址addr中包含的ip地址+port信息转换为C风格字符串, 存放到数组buf[size] */
void toIpPort(char* buf, size_t size, const struct sockaddr* addr);
/* 将地址addr中包含的ip地址转换为C风格字符串, 存放到数组buf[size] */
void toIp(char* buf, size_t size, const struct sockaddr* addr);

/* 将参数ip, port转换为sockaddr_in结构的addr */
void fromIpPort(const char* ip, uint16_t port, struct sockaddr_in* addr);
/* 将参数ip, port转换为sockaddr_in6结构的addr */
void fromIpPort(const char* ip, uint16_t port, struct sockaddr_in6* addr);


// 地址类型转型

/* const sockaddr_in转型为const sockaddr　*/
const struct sockaddr* sockaddr_cast(const struct sockaddr_in* addr);
/* const sockaddr_in6转型为const sockaddr　*/
const struct sockaddr* sockaddr_cast(const struct sockaddr_in6* addr);
/* sockaddr_in6转型为sockaddr　*/
struct sockaddr* sockaddr_cast(struct sockaddr_in6* addr);
/* const sockaddr转型为const sockaddr_in */
const struct sockaddr_in* sockaddr_in_cast(const struct sockaddr* addr);
/* const sockaddr转型为const sockaddr_in6 */
const struct sockaddr_in6* sockaddr_in6_cast(const struct sockaddr* addr);

// 协议栈信息

/* 获取tcp/ip协议栈错误 */
int getSocketError(int sockfd);
/* 获取sockfd对应的本地地址 */
struct sockaddr_in6 getLocalAddr(int sockfd);
/* 获取sockfd对应的对端地址 */
struct sockaddr_in6 getPeerAddr(int sockfd);
/* 判断sockfd是否为自连接 */
bool isSelfConnect(int sockfd);
```

* sockets::createNonblockingOrDie()

创建非阻塞sock fd

```c++
/**
* 创建一个非阻塞sock fd
* @param family 协议族, 可取值AF_UNIX/AF_INET/AF_INET6 etc.
* @return 成功, 返回sock fd; 失败, 程序终止(LOG_SYSFATAL)
*/
int sockets::createNonblockingOrDie(sa_family_t family)
{
#if VALGRIND // a kind of memory test tool
    int sockfd = ::socket(family, SOCK_STREAM, IPPROTO_TCP);
    if (sockfd < 0)
    {
        LOG_SYSFATAL << "sockets::createNonblockingOrDie";
    }
    setNonBlockAndCloseOnExec(sockfd);
#else
    int sockfd = ::socket(family, SOCK_STREAM | SOCK_NONBLOCK | SOCK_CLOEXEC, IPPROTO_TCP);
    if (sockfd < 0)
    {
        LOG_SYSFATAL << "sockets::createNonblockingOrDie";
    }
#endif
    return sockfd;
}
```

* connect

请求连接服务器端addr

```c++
int sockets::connect(int sockfd, const struct sockaddr *addr)
{
    return ::connect(sockfd, addr, static_cast<socklen_t>(sizeof(struct sockaddr_in6)));
}
```

* bindOrDie

绑定sock fd与本地地址addr

```c++
void sockets::bindOrDie(int sockfd, const struct sockaddr *addr)
{
    int ret = ::bind(sockfd, addr, static_cast<socklen_t>(sizeof(struct sockaddr_in6)));
    if (ret < 0)
    {
        LOG_SYSFATAL << "sockets::bindOrDie";
    }
}
```

* listenOrDie

监听本地sock fd。如果协议支持重传（如TCP协议），那么listen第二个参数backlog会被忽略。

```c++
void sockets::listenOrDie(int sockfd)
{
    int ret = ::listen(sockfd, SOMAXCONN);
    if (ret < 0)
    {
        LOG_SYSFATAL << "sockets::listenOrDie";
    }
}
```

* accept

接受连接请求。
被包裹函数accept或accep4，其区别为：accept4一次调用能同时指定SOCK_NONBLOCK和SOCK_CLOEXEC选项；如果要用accept，则还需要额外调用setNonBlockAndCloseOnExec()，来设置sock fd的non-block、close-on-exec属性。

accept调用出错时，跟log记录错误号。

```c++
/**
* accept(2)/accept4(2)包裹函数, 接受连接并获取对端ip地址
* @param sockfd 服务器sock fd, 指向本地监听的套接字资源
* @param addr ip地址信息
* @return 由sockfd接收连接请求得到到连接fd
*/
int sockets::accept(int sockfd, struct sockaddr_in6 *addr)
{
    socklen_t addrlen = static_cast<socklen_t>(sizeof(*addr));
#if VALGRIND || defined(NO_ACCEPT4) // VALGRIND: memory check tool
    int connfd = ::accept(sockfd, sockaddr_cast(addr), &addrlen);
    setNonBlockAndCloseOnExec(connfd);
#else
    // set flags for conn fd returned by accept() at one time
    int connfd = ::accept4(sockfd, sockaddr_cast(addr),
                           &addrlen, SOCK_NONBLOCK | SOCK_CLOEXEC);
#endif
    if (connfd < 0)
    {
        int savedErrno = errno;
        LOG_SYSERR << "Socket::accept";
        switch (savedErrno)
        {
            case EAGAIN:
            case ECONNABORTED:
            case EINTR:
            case EPROTO:
            case EMFILE:
                // expected errors
                errno = savedErrno;
                break;
            case EBADF:
            case EFAULT:
            case EINVAL:
            case ENFILE:
            case ENOBUFS:
            case ENOMEM:
            case ENOTSOCK:
            case EOPNOTSUPP:
                // unexpected errors
                LOG_FATAL << "unexpected error of ::accept " << savedErrno;

            default:
                LOG_FATAL << "unknown error of ::accept " << savedErrno;
                break;
        }
    }
    return connfd;
}
```

* read、readv

read直接转发给read(2)，没有特殊处理；
readv直接转发给readv(2)，没有特殊处理。

```c++
ssize_t sockets::read(int sockfd, void *buf, size_t count)
{
    return ::read(sockfd, buf, count);
}

ssize_t sockets::readv(int sockfd, const struct iovec *iov, int iovcnt)
{
    return ::readv(sockfd, iov, iovcnt);
}
```

* write

write直接转发给write(2)，没有特殊处理；

```c++
ssize_t sockets::write(int sockfd, const void *buf, size_t count)
{
    return ::write(sockfd, buf, count);
}
```

为何有readv，而没有writev？
推测是因为read的时候，可能希望尽量读更多的数据，但由于Buffer大小限制，增加了额外的空间，就用readv来读取；
write的时候，如果要write数据过多，可以保存进度，然后在回调中继续write。

* close

关闭sockfd。

```c++
void sockets::close(int sockfd)
{
    if (::close(sockfd) <0)
    {
        LOG_SYSERR << "sockets::close";
    }
}
```

* shutdownWrite

shutdownWrite关闭连接写方向：shutdown(2) + SHUT_WR

```c++
void sockets::shutdownWrite(int sockfd)
{
    if (::shutdown(sockfd, SHUT_WR) < 0)
    {
        LOG_SYSERR << "sockets::shutdownWrite";
    }
}
```

* toIpPort, toIp

将ip地址、port信息由sockaddr对象，转换为字符串。核心调用inet_ntop(2)， 将IPv4、IPv6地址由二进制转化为文本。
利用snprintf，将ip地址和port文本信息组装到一起。

```c++
/**
* convert struct sockaddr containing ip info to ip string pointed by buf
* @param buf [out] point to ip string buffer
* @param size size of buf (bytes)
* @param addr [in] point to struct sockaddr containing ip address and port info
* @note port of struct sockaddr is network byte order, but local operation needs
* host byte order.
*/
void sockets::toIpPort(char *buf, size_t size, const struct sockaddr *addr)
{
    if (addr->sa_family == AF_INET6)
    { // IPv6
        buf[0] = '[';
        toIp(buf + 1, size - 1, addr);
        size_t end = ::strlen(buf);
        const struct sockaddr_in6* addr6 = sockaddr_in6_cast(addr);
        uint16_t port = sockets::networkToHost16(addr6->sin6_port);
        assert(size > end);
        snprintf(buf + end, size - end, "]:%u", port);
        return;
    }
    // IPv4
    toIp(buf, size, addr);
    size_t end = ::strlen(buf);
    const struct sockaddr_in* addr4 = sockaddr_in_cast(addr);
    uint16_t port = sockets::networkToHost16(addr4->sin_port);
    assert(size > end);
    snprintf(buf + end, size - end, ":%u", port);
}

/**
* convert IP address info from struct sockaddr to string buffer
* @param buf [out] string buffer with NUL-byte
* @param size length of string buffer
* @param addr [in] point to struct sockaddr, which contains ip, port info
*/
void sockets::toIp(char* buf, size_t size,
                   const struct sockaddr* addr)
{
    if (addr->sa_family == AF_INET)
    {
        assert(size >= INET_ADDRSTRLEN);
        const struct sockaddr_in* addr4 = sockaddr_in_cast(addr);
        ::inet_ntop(AF_INET, &addr4->sin_addr, buf, static_cast<socklen_t>(size));
    }
    else if (addr->sa_family == AF_INET6)
    {
        assert(size >= INET6_ADDRSTRLEN);
        const struct sockaddr_in6* addr6 = sockaddr_in6_cast(addr);
        ::inet_ntop(AF_INET6, &addr6->sin6_addr, buf, static_cast<socklen_t>(size));
    }
}
```

* fromIPPort

将ip地址、端口号文本转换为二进制（sockaddr_in/sockaddr_in6），sockaddr_in适用于IPv4，sockaddr_in6适用于IPv6。
2个重载函数是toIpPort()的逆过程。

```c++
/**
* convert ipv4 string to struct sockaddr_in
* @param ip ipv4 address string with format like "127.0.0.1"
* @param port local port for TCP/UDP
* @param addr [out] store ipv4 info
*/
void sockets::fromIpPort(const char *ip, uint16_t port, struct sockaddr_in *addr)
{
    addr->sin_family = AF_INET;
    addr->sin_port = hostToNetwork16(port);
    if (::inet_pton(AF_INET, ip, &addr->sin_addr) <= 0)
    {
        LOG_SYSERR << "sockets::fromIpPort";
    }
}

/**
* convert ipv6 string to struct socket_in6
* @param ip ipv6 address string with format like "2409:8a4c:662f:2900:b42c:a0d9:fe5:2037"
* @param port local port for TCP/UDP
* @param addr [out] store ipv6 info
*/
void sockets::fromIpPort(const char *ip, uint16_t port, struct sockaddr_in6 *addr)
{
    addr->sin6_family = AF_INET6;
    addr->sin6_port = hostToNetwork16(port);
    if (::inet_pton(AF_INET6, ip, &addr->sin6_addr) <= 0)
    {
        LOG_SYSERR << "sockets::fromIpPort";
    }
}
```

* 地址转型

提供不同地址类型之间的转型，如sockaddr_in */sockaddr_in6* /sockaddr*，要求成员内存布局必须是一样的。这也是为什么前面用static_assert来断言sockaddr_in/sockaddr_in6成员偏移的原因（offsetof），因为如果成员偏移不一样，也就是说对象的内存布局不一样，通过指针直接转型是不对的。

**为什么用static_cast对指针进行转型，而不用reinterpret_cast？**
单独的static_cast，是无法将一种指针类型转换为另一种指针类型的，需要先利用implicit_cast（隐式转型）/static_cast（显式转型）将指针类型转换为void*/const void* （无类型）指针，然后才能转换为模板类型指针。

而reinterpret_cast可以直接做到，但reinterpret_cast通常并不安全，编译期也不会在编译期报错，通常不推荐使用。

```c++
// const sockaddr_in6* => const sockaddr*
const struct sockaddr* sockets::sockaddr_cast(const struct sockaddr_in6* addr)
{
//    reinterpret_cast<const struct sockaddr*>();
    return static_cast<const struct sockaddr*>(implicit_cast<const void*>(addr));
}

// sockaddr_in6* => sockaddr*
struct sockaddr* sockets::sockaddr_cast(struct sockaddr_in6* addr)
{
    return static_cast<struct sockaddr*>(implicit_cast<void*>(addr));
}

// const sockaddr_in* => const sockaddr*
const struct sockaddr* sockets::sockaddr_cast(const struct sockaddr_in* addr)
{
    return static_cast<const struct sockaddr*>(implicit_cast<const void*>(addr));
}

// const sockaddr* => const sockaddr_in*
const struct sockaddr_in* sockets::sockaddr_in_cast(const struct sockaddr* addr)
{
    return static_cast<const struct sockaddr_in*>(implicit_cast<const void*>(addr));
}

// const sockaddr* => const sockaddr_in6*
const struct sockaddr_in6* sockets::sockaddr_in6_cast(const struct sockaddr *addr)
{
    return static_cast<const struct sockaddr_in6*>(implicit_cast<const void*>(addr));
}
```

* getSocketError

获取tcp协议栈错误。利用getsockopt + SO_ERROR选项，获取tcp协议栈内部错误。
通常，在处理连接的读写事件时调用，检查是否发生错误。

```c++
int sockets::getSocketError(int sockfd)
{
    int optval;
    socklen_t optlen = static_cast<socklen_t>(sizeof(optval));


    if (::getsockopt(sockfd, SOL_SOCKET, SO_ERROR, &optval, &optlen))
    {
        return errno;
    }
    else
    {
        return optval;
    }
}
```

* getLocalAddr

从连接获取本地ip地址（包括端口号）。不论IPv4，还是IPv6，统一存放到sockaddr_in6结构对象中，因为该对象长度最长。
核心调用getsockname(2)。

```c++
/**
* Get a local ip address from an opened sock fd
* @param sockfd an opened sockfd
* @return local ip address info
*/
struct sockaddr_in6 sockets::getLocalAddr(int sockfd)
{
    struct sockaddr_in6 localaddr;
    memZero(&localaddr, sizeof(localaddr));
    socklen_t addrlen = static_cast<socklen_t>(sizeof(localaddr));
    // get local ip addr info bound to sockfd
    if (::getsockname(sockfd, sockaddr_cast(&localaddr), &addrlen) < 0)
    {
        LOG_SYSERR << "sockets::getLocalAddr";
    }
    return localaddr;
}
```

* getPeerAddr

获取连接对端的ip地址（包括端口号）。类似于getLocalAddr，地址信息都存放到sockaddr_in6结构对象中。
核心调用getpeername(2)。

```c++
/**
* Get a peer ip address from an opened sock fd
* @param sockfd an opened sockfd
* @return peer ip address info
*/
struct sockaddr_in6 sockets::getPeerAddr(int sockfd)
{
    struct sockaddr_in6 peeraddr;
    memZero(&peeraddr, sizeof(peeraddr));
    socklen_t addrlen = static_cast<socklen_t>(sizeof(peeraddr));
    if (::getpeername(sockfd, sockaddr_cast(&peeraddr), &addrlen))
    {
        LOG_SYSERR << "sockets::getPeerAddr";
    }
    return peeraddr;
}
```

* isSelfConnect

检查是否为自连接。利用了getLocalAddr()和getPeerAddr()，检查ip地址是否相同，来判断连接对端地址信息是否为本机。

同样分IPv4和IPv6两种情况，依据是sockaddr_in6的sin6_family成员。

注意：对于IPv4，地址sin_addr.s_addr是32bit，能用“ =”判断是否相等；而对于IPv6，地址sin6_addr是28byte，无法用“= ”判断，需要用memcmp来比较二进制位。

```c++
/**
* 检查是否为自连接, 判断连接sockfd两端ip地址信息是否相同.
* @param sockfd 连接对应的文件描述符
* @return true: 是自连接; false: 不是自连接
*/
bool sockets::isSelfConnect(int sockfd)
{
    struct sockaddr_in6 localaddr = getLocalAddr(sockfd);
    struct sockaddr_in6 peeraddr = getPeerAddr(sockfd);
    if (localaddr.sin6_family == AF_INET)
    { // IPv4
        const struct sockaddr_in* laddr4 = reinterpret_cast<struct sockaddr_in*>(&localaddr);
        const struct sockaddr_in* raddr4 = reinterpret_cast<struct sockaddr_in*>(&peeraddr);
        return laddr4->sin_port == raddr4->sin_port
        && laddr4->sin_addr.s_addr == raddr4->sin_addr.s_addr;
    }
    else if (localaddr.sin6_family == AF_INET6)
    { // IPv6
        return localaddr.sin6_port == peeraddr.sin6_port
        && memcmp(&localaddr.sin6_addr, &peeraddr.sin6_addr, sizeof(localaddr.sin6_addr)) != 0;
    }
    else
    {
        return false;
    }
}
```

##### 应用层缓冲区Buffer设计

##### Buffer设计思想

应用层缓冲区Buffer设计

* 为什么要需要有应用层缓冲区
* Buffer结构

epoll使用LT模式原因

* 与poll兼容
* LT模式不会发生漏掉事件的BUG，但POLLOUT事件不能一开始就关注，否则会出现busy loop，而应该在write无法完全写入内核缓冲区的时候才关注，将未写入内核缓冲区的数据添加到应用层output buffer，直到应用层output buffer写完，停止关注POLLOUT事件。-- 相当于写完成回调，处理写完成事件。
* 读写的时候不必等候EAGAIN，可以节省系统调用次数，降低延迟。（注：如果用ET模式，读的时候读到EAGAIN，写的时候直到output buffer写完成或者EAGAIN）

**为什么需要有应用层缓冲区？**

non-blocking网络编程中，non-blocking IO核心思想是避免阻塞在read()/write()或其他IO系统调用上，可以最大限度复用thread-of-control，让一个线程能服务于多个socket连接。而IO线程只能阻塞在IO-multiplexing函数上，如select()/poll()/epoll_wait()，这样应用层的缓冲区就是必须的，每个TCP socket都要有stateful的input buffer和output buffer。

具体来说，

* **TcpConnection必须要有output buffer**
  一个常见场景：程序想通过TCP连接发送100K byte数据，但在write()调用中，OS只接收80K（受TCP通告窗口advertised window的控制），而程序又不能原地阻塞等待，事实上也不知道要等多久。程序应该尽快交出控制器，返回到event loop。此时，剩余20K数据怎么办？

对应用程序，它只管生成数据，不应该关系到底数据是一次发送，还是分几次发送，这些应该由网络库操心，程序只需要调用TcpConnection::send()就行。网络库应该接管剩余的20K数据，把它保存到TcpConnection的output buffer，然后注册POLLOUT事件，一旦socket变得可写就立刻发送数据。当然，第二次不一定能完全写入20K，如果有剩余，网络库应该继续关注POLLOUT事件；如果写完20K，网络库应该停止关注POLLOUT，以免造成busy loop。

如果程序又写入50K，而此时output buffer里还有待发20K数据，那么网络库不应该直接调用write()，而应该把这50K数据append到那20K数据之后，等socket变得可写时再一并写入。

如果output buffer里还有待发送数据，而程序又想关闭连接，但对程序而言，调用TcpConnection::send()后就认为数据迟早会发出去，此时网络库不应该直接关闭连接，而要等数据发送完毕。因为此时数据可能还在内核缓冲区中，并没有通过网卡成功发送给接收方。
将数据append到buffer，甚至write进内核，都不代表数据成功发送给对端。

综上，要让程序在write操作上不阻塞，网络库必须给每个tcp connection配置output buffer。

* **TcpConnection必须要有input buffer**
  TCP是一个无边界的字节流协议，接收方必须要处理“收到的数据尚不构成一条完整的消息”“一次收到两条消息的数据”等情况。
  一个常见场景：发送方send 2条10K byte消息（共计20K），接收方收到数据的可能情况：
* 一次性收到20K数据
* 分2次收到，第一次5K，第二次15K
* 分2次收到，第一次15K，第二次5K
* 分2次收到，第一次10K，第二次10K
* 分3次收到，第一次6K，第二次8K，第三次6K
* 其他任何可能

这些情况统称为粘包问题。

LT模式下，如何解决的粘包问题？
可以一次把内核缓冲区中的数据读完，存至input buffer，通知应用程序，进行onMessage(Buffer* buffer)回调。在onMessage回调中，应用层协议判定是否是一个完整的包，如果不是一条完整的消息，不会取走数据，也不会进行相应的处理；如果是一条完整的消息，将取走这条消息，并进行相应的处理。

如何判断是一条完整的消息？
相应应用层协议制定协议，不由网络库负责。

网络库如何处理 “socket可读”事件？
一次性把socket中数据读完，从内核缓冲区读取到应用层buffer，否则会反复触发POLLIN事件，造成busy-loop。

综上，tcp网络编程中，网络库必须给每个tcp connection配置input buffer。

应用层与input buffer、output buffer

muduo中的IO都是带缓冲的IO（buffered IO），应用层不会自行去read()或write()某个socket，只会操作TcpConnection的input buffer和output buffer。更准确来说，是在onMessage()回调中读取input buffer；调用TcpConnection::send() 来间接操作output buffer，而不直接操作output buffer。

##### Buffer要求

muduo buffer 的设计考虑常见网络编程需求，在易用性和性能之间找一个平衡点。目前更偏向易用性。

muduo Buffer设计要点：

* 对外表现为一块连续的内存（char*, len），以便客户代码的编写。
* size()可以自动增长，以适应不同大小的消息。不是一个fixed size array（不是固定大小数组，char buf[8192]）。
* 内部以vector of char来保存数据，并提供相应的访问函数。

Buffer更像一个queue，从末尾写入数据，从头部读出数据。

谁使用Buffer，读、写Buffer？
TcpConnection有2个Buffer：input buffer，output buffer。

* input buffer，TcpConnection会从socket读取数据，然后写入input buffer（由Buffer::readFd()完成）；客户代码在onMessage回调中，从input buffer读取数据。
* output buffer，客户代码把数据写入output buffer（用Connection::send()完成）；TcpConnection从output buffer读取数据并写入socket。

input、output是针对客户代码而言的，对TcpConnection来说读写方向相反。

##### 线程安全

Buffer故意设计成非线程安全的，原因如下：

* 对于input buffer，onMessage()回调发生在该TcpConnection所属IO线程，应用程序应该在onMessage()完成对input buffer的操作，并且不要把input buffer暴露给其他线程。这样，对input buffer的操作都在同一个IO线程，因此Buffer class不必是线程安全的。
* 对于output buffer，应用程序不会直接操作它，而是调用TcpConenction::send()来发送数据，后者是线程安全的。准确来说，是会让output buffer只会在所属IO线程操作。

##### Buffer数据结构

![](https://img2022.cnblogs.com/blog/741401/202204/741401-20220412171109103-1465773870.png)

2个indices（标记）readIndex、writeIndex把vector内容分为3块：prependable、readable、writable，各块大小关系：
prependable = readIndex
readable = writeIndex - readIndex
writable = size() - writeIndex

灰色部分是Buffer的有效载荷（payload），prependable能让程序以极低代价在数据前添加几个字节，从而简化客户代码。

readIndex，writeIndex满足以下不变式（invariant）：
0 ≤ readIndex ≤ writeIndex ≤ data.size()

Buffer class的核心数据成员：

```c++
class Buffer : public muduo::copyable
{
public:
    ...
private:
    std::vector<char> buffer_; // 存储数据的线性缓冲区, 大小可变
    size_t readerIndex_; // 可读数据首地址, i.e. readable空间首地址
    size_t writerIndex_; // 可写数据首地址, i.e. writable空间首地址
    ...
};
```

Buffer中有2个常数：kCheapPrepend，kInitialSize，分别定义了prependable初始大小，writable的初始大小。readable初始大小0。

```c++
    static const size_t kCheapPrepend = 8;   // 初始预留的prependable空间大小
    static const size_t kInitialSize = 1024; // Buffer初始大小
```

初始化完成后，Buffer数据结构如下：
![](https://img2022.cnblogs.com/blog/741401/202204/741401-20220412173407579-1717880116.png)

##### Buffer的操作

##### 基本IO操作

Buffer初始化完成后，向Buffer写入200byte，其布局是：

![](https://img2022.cnblogs.com/blog/741401/202204/741401-20220412173502458-783290308.png)

可以看到，writeIndex向后移动了200，readIndex保持不变。readable、writable都有变化。

如果从Buffer read(), retrieve()（“读入”）50byte，其布局是：
![](https://img2022.cnblogs.com/blog/741401/202204/741401-20220412173532217-1169824755.png)

可以看到，readIndex向后移动50，writeIndex保持不变，readable和writable的值也有变化。

##### 自动增长

Buffer长度不是固定的，可以自动增长，因为底层存储利用的是vector。

假设当前可写空间writable为624 byte，现客户代码一次写入1000，那么buffer会自动增长，如下图。

增长前：
![](https://img2022.cnblogs.com/blog/741401/202204/741401-20220412173542958-1236803670.png)

增长后：
![](https://img2022.cnblogs.com/blog/741401/202204/741401-20220412173546598-1288273385.png)

readable由350增长为1350（刚好增加了1000），writable由624减为0。另外，readIndex由58回到了初始位置8，保证prependable等于kCheapPrependable。

buffer没有缩小功能，下次写入1350byte就不会重新分配内存，一方面避免浪费内存，另一方面避免反复分配内存。如果有需要，客户代码也可以手动shrink() buffer size()。

##### size()和capacity()

Buffer使用vector存储数据，可以得到它的capacity()机制的好处，减少内存分配次数。在写入的数据不超过capacity() - size()时，都不会重新分配内存。
![](https://img2022.cnblogs.com/blog/741401/202204/741401-20220412173558439-1839675400.png)

在muduo中，不调用reserve()预先分配空间，而是在Buffer构造时把初始size()设为1K，当size()超过1K时，vector会把capacity()加倍，等于说resize()做了reserve()的事。

##### 内部腾挪

经过若干次读写，readIndex移到了比较靠后的位置，留下了很大的prependable空间，如下图已由初始8 byte，变成532。
![](https://img2022.cnblogs.com/blog/741401/202204/741401-20220412173616065-1397427603.png)

此时，如果想写入300byte，而writable只有200，怎么办？
Buffer不会重新分配内存，而是先把已有的数据移到前面去，减小多余prependable空间，为writable腾出空间。
![](https://img2022.cnblogs.com/blog/741401/202204/741401-20220412173620685-816517251.png)

这样，writable变成724 byte，就可以写入300 byte 数据了。

##### prepend

prepend 提供prependable空间，让程序能以极低的代价在数据前添加几个字节。通过预留kCheapPrepend空间，能简化客户代码。
![](https://img2022.cnblogs.com/blog/741401/202204/741401-20220412173635921-294129537.png)

##### Buffer类的实现

##### 构造函数与析构函数

构造函数见下，析构函数才用trivial dctor（编译器合成的默认版本，即平凡的析构函数）

##### 读取prependable, readable, writable空间地址、大小等属性的方法

```c++
    /* 返回 readable 空间大小 */
    size_t readableBytes() const
    { return writerIndex_ - readerIndex_; }

    /* 返回 writeable 空间大小 */
    size_t writableBytes() const
    { return buffer_.size() - writerIndex_; }

    /* 返回 prependable 空间大小 */
    size_t prependableBytes() const
    { return readerIndex_; }

    /* readIndex 对应元素地址 */
    const char* peek() const
    { return begin() + readerIndex_; }

    /* 返回待写入数据的地址, 即writable空间首地址 */
    char* beginWrite()
    { return begin() + writerIndex_; }
    const char* beginWrite() const
    { return begin() + writerIndex_; }

    /* 将writerIndex_往后移动len byte, 需要确保writable空间足够大 */
    void hasWritten(size_t len)
    {
        assert(len <= writableBytes());
        writerIndex_ += len;
    }
    /* 将writerIndex_往前移动len byte, 需要确保readable空间足够大 */
    /*
     * Cancel written bytes.
     */
    void unwrite(size_t len)
    {
        assert(len <= readableBytes());
        writerIndex_ -= len;
    }

private:
    /* 返回缓冲区的起始位置, 也是prependable空间起始位置 */
    char* begin()
    {
//       <=> return buffer_.data();
        return &*buffer_.begin();
    }
    const char* begin() const
    { return &*buffer_.begin(); }
```

##### retrieve 取走数据

retrieve系列函数从readable空间取走数据，只关心移动readableIndex_，改变readable空间大小，通常不关心读取数据具体内容，除非有指定具体的返回值，如retrieveAllAsString。
retrieve系列函数会改变readable空间大小，但通常不会改变writable空间大小，除非retrieveAll取完所有readable空间的数据，readable空间将会合并到writable空间。

```c++
    /* 从readable头部取走最多长度为len byte的数据. 会导致readable空间变化, 可能导致writable空间变化.
     * 这里取走只是移动readerIndex_, writerIndex_, 并不会直接读取或清除readable, writable空间数据　*/
    /*
     * retrieve() returns void, to prevent
     * string str(retrieve(readableBytes()), readableBytes());
     * the evaluation of two functions are unspecified
     */
    void retrieve(size_t len)
    {
        assert(len <= readableBytes());
        if (len < readableBytes()) // readable 中数据充足时, 只取走len byte数据
        {
            readerIndex_ += len;
        }
        else
        { // readable中数据不足时, 取走所有数据
            retrieveAll();
        }
    }
    /* 从readable空间取走 [peek(), end)这段区间数据, peek()是readable空间首地址 */
    void retrieveUntil(const char* end)
    {
        assert(peek() <= end);
        assert(end <= beginWrite());
        retrieve(end - peek());
    }
    /* 从readable空间取走一个int64_t数据, 长度8byte */
    void retrieveInt64()
    {
        retrieve(sizeof(int64_t));
    }
    /* 从readable空间取走一个int32_t数据, 长度4byte */
    void retrieveInt32()
    {
        retrieve(sizeof(int32_t));
    }
    /* 从readable空间取走一个int16_t数据, 长度2byte */
    void retrieveInt16()
    {
        retrieve(sizeof(int16_t));
    }
    /* 从readable空间取走一个int8_t数据, 长度1byte */
    void retrieveInt8()
    {
        retrieve(sizeof(int8_t));
    }
    /* 从readable空间取走所有数据, 直接移动readerIndex_, writerIndex_指示器即可 */
    void retrieveAll()
    {
        readerIndex_ = kCheapPrepend;
        writerIndex_ = kCheapPrepend;
    }
    /* 从readable空间取走所有数据, 转换为字符串返回 */
    std::string retrieveAllAsString()
    {
        return retrieveAsString(readableBytes());
    }
    /* 从readable空间头部取走长度len byte的数据, 转换为字符串返回 */
    std::string retrieveAsString(size_t len)
    {
        assert(len <= readableBytes());
        string result(peek(), len);
        retrieve(len);
        return result;
    }
```

##### readInt 取数据

readInt系列函数从readable空间读取指定长度（类型）的数据，不仅从readable空间读取数据，还会利用相应的retrieve函数把数据从中取走，导致readable空间变小。

```c++
    /* 从readable空间头部读取一个int64_类型数, 由网络字节序转换为本地字节序 */
    /**
     * Read int64_t from network endian
     *
     * Require: buf->readableBytes() >= sizeof(int64_t)
     */
    int64_t readInt64()
    {
        int64_t result = peekInt64();
        retrieveInt64();
        return result;
    }
    /* 从readable空间头部读取一个int32_类型数, 由网络字节序转换为本地字节序 */
    int32_t readInt32()
    {
        int32_t result = peekInt32();
        retrieveInt32();
        return result;
    }
    /* 从readable空间头部读取一个int16_类型数, 由网络字节序转换为本地字节序 */
    int16_t readInt16()
    {
        int16_t result = peekInt16();
        retrieveInt16();
        return result;
    }
    /* 从readable空间头部读取一个int8_类型数, 由网络字节序转换为本地字节序 */
    int8_t readInt8()
    {
        int8_t result = peekInt8();
        retrieveInt8();
        return result;
    }
```

##### peek读取而不取走缓冲区数据

peek系列函数只从readable空间头部（peek()）读取数据，而不取走数据，不会导致readable空间变化。

```c++
    /* 从readable的头部peek()读取一个int64_t数据, 但不移动readerIndex_, 不会改变readable空间 */
    /**
     * Peek int64_t from network endian
     *
     * Require: buf->readableBytes() >= sizeof(int64_t)
     */
    int64_t peekInt64() const
    {
        assert(readableBytes() >= sizeof(int64_t));
        int64_t be64 = 0;
        ::memcpy(&be64, peek(), sizeof(be64));
        return sockets::networkToHost64(be64); // 网络字节序转换为本地字节序
    }
    /* 从readable的头部peek()读取一个int32_t数据, 但不移动readerIndex_, 不会改变readable空间 */
    int32_t peekInt32() const
    {
        assert(readableBytes() >= sizeof(int32_t));
        int32_t be32 = 0;
        ::memcpy(&be32, peek(), sizeof(be32));
        return sockets::networkToHost32(be32); // 网络字节序转换为本地字节序
    }
    /* 从readable的头部peek()读取一个int16_t数据, 但不移动readerIndex_, 不会改变readable空间 */
    int16_t peekInt16() const
    {
        assert(readableBytes() >= sizeof(int16_t));
        int16_t be16 = 0;
        ::memcpy(&be16, peek(), sizeof(be16));
        return sockets::networkToHost16(be16); // 网络字节序转换为本地字节序
    }
    /* 从readable的头部peek()读取一个int8_t数据, 但不移动readerIndex_, 不会改变readable空间.
     * 1byte数据不存在字节序问题 */
    int8_t peekInt8() const
    {
        assert(readableBytes() >= sizeof(int8_t));
        int8_t x = *peek();
        return x;
    }
```

##### prepend预置数据到缓冲区

prepend系列函数将预置指定长度数据到prependable空间，但不会改变prependable空间大小。

```c++
    /* 在prependable空间末尾预置int64_t类型网络字节序的数x, 预置数会被转化为本地字节序 */
    /**
     * Prepend int64_t using network endian
     */
     void prependInt64(int64_t x)
    {
         int64_t be64 = sockets::hostToNetwork64(x);
         prepend(&be64, sizeof(be64));
    }
    /* 在prependable空间末尾预置int32_t类型网络字节序的数x, 预置数会被转化为本地字节序 */
    /**
     * Prepend int32_t using network endian
     */
    void prependInt32(int32_t x)
    {
        int64_t be32 = sockets::hostToNetwork32(x);
        prepend(&be32, sizeof(be32));
    }
    /* 在prependable空间末尾预置int16_t类型网络字节序的数x, 预置数会被转化为本地字节序 */
    void prependInt16(int16_t x)
    {
        int16_t be16 = sockets::hostToNetwork16(x);
        prepend(&be16, sizeof(be16));
    }
    /* 在prependable空间末尾预置int8_t类型网络字节序的数x, 预置数会被转化为本地字节序 */
    void prependInt8(int8_t x)
    {
        prepend(&x, sizeof(x));
    }
    /* 在prependable空间末尾预置一组二进制数据data[len].
     * 表面上看是加入prependable空间末尾, 实际上是加入readable开头,　会导致readerIndex_变化 */
    void prepend(const void* data, size_t len)
    {
        assert(len <= prependableBytes());
        readerIndex_ -= len;
        const char* d = static_cast<const char*>(data);
        std::copy(d, d+len, begin()+readerIndex_);
    }
```

##### 内部缓冲区操作

包含3个操作：
1）shrink自动收缩内部缓冲区（buffer_）大小；
2）返回buffer_的capaticy；
3）makeSpace生产足够大小的writable空间，以写入新的len byte数据；

```c++
    /* 收缩缓冲区空间, 将缓冲区中数据拷贝到新缓冲区, 确保writable空间最终大小为reserve */
    void shrink(size_t reserve)
    {
        // FIXME: use vector::shrink_to_fit() in C++ 11 if possible.
        Buffer other;
        other.ensureWritableBytes(readableBytes() + reserve);
        other.append(toStringPiece());
        swap(other);
    }

    /* 返回buffer_的容量capacity() */
    size_t internalCapacity() const
    {
        return buffer_.capacity();
    }

    /* writable空间不足以写入len byte数据时,
     * 1)如果writable空间 + prependable空间不足以存放数据, 就resize 申请新的更大的内部缓冲区buffer_
     * 2)如果足以存放数据, 就将prependable多余空间腾挪出来, 合并到writable空间 */
    void makeSpace(size_t len)
    {
        if (writableBytes() + prependableBytes() < len + kCheapPrepend)
        { // writable 空间大小 + prependable空间大小 不足以存放len byte数据, resize内部缓冲区大小
            // FIXME: move readable data
            buffer_.resize(writerIndex_ + len);
        }
        else
        { // writable 空间大小 + prependable空间大小 足以存放len byte数据, 移动readable空间数据, 合并多余prependable空间到writable空间
            // TODO: ???
            // move readable data to the front, make space inside buffer
            assert(kCheapPrepend < readerIndex_);
            size_t readable = readableBytes();
            std::copy(begin() + readerIndex_,
                      begin() + writerIndex_,
                      begin() + kCheapPrepend);
            readerIndex_ = kCheapPrepend;
            writerIndex_ = readerIndex_ + readable;
            assert(readable == readableBytes());
        }
    }
```

##### readFd从指定fd读取数据

readFd从指定连接对应fd读取数据，当读取的数据超过内部缓冲区writable空间大小时，采用的策略是先用一个64K栈缓存extrabuf临时存储，然后根据需要合并prependable空间到writable空间，或者resize buffer_大小。

```c++
/**
* 从fd读取数据到内部缓冲区, 将系统调用错误保存至savedErrno
* @param 要读取的fd, 通常是代表连接的conn fd
* @param savedErrno[out] 保存的错误号
* @return 读取数据结果. < 0, 发生错误; >= 成功, 读取到的字节数
*/
ssize_t Buffer::readFd(int fd, int* savedErrno)
{
    // saved an ioctl()/FIONREAD call to tell how much to read.
    char extrabuf[65536]; // 65536 = 64K bytes
    struct iovec vec[2];
    const size_t writable = writableBytes();
    vec[0].iov_base = begin() + writerIndex_;
    vec[0].iov_len = writable;
    vec[1].iov_base = extrabuf;
    vec[1].iov_len = sizeof(extrabuf);

    const int iovcnt = (writable < sizeof(extrabuf)) ? 2 : 1;
    const ssize_t n = sockets::readv(fd, vec, iovcnt);
    if (n < 0)
    { // ::readv系统调用错误
        *savedErrno = errno;
    }
    else if (implicit_cast<size_t>(n) <= writable)
    {
        writerIndex_ += n;
    }
    else
    {// 读取的数据超过现有内部buffer_的writable空间大小时, 启用备用的extrabuf 64KB空间, 并将这些数据添加到内部buffer_的末尾
        // 过程可能会合并多余prependable空间或resize buffer_大小, 以腾出足够writable空间存放数据
        // n >= 0 and n > writable
        // => buffer_ is full, then append extrabuf to buffer_
        writerIndex_ = buffer_.size();
        append(extrabuf, n - writable);
    }
    return n;
}
```

##### readable空间特定字符串查找

有时，用户需要先对readable空间中的字符进行专门的查找，如找CRLF，EOL，而不取走数据，根据查找结果再决定取走多少数据。muduo库提供了几个辅助函数，便于用户进行这样的读取操作。

```c++
    /* 在readable空间找CRLF位置, 返回第一个出现CRLF的位置 */
    // CR = '\r', LF = '\n'
    const char* findCRLF() const
    {
        // FIXME: replace with memmem()?
        const char* crlf = std::search(peek(), beginWrite(), kCRLF, kCRLF+2);
        return crlf == beginWrite() ? NULL : crlf;
    }

    /* 在start~writable首地址 之间找CRLF, 要求start在readable地址空间中 */
    // CR = '\r', LF = '\n'
    const char* findCRLF(const char* start) const
    {
        assert(peek() <= start);
        assert(start <= beginWrite());
        // FIXME: replace with memmem()?
        const char* crlf = std::search(start, beginWrite(), kCRLF, kCRLF+2);
        return crlf == beginWrite() ? NULL : crlf;
    }

    /* 在readable空间中找EOL, 即LF('\n') */
    // EOL = '\n'
    // find end of line ('\n') from range [peek(), end)
    const char* findEOL() const
    {
        const void* eol = memchr(peek(), '\n', readableBytes());
        return static_cast<const char*>(eol);
    }

    /* 在start~writable首地址 之间找EOL, 要求start在readable地址空间中 */
    // EOL = '\n
    // find end of line ('\n') from range [start(), end)
    // @require peek() < start
    const char* findEOL(const char* start) const
    {
        assert(peek() <= start);
        assert(start <= beginWrite());
        const void* eol = memchr(start, '\n', beginWrite() - start);
        return static_cast<const char*>(eol);
    }
```

##### 大小端转换

网络字节序默认大端，本地字节序可能大端，也可能小端。网络字节序和本地字节序，可以用htobe* / betoh* 系列函数转换，也可以用htonl/htons/ntohl/ntohs转换，前者是glibc 2.9的部分，属于BSDs标准，后者属于POSIX.1-2001标准。也就是说，如果要开发可移植的程序，使用htonl/htons/ntohl/ntohs系列函数转换；否则，如果只考虑特定平台如Linux，可以使用htobe* / betoh* 系列函数转换。

htobe* / betoh* 系列函数：
支持16bit、32bit、64bit 字节序的数据转换

```c++
       #define _BSD_SOURCE             /* See feature_test_macros(7) */
       #include <endian.h>

       uint16_t htobe16(uint16_t host_16bits);
       uint16_t htole16(uint16_t host_16bits);
       uint16_t be16toh(uint16_t big_endian_16bits);
       uint16_t le16toh(uint16_t little_endian_16bits);

       uint32_t htobe32(uint32_t host_32bits);
       uint32_t htole32(uint32_t host_32bits);
       uint32_t be32toh(uint32_t big_endian_32bits);
       uint32_t le32toh(uint32_t little_endian_32bits);

       uint64_t htobe64(uint64_t host_64bits);
       uint64_t htole64(uint64_t host_64bits);
       uint64_t be64toh(uint64_t big_endian_64bits);
       uint64_t le64toh(uint64_t little_endian_64bits);
```

htonl/htons/ntohl/ntohs系列函数：
只支持16bit、32bit 字节序的数据转换

```c++
       #include <arpa/inet.h>

       uint32_t htonl(uint32_t hostlong);
       uint16_t htons(uint16_t hostshort);
       uint32_t ntohl(uint32_t netlong);
       uint16_t ntohs(uint16_t netshort);
```

#### I/O 复用

**Poller的存在，是为了监听事件，但具体监听什么事件呢？**
这就需要用到Channel类。一个Channel对象绑定了一个fd（文件描述符），可以用来监听发生在fd上的事件，事件包括空事件（不监听）、可读事件、写完成事件。当fd上被监听事件就绪时，对应Channel对象就会被Poller放入激活队列（activeChannels_），进而在loop循环中调用封装在Channel的相应回调来处理事件。

Channel可以通过EventLoop，向Poller更新自己关心的（监听）事件（通过map Poller::channels_存储）。具体来说，对于PollPoller对象，会同步更新（poll(2)）传给内核的poll事件数组pollfds_；对于EPollPoller对象，会同步更新（epoll(7)）传递给内核的epoll事件数组events_；

可以这样理解，poll/epoll监听的是fd（上指定的事件pollfd.events），Poller监听的是Channel对象（上指定的事件events_），当监听到事件就绪时，将对应通道加入激活通道队列，在EventLoop的loop循环中依次调用Channel中注册的事件回调。

##### Channel 类

每个Channel对象从始至终只负责一个文件描述符（fd）的IO事件分发，但不拥有fd，也不会在析构时关闭fd。而是由诸如TcpConnection、Acceptor、EventLoop等，这样需要监听指定文件描述符上事件的类，将fd通过构造函数传递给Channel。
Channel会把不同的IO事件分发为不同的回调，如ReadCallback、WriteCallback，回调对象类型用std::function<>表示，用来定义某个可调用类型。

事件回调类型：

```c++
#include <functional>

typedef std::function<void()> EventCallback;
typedef std::function<void(Timestamp)> ReadEventCallback;
```

Channel成员函数主要包括：
1）设置事件处理的回调函数set *Callback（如setReadCallback）；
2）使能fd关心的事件events_，可调用enable* （如enableReading），该fd及关心的事件会注册到Poller中进行监听；
3）关闭fd关心的事件events_，可调用disable*（如disableReading），会更新该fd在Poller中监听的事件；
4）关闭fd关心的所有事件events_，可调用disableAll，会更新该fd在Poller中监听的事件；
5）删除对fd的监听，会将其从Poller的ChannelMap中移除；
6）Poller监听到Channel事件被激活时，将其加入到激活列表，在EventLoop中回调handleEvent。

##### Channel类声明

```c++
/**
* Channel绑定一个fd, 用于设置fd上要监听的事件, 以及相应的回调函数.
* Poller监听到有通道绑定的事件发生, 就会将其加入激活的通道列表,
* 然后在EventLoop::loop()中调用该Channel对应事件注册的回调函数
*/
class Channel : private noncopyable
{
public:
    typedef std::function<void()> EventCallback; // 除了读事件, 用于其他事件(如写/关闭/错误)回调类型
    typedef std::function<void(Timestamp)> ReadEventCallback; // 读事件回调类型

    Channel(EventLoop* loop, int fd__);
    ~Channel()

    /* 处理事件, 监听事件激活时, 由EventLoop::loop调用 */
    void handleEvent(Timestamp recevieTime);
    /* 设置事件回调，由Channel对象持有者配置Channel事件回调时调用 */
    void setReadCallback(ReadEventCallback cb)
    { readCallback_ = std::move(cb); }
    void setWriteCallback(EventCallback cb)
    { writeCallback_ = std::move(cb); }
    void setCloseCallback(EventCallback cb)
    { closeCallback_ = std::move(cb); }
    void setErrorCallback(EventCallback cb)
    { errorCallback_ = std::move(cb); }

    /* 将shared_ptr管理的对象系到本地weak_ptr管理的tie_, 可用于保存TcpConnection指针 */
    void tie(const std::shared_ptr<void>&);

    int fd() const { return fd_; }
    int events() const { return events_; }
    void set_revents(int revt) { revents_ = revt; } // used by poller
//    int revents() const { return revents_; }
    bool isNoneEvent() const { return events_ == kNoneEvent; }

    /* 使能/禁用 监听 可读/可写事件, 会影响Poller监听的通道列表 */
    void enableReading() { events_ |= kReadEvent; update(); }
    void disableReading() { events_ &= ~kReadEvent; update(); }
    void enableWriting() { events_ |= kWriteEvent; update(); }
    void disableWriting() { events_ &= ~kWriteEvent; update(); }
    void disableAll() { events_ = kNoneEvent; update(); }
    /* 判断是否请求监听 可写事件 */
    bool isWriting() const { return events_ & kWriteEvent; }
    /* 判断是否请求监听 可读事件 */
    bool isReading() const { return events_ & kReadEvent; }

    // for Poller
    int index() { return index_; }
    void set_index(int idx) { index_ = idx; }

    // for debug
    string reventsToString() const;
    string eventsToString() const;

    void doNotLogHup() { logHup_ = false; }

    EventLoop* ownerLoop() { return loop_; }
    /* 从EventLoop中移除当前通道.
     * 建议在移除前禁用所有事件
     */
    void remove();

private:
    /* 将fd对应事件转化为字符串 */
    static string eventsToString(int fd, int ev);
    /* update()将调用EventLoop::updateChannel更新监听的通道 */
    void update();
    /* 根据不同的事件源激活不同的回调函数，来处理事件 */
    void handleEventWithGuard(Timestamp receiveTime);

    static const int kNoneEvent;
    static const int kReadEvent;
    static const int kWriteEvent;

    EventLoop* loop_;
    const int fd_; // file descriptor
    int events_;   // request events, set by user
    int revents_;  // returned events, current active events, set by EventLoop/Poller
    // used by Poller
    // PollPoller: index of poll fds array mapped to fd_
    // EPollPoller: operation type for fd: kNew, kAdded, kDeleted
    int index_;
    bool logHup_;
    /* 使用weak_ptr指向shared_ptr所指对象, 防止循环引用. 通常是生命周期不确定的对象, 如TcpConnection */
    std::weak_ptr<void> tie_;
    bool tied_; /* weak_ptr tie_绑定对象的标志 */
    bool eventHandling_; /* 正在处理事件的标志 */
    bool addedToLoop_;   /* 加入到loop中, 被监听/处理的标志 */
    ReadEventCallback readCallback_; /* 可读事件回调 */
    EventCallback writeCallback_;    /* 可写事件回调 */
    EventCallback closeCallback_;    /* 关闭事件回调 */
    EventCallback errorCallback_;    /* 错误事件回调 */
};
```

Channel中的几个重要函数：

##### handleEvent 处理事件

处理激活的Channel事件，由Poller更新激活的Channel列表，EventLoop::loop()根据激活Channel列表，逐个执行Channel中已注册好的相应回调。实际事件处理工作，由handleEventWithGuard完成。

```c++
/**
* 处理激活的Channel事件
* @details Poller中监听到激活事件的Channel后, 将其加入激活Channel列表,
* EventLoop::loop根据激活Channel回调对应事件处理函数.
* @param recevieTime Poller中调用epoll_wait/poll返回后的时间. 用户可能需要该参数.
*/
void Channel::handleEvent(Timestamp recevieTime)
{
    /*
     * shared_ptr通过RAII方式管理对象资源guard
     * weak_ptr::lock可将weak_ptr提升为shared_ptr, 引用计数+1
     */
    std::shared_ptr<void> guard;
    if (tied_)
    {
        /*
         * 为什么使用 tie?
         * 确保在执行事件处理动作时, 所需的对象不会被释放, 但又不能用shared_ptr,
         * 否则可能导致循环引用. 最好使用weak_ptr, 然后lock提升为shared_ptr, 这样更安全.
         */
        guard = tie_.lock();
        if (guard)
        {
            handleEventWithGuard(recevieTime);
        }
    }
    else
    {
        handleEventWithGuard(recevieTime);
    }
}
```

##### handleEventWithGuard 识别事件并回调

根据不同的激活原因，调用不的回调函数。这些回调函数，是在持有Channel对象，需要进行事件监听的class中进行设置，比如TcpConnection，EventLoop，Acceptor，TimerQueue等。而有些回调函数，经过层层传递，会呈现可网络库的调用者，比如TcpConnection会将处理一个socket fd的读事件回调（新建连接请求），传递给TcpServer::newConnection，这样用户就能通过TcpServer::setConnectionCallback设置其回调。

```c++
/**
* 根据不同的激活原因, 调用不同的回调函数
*/
void Channel::handleEventWithGuard(Timestamp receiveTime)
{
    eventHandling_ = true; // 正在处理事件
    LOG_TRACE << reventsToString(); // 打印fd及就绪事件
    if ((revents_ & POLLHUP) && !(revents_ & POLLIN))
    { // fd挂起(套接字已不在连接中), 并且没有数据可读
        if (logHup_)
        { // 打印挂起log
            LOG_WARN << "fd = " << fd_ << " Channel::handle_event() POLLHUP";
        }
        // 调用关闭回调
        if (closeCallback_) closeCallback_();
    }
    if (revents_ & POLLNVAL) // 无效请求, fd没打开
    { // fd dont be opened
        LOG_WARN << "fd = " << fd_ << " Channel::handle_event() POLLNVAL";
    }
    if (revents_ & (POLLERR | POLLNVAL)) // 错误条件, 或 无效请求, fd没打开
    { // error or fd dont be opened
        if (errorCallback_) errorCallback_();
    }
    if (revents_ & (POLLIN | POLLPRI | POLLRDHUP)) // 有待读数据, 或 紧急数据(e.g. TCP带外数据), 或流套接字对端关闭连接/写半连接
    { // there is data, urgent data,  to be read
        if (readCallback_) readCallback_(receiveTime);
    }
    if (revents_ & POLLOUT)
    {
        if (writeCallback_) writeCallback_();
    }
    eventHandling_ = false;
}
```

##### update 更新通道

通过EventLoop对象，传递给Poller对象，然后更新其监听的通道列表中对应通道。支持ADD/MOD操作。

```c++
void Channel::update()
{
    addedToLoop_ = true;
    loop_->updateChannel(this);
}

void EventLoop::updateChannel(Channel *channel)
{
    assert(channel->ownerLoop() == this);
    assertInLoopThread();
    poller_->updateChannel(channel);
}

/**
* Update array pollfds_
*
* O(logN)
*/
void PollPoller::updateChannel(Channel *channel)
{
    Poller::assertInLoopThread();
    LOG_TRACE << "fd = " << channel->fd() << " events = " << channel->events();
    if (channel->index() < 0)
    { // a new one, add to pollfds_
        // ensure channel point to a new one
        assert(channels_.find(channel->fd()) == channels_.end());
        struct pollfd pfd;
        pfd.fd = channel->fd();
        pfd.events = static_cast<short>(channel->events());
        pfd.revents = 0;
        pollfds_.push_back(pfd);
        int idx = static_cast<int>(pollfds_.size()) - 1;
        channel->set_index(idx);
        channels_[pfd.fd] = channel; // insert (fd, channel)
    }
    else
    { // update existing one
        assert(channels_.find(channel->fd()) != channels_.end());
        assert(channels_[channel->fd()] == channel);
        int idx = channel->index();
        // ensure channel does exist in pollfds_
        assert(0 <= idx && idx < static_cast<int>(pollfds_.size()));
        struct pollfd& pfd = pollfds_[idx];
        assert(pfd.fd == channel->fd() || pfd.fd == -channel->fd() - 1);
        pfd.fd = channel->fd();
        pfd.events = static_cast<short>(channel->events());
        pfd.revents = 0;
        if (channel->isNoneEvent())
        {
            // ignore this pollfd
            pfd.fd = -channel->fd() - 1;
        }
    }
}
```

##### remove 移除通道

与update类似，也是通过EventLoop传递给Poller对象，将当前通道从Poller的事件列表中删除。支持DEL操作。

```c++
void Channel::update()
{
    addedToLoop_ = true;
    loop_->updateChannel(this);
}

void EventLoop::updateChannel(Channel *channel)
{
    assert(channel->ownerLoop() == this);
    assertInLoopThread();
    poller_->updateChannel(channel);
}

/**
* 从监听的通道数组channels_中, 移除指定通道
*/
void PollPoller::removeChannel(Channel *channel)
{
    Poller::assertInLoopThread();
    LOG_TRACE << "fd = " << channel->fd();
    assert(channels_.find(channel->fd()) != channels_.end());
    assert(channels_[channel->fd()] == channel);
    assert(channel->isNoneEvent());
    int idx = channel->index();
    assert(0 <= idx && idx < static_cast<int>(pollfds_.size()));
    const struct pollfd& pfd = pollfds_[idx]; (void)pfd;

    // ensure remove one invalid channel from channels_
    assert(pfd.fd == -channel->fd() - 1 && pfd.events == channel->events());
    size_t n = channels_.erase(channel->fd());
    assert(n == 1); (void)n;

    // remove pollfd from pollfds_ by index
    if (implicit_cast<size_t>(idx) == pollfds_.size() - 1)
    { // last of pollfds_
        pollfds_.pop_back();
    }
    else
    {
        // swap the pollfd to be removed with the last of pollfds_,
        // then remove the last
        int channelAtEnd = pollfds_.back().fd;
        iter_swap(pollfds_.begin() + idx, pollfds_.end() - 1);
        if (channelAtEnd < 0)
        {
            channelAtEnd = -channelAtEnd - 1;
        }
        channels_[channelAtEnd]->set_index(idx);
        pollfds_.pop_back();
    }
}
```

I/O复用使得程序能同时监听多个文件描述符，能有效提高程序性能。Linux下，实现I/O复用的系统调用主要有3个：
1）select(2)；2）poll(2)；3）epoll(7)。

muduo采用了2）和3），分别用PollPoller/EPollPoller对poll/epoll进行了封装，基类Poller主要用于提供统一的接口。

##### Poller类

先来看看基类Poller定义：

```c++
/**
* IO Multiplexing Interface
* Support poll(2), epoll(7)
*
* Only owner EventLoop IO thread can invoke it, so thread safe is not necessary.
*/
/**
* IO复用接口
* 禁止编译器生成copy构造函数和copy assignment
* 支持poll(2), epoll(7)
*/
class Poller : noncopyable
{
public:
    typedef std::vector<Channel*> ChannelList;
    explicit Poller(EventLoop* loop);
    virtual ~Poller();
    /**
     * Polls the I/O events.
     * Must be called in the loop thread.
     * poll(2) for PollPoller, epoll_wait(2) for EPollPoller
     */
    /*
     * 监听函数，根据激活的通道列表，监听指定fd的相应事件
     * 对于PollPoller会调用epoll_wait(2), 对于EPollPoller会调用poll(2)
     *
     * 返回调用完epoll_wait/poll的当前时间（Timestamp对象）
     */
    virtual Timestamp poll(int timeoutMs, ChannelList* activeChannels) = 0;

    /**
     * Update channel listened
     */
    /* 更新监听通道的事件 */
    virtual void updateChannel(Channel* channel) = 0;
    /* 删除监听通道 */
    virtual void removeChannel(Channel* channel) = 0;
    /* 判断当前Poller对象是否持有指定通道 */
    virtual bool hasChannel(Channel* channel) const;

    /* 默认创建Poller对象的类函数 */
    static Poller* newDefaultPoller(EventLoop* loop);
    /*
     * 断言所属EventLoop为当前线程.
     * 如果断言失败，将终止程序（LOG_FATAL）
     */
    void assertInLoopThread() const
    {
        ownerLoop_->assertInLoopThread();
    }
protected:
    /*
     * 该类型保存fd和需要监听的events，以及各种事件回调函数（可读/可写/错误/关闭等）
     */
    typedef std::map<int, Channel*> ChannelMap;
    // Poller don't own the Channel, so the channel must be unregister(EventLoop::removeChannel) before its dtor.
    // std::map used for speeding up to find out a channel by fd
    /* 保存所有事件的Channel，一个Channel绑定一个fd */
    ChannelMap channels_;

private:
    /*
     * 事件驱动循环, 用于调用poll监听fd事件
     */
    EventLoop* ownerLoop_;
};
```

只有拥有EventLoop的IO线程，才能调用EventLoop所拥有的Poller对象的接口，因此考虑Poller的线程安全不是必要的。

一个Channel对应一个fd（文件描述符），一个fd有三种事件状态：空事件（kNoneEvent），读事件（kReadEvent，即POLLIN | POLLPRI），写事件（kWriteEvent，即POLLOUT）。只有后2个，poll/epoll才会进行监听。

EventLoop会根据Poller::newDefaultPoller()，Poller对象。实际策略是根据是否设置了环境变量，来选择创建PollPoller，还是EPollPoller。

```c++
Poller *Poller::newDefaultPoller(EventLoop *loop) // static
{
    if (::getenv("MUDUO_USE_POLL")) // 如果设置了环境变量
    {
        return new PollPoller(loop);
    }
    else
    {
        return new EPollPoller(loop);
    }
    return nullptr;
}
```

##### 派生类EPollPoller

EPollPoller 以epoll为核心，实现了基类Poller的virtual函数，在其中调用了epoll_create/ctl/wait等接口。poll返回后，会将就绪的fd添加到激活队列activeChannels中管理。

```c++
/**
* IO Multiplexing with epoll(7).
*/
class EPollPoller : public Poller
{
public:
    EPollPoller(EventLoop* loop);
    ~EPollPoller() override;
    /* 监听函数, 调用epoll_wait() */
    Timestamp poll(int timeoutMs, ChannelList* activeChannels) override;
    /* ADD/MOD/DEL */
    void updateChannel(Channel* channel) override;
    /* DEL */
    void removeChannel(Channel* channel) override;

private:
    /* events_数组初始大小 */
    static const int kInitEventListSize = 16;
    /* 将op(EPOLL_CTL_Add/MOD/DEL)转换成字符串 */
    static const char* operationToString(int op);
    /* poll返回后将就绪的fd添加到激活通道中activeChannels */
    void fillActiveChannels(int numEvents,
                            ChannelList* activeChannels) const;
    /* 由updateChannel/removeChannel调用，真正执行epoll_ctl()控制epoll的函数 */
    void update(int operation, Channel* channel);

    typedef std::vector<struct epoll_event> EventList;
    /* epoll文件描述符，由epoll_create返回 */
    int epollfd_;
    /* epoll事件数组，为了适配epoll_wait参数要求 */
    EventList events_;
};
```

muduo在实现时，创建epoll fd时，并没有用epoll_create，而是用 epoll_create1。原因在于： epoll_create1在打开epoll文件描述符时，可以直接指定FD_CLOEXEC选项，相当于open时指定O_CLOSEXEC。另外，epoll_create的size参数在Linux2.6.8以后，就已经没用了（>0即可），内核会实现自动增长内部数据结构以描述监听事件。

值得一提的是，在Channel中定义了一个名为index_的成员，由Channel构造初值为0，可通过Channel::index()/set_index()访问，在不同的Poller中有不同的含义：在EPollPoller中，index_用来表示事件类型（kNew/kAdded/kDeleted）；在PollPoller中的含义，到PollPoller类解析中再讲。

#### 定时器

##### 定时功能相关类

muduo定时功能如何将timerfd融入select/poll/select框架？
由3个class实现：TimerID、Timer、TimerQueue。用户可见的只有TimerId。

Timestamp类是时间戳类，用来保存超时时刻（精确到1us），保存的是UTC时间，即从 Unix Epoch（1970-01-01 00:00:00）到指定时间的微秒数。

Timer类对应一个超时任务，保存了超时时刻Timestamp，超时回调函数，以及超时任务类型（一次 or 周期）。

TimerId类用于保存Timer对象，以及独一无二的id。

TimerQueue类用于设置所有超时任务（Timer），需要高效组织尚未到期的Timer，快速查找已到期Timer，以及高效添加和删除Timer。TimerQueue用std::set存储 ，set会对Timer按到期时间先后顺序进行二叉搜索树排序，时间复杂度O(logN)。

TimerQueue的定时接口并不是直接暴露给库的使用者的，而是通过EventLoop的runAfter和runEvery来运行用户任务的。其中，runAfter延迟固定秒数后运行一次指定用户任务；runEvery延迟固定秒数后运行用户任务，后续以指定周期运行用户任务。

TimerQueue回调用户代码onTimer()的时序：
![](https://img2022.cnblogs.com/blog/741401/202203/741401-20220313202759092-1606646527.png)

时序图里的TimerQueue获取超时Timer（getExpired()）后，User及onTimer()是指用户自定义的超时处理函数，并非库本身的。

与普通Channel事件一样，超时任务TimerQueue也会使用一个Channel，专门用于绑定timerfd，交由Poller监听，发生可读事件（代表超时）后加入激活通道列表，然后EventLoop::loop()逐个Channel调用对应的回调，从而处理超时事件。
注意：一个EventLoop只持有一个TimerQueue对象，而TimerQueue通过std::set持有多个Timer对象，但只会设置一个Channel。

##### Timer类

Timer类代表一个超时任务，但并不直接绑定Channel。Timer主要包含超时时刻（expiration_），超时回调（callback_），周期时间值（interval_），全局唯一id（sequence_）。

其声明如下：

```c++
/**
* 用于定时事件的内部类
*/
class Timer : noncopyable
{
public:
    Timer(TimerCallback cb, Timestamp when, double interval)
    : callback_(std::move(cb)),
    expiration_(when),
    interval_(interval),
    repeat_(interval > 0.0),
    sequence_(s_numCreated_.incrementAndGet())
    { }
    /* 运行超时回调函数 */
    void run() const
    {
        callback_();
    }
    /* 返回超时时刻 */
    Timestamp expiration() const { return expiration_; }
    /* 周期重复标志 */
    bool repeat() const { return repeat_; }
    /* 全局唯一序列号, 用来表示当前Timer对象 */
    int64_t sequence() const { return sequence_; }
    /* 重启定时器, 只对周期Timer有效(repeat_为true) */
    void restart(Timestamp now);
    /* 当前创建的Timer对象个数, 每新建一个Timer对象就会自增1 */
    static int64_t numCreated() { return s_numCreated_.get(); }

private:
    const TimerCallback callback_; /* 超时回调 */
    Timestamp expiration_;         /* 超时时刻 */
    const double interval_;        /* 周期时间, 单位秒, 可用来结合基础时刻expiration_, 计算新的时刻 */
    const bool repeat_;            /* 重复标记. true: 周期Timer; false: 一次Timer */
    const int64_t sequence_;       /* 全局唯一序列号 */

    // global increasing number, atomic. help to identify different Timer
    static AtomicInt64 s_numCreated_; /* 类变量, 创建Timer对象的个数, 用来实现全局唯一序列号 */
};
```

每当创建一个新Timer对象时，原子变量s_numCreated_就会自增1，作为全剧唯一序列号sequence_，用来标识该Timer对象。

* 周期Timer

创建Timer时，超时时刻when决定了回调超时事件时间点，而interval决定了Timer是一次性的，还是周期性的。如果是周期性的，会在TimerQueue::reset中，调用Timer::restart，在当前时间点基础上，重启定时器。

* restart函数

restart重启Timer，根据Timer是否为周期类型，分为两种情况：
1）周期Timer，restart将重置超时时刻expiration_为当前时间 + 周期间隔时间；
2）非周期Timer，即一次性Timer，将restart将expiration_置为无效时间（默认自UTC Epoch以来的微妙数为0）；

```c++
void Timer::restart(Timestamp now)
{
    if (repeat_)
    {
        expiration_ = addTime(now, interval_);
    }
    else
    {
        expiration_ = Timestamp::invalid();
    }
}
```

##### TimerId类

TimerId来主要用来作为Timer的唯一标识，用于取消（canceling）Timer。

其实现代码很简单：

```c++
/**
* An opaque identifier, for canceling Timer.
*/
class TimerId : public muduo::copyable
{
public:
    TimerId()
    : timer_(NULL),
    sequence_(0)
    { }

    TimerId(Timer* timer, int64_t seq)
    : timer_(timer),
    sequence_(seq)
    { }

    // default copy-ctor, dtor and assignment are okay

    friend class TimerQueue;

private:
    Timer* timer_;
    int64_t sequence_;
};
```

注意：TimerId并不直接生成Timer序列号sequence_，这是由Timer来生成的，通过构造函数传递给TimerId。而生成Timer标识的方式，在Timer类介绍中也提到过，只需要创建一个Timer对象即可，然后通过Timer::sequence()方法就可以取得该序列号。

##### TimerQueue类

定时器队列TimerQueue是定时功能的核心，由所在EventLoop持有，绑定一个Channel，同时维护多个定时任务（Timer）。为用户（EventLoop）提供添加定时器（addTimer）、取消定时器（cancel）接口。

同样是定时，TimerQueue与Timer有什么区别？

TimerQueue包含2个Timer集合：
1）timers_定时器集合：包含用户添加的所有Timer对象，std::set会用AVL搜索树，对集合元素按时间戳（Timestamp）从小到大顺序；
2）activeTimers_激活定时器集合：包含激活的Timer对象，与timers_包含的Timer对象相同，个数也相同，std::set会根据Timer*指针大小，对元素进行排序；3）cancelingTimers_取消定时器集合：包含所有取消的Timer对象，与activeTimers_相对。

注意：timers_和activeTimers_的类型并不相同，只是包含的Timer*相同。cancelingTimers_和activeTimers_的类型相同。

这也是TimerQueue并非Timer的原因，是一个Timer集合，根据其时间戳大小进行排序，更像是一个队列，先到期的先触发超时事件。因此，可称为Timer队列，即TimerQueue。

调用TimerQueue::addTimer的，只有EventLoop中这3个函数：

```c++
/**
* 定时功能，由用户指定绝对时间
* @details 每为定时器队列timerQueue添加一个Timer,
* timerQueue内部就会新建一个Timer对象, TimerId就保含了这个对象的唯一标识(序列号)
* @param time 时间戳对象, 单位1us
* @param cb 超时回调函数. 当前时间超过time代表时间时, EventLoop就会调用cb
* @return 一个绑定timerQueue内部新增的Timer对象的TimerId对象, 用来唯一标识该Timer对象
*/
TimerId EventLoop::runAt(Timestamp time, TimerCallback cb)
{
    return timerQueue_->addTimer(std::move(cb), time, 0.0);
}

/**
* 定时功能, 由用户相对时间, 通过runAt实现
* @param delay 相对时间, 单位s, 精度1us(小数)
* @param cb 超时回调
*/
TimerId EventLoop::runAfter(double delay, TimerCallback cb)
{
    Timestamp time(addTime(Timestamp::now(), delay));
    return runAt(time, std::move(cb));
}

/**
* 定时功能, 由用户指定周期, 重复运行
* @param interval 运行周期, 单位s, 精度1us(小数)
* @param cb 超时回调
* @return 一个绑定timerQueue内部新增的Timer对象的TimerId对象, 用来唯一标识该Timer对象
*/
TimerId EventLoop::runEvery(double interval, TimerCallback cb)
{
    Timestamp time(addTime(Timestamp::now(), interval));
    return timerQueue_->addTimer(std::move(cb), time, interval);
}
```

下面是TimerQueue中，3个集合相关的类型及成员定义：

```c++
    typedef std::pair<Timestamp, Timer*> Entry;
    typedef std::set<Entry> TimerList;
    typedef std::pair<Timer*, int64_t> ActiveTimer;
    typedef std::set<ActiveTimer> ActiveTimerSet;

    // Timer list sorted by expiration
    /* 用户添加的所有Timer对象集合
     * 需要为set元素比较实现operator< */
    TimerList timers_;

    // for cancel()
    ActiveTimerSet activeTimers_;
    bool callingExpiredTimers_; /* atomic */
    ActiveTimerSet cancelingTimers_;
```

##### TimerQueue声明

除了前面提到的3个集合相关类型及成员，其他成员函数和变量声明如下：

```c++
/**
* 定时器队列.
* 不能保证回调能及时调用.
*
* 只能在所在loop线程中运行, 因此线程安全是非必须的
*/
class TimerQueue : noncopyable
{
public:
    explicit TimerQueue(EventLoop* loop);
    ~TimerQueue();
    /*
     * 添加一个定时器.
     * 运行到指定时间， 调度相应的回调函数.
     * 如果interval参数 > 0.0, 就周期重复运行.
     * 必须线程安全: 可能会由其他线程调用
     */
    TimerId addTimer(TimerCallback cb, Timestamp when, double interval);
    /* 取消指定TimerId的定时器 */
    void cancel(TimerId);

private:
    ...
    void addTimerInLoop(Timer* timer);
    void cancelInLoop(TimerId timerId);
    // called when timerfd alarms
    void handleRead();
    // move out all expired timers
    std::vector<Entry> getExpired(Timestamp now);
    void reset(const std::vector<Entry>& expired, Timestamp now);

    bool insert(Timer* timer);

    EventLoop* loop_;
    const int timerfd_;
    Channel timerfdChannel_; // watch readable event of timerfd
    ...
}
```

TimerQueue所属EventLoop对象，通过一个EventLoop*来传递，注意这是一个raw pointer，而非smart pointer。EventLoop对象与TimerQueue对象生命周期相同，而且只会通过EventLoop对象来调用TimerQueue对象方法，因此不存在与之相关的内存泄漏或非法访问的问题。

##### TimerQueue构造函数

```c++
TimerQueue::TimerQueue(EventLoop *loop)
: loop_(loop),
timerfd_(createTimerfd()),
timerfdChannel_(loop, timerfd_),
timers_(),
callingExpiredTimers_(false)
{
    timerfdChannel_.setReadCallback(std::bind(&TimerQueue::handleRead, this));
    // we are always reading the timerfd, we disrm it with timerfd_settime.
    timerfdChannel_.enableReading();
}
```

构造TimerQueue对象时，就会绑定TimerQueue所属EventLoop，即创建TimerQueue的EventLoop对象。
另外，调用Channel::enableReading()，会将通道事件加入Poller的监听通道列表中。

交给Poller监听的timerfd，是由createTimerfd创建的：

```c++
int createTimerfd()
{
    // create timers that notify via fd
    int timerfd = ::timerfd_create(CLOCK_MONOTONIC, TFD_NONBLOCK | TFD_CLOEXEC);
    if (timerfd < 0)
    {
        LOG_SYSFATAL << "Failed in timerfd_create";
    }
    return timerfd;
}
```

##### TimerQueue析构

析构有2点需要注意：
1）在remove绑定的通道前，要先disableAll停止监听所有通道事件；
2）timers_中Timer对象是在TimerQueue::addTimer中new出来的，需要手动delete；
另外，对注释“do not remove channel, since we're in EventLoop::dtor();”并不明白是何用意。

```c++
TimerQueue::~TimerQueue()
{
    // 关闭所有(通道)事件, Poller不再监听该通道
    timerfdChannel_.disableAll();
    // 如果正在处理该通道, 会从激活的通道列表中移除, 同时Poller不再监听该通道
    timerfdChannel_.remove();
    // 关闭通道对应timerfd
    ::close(timerfd_);

    // FIXME: I dont understand why "do not remove channel". What does it mean?
    // do not remove channel, since we're in EventLoop::dtor();

    // TimerQueue::addTimer中new出来的Timer对象, 需要手动delete
    for (const Entry& timer : timers_)
    {
        delete timer.second;
    }
}
```

##### TimerQueue重要接口

##### addTimer 添加定时器

注意到addTimer 会在构造一个Timer对象后，将其添加到timers_的工作转交给addTimerInLoop完成了。这是为什么？

因为调用EventLoop::runAt/runEvery的线程，可能并非TimerQueue的loop线程，而修改TimerQueue数据成员时，必须在所属loop线程中进行，因此需要通过loop_->runInLoop将工作转交给所属loop线程。
runInLoop：如果当前线程是所属loop线程，则直接运行函数；如果不是，就排队到所属loop线程末尾，等待运行。

```c++
/**
* 添加一个定时器.
* @details 运行到指定时间点when, 调度相应的回调函数cb.
* 如果interval参数 > 0.0, 就周期重复运行.
* 可能会由其他线程调用, 需要让对TimerQueue数据成员有修改的部分, 在所属loop所在线程中运行.
* @param cb 超时回调函数
* @param when 触发超时的时间点
* @param interval 循环周期. > 0.0 代表周期定时器; 否则, 代表一次性定时器
* @return 返回添加的Timer对应TimerId, 用来标识该Timer对象
*/
TimerId TimerQueue::addTimer(TimerCallback cb, Timestamp when, double interval)
{
    Timer* timer = new Timer(std::move(cb), when, interval);
    loop_->runInLoop(std::bind(&TimerQueue::addTimerInLoop, this, timer)); // 转交所属loop线程运行
    return TimerId(timer, timer->sequence());
}

/**
* 在loop线程中添加一个定时器.
* @details addTimerInLoop 必须在所属loop线程中运行
*/
void TimerQueue::addTimerInLoop(Timer *timer)
{
    loop_->assertInLoopThread();
    bool earliestChanged = insert(timer);

    if (earliestChanged)
    {
        resetTimerfd(timerfd_, timer->expiration());
    }
}
```

addTimerInLoop的主要工作由2个函数来完成：insert，resetTimerfd。

```c++
/**
* 插入一个timer指向的定时器
* @details timers_是std::set<std::pair<Timestamp, Timer*>>类型, 容器会自动对元素进行排序,
* 默认先按pair.first即Timestamp进行排序, 其次是pair.second(.first相同情况下才比较second),
* 这样第一个元素就是时间戳最小的元素.
* @return 定时器timer当前是否已经超时
* - true timers_为空或已经超时
* - false timers_非空, 且最近的一个定时器尚未超时
*/
bool TimerQueue::insert(Timer *timer)
{
    loop_->assertInLoopThread();
    assert(timers_.size() == activeTimers_.size());
    bool earliestChanged = false;
    Timestamp when = timer->expiration(); // 超时时刻
    TimerList::iterator it = timers_.begin();
    if (it == timers_.end() || when < it->first)
    { // 定时器集合为空 或者 新添加的timer已经超时(因为it指向的Timer超时时刻是距离当前最近的)
        earliestChanged = true; // timer已经超时
    }

    // 同时往timers_和activeTimers_集合中, 添加timer
    // 注意: timers_和activeTimers_元素类型不同, 但所包含的Timer是相同的, 个数也相同

    { // ensure insert new timer to timers_ successfully
        std::pair<TimerList::iterator, bool> result
        = timers_.insert(Entry(when, timer));
        assert(result.second); (void)result;
    }

    { // ensure insert new timer to activeTimers_ successfully
        std::pair<ActiveTimerSet::iterator, bool> result
        = activeTimers_.insert(ActiveTimer(timer, timer->sequence()));
        assert(result.second); (void)result;
    }

    assert(timers_.size() == activeTimers_.size());
    return earliestChanged;
}
```

##### cancel 取消定时器

一个已超时的定时器，会通过TimerQueue::getExpired自动清除，但一个尚未到期的定时器如何取消？
可以通过调用TimerQueue::cancel。类似于addTimer，cancel也可能在别的线程被调用，因此需要将其转交给cancelInLoop执行。

```c++
/**
* 取消一个定时器, 函数可能在别的线程调用
* @param timerId 每个定时器都有一个唯一的TimerId作为标识
*/
void TimerQueue::cancel(TimerId timerId)
{
    loop_->runInLoop(
            std::bind(&TimerQueue::cancelInLoop, this, timerId));
}

/**
* 在所属loop线程中, 取消一个定时器
* @details 同时擦出timers_, activeTimers_中包含的Timer对象, timerId用来查找该Timer对象.
* @param timerId 待取消Timer的唯一Id标识
*/
void TimerQueue::cancelInLoop(TimerId timerId)
{
    loop_->assertInLoopThread(); // 确保当前线程是所属loop线程
    assert(timers_.size() == activeTimers_.size());
    ActiveTimer timer(timerId.timer_, timerId.sequence_);
    ActiveTimerSet::const_iterator it = activeTimers_.find(timer);
    if (it != activeTimers_.end())
    {
        // 注意timers_和activeTimers_的Timer指针指向相同对象, 只能delete一次
        size_t n = timers_.erase(Entry(it->first->expiration(), it->first));
        assert(n == 1); (void)n;
        delete it->first; // FIXME: no delete please
        activeTimers_.erase(it);
    }
    else if (callingExpiredTimers_)
    { // 如果正在处理超时定时器
        cancelingTimers_.insert(timer);
    }
    assert(timers_.size() == activeTimers_.size());
}
```

##### handleRead处理TimerQueue上所有超时任务

handleRead有几个要点：
1）必须在所在loop线程运行；
2）可能不止一个定时任务超时，可用getExpired()获取；
3）所有超时任务执行完后，重置周期定时任务，释放一次性定时任务；

```c++
/**
* 处理读事件, 只能是所属loop线程调用
* @details 当PollPoller监听到超时发生时, 将channel加入激活通道列表, loop中回调
* 事件处理函数, TimerQueue::handleRead.
* 发生超时事件时, 可能会有多个超时任务超时, 需要通过getExpired一次性全部获取, 然后逐个执行回调.
* @note timerfd只会发生读事件.
*/
void TimerQueue::handleRead()
{
    loop_->assertInLoopThread();
    Timestamp now(Timestamp::now());
    readTimerfd(timerfd_, now);

    std::vector<Entry> expired = getExpired(now); // 获取所有超时任务

    // 正在调用超时任务回调时, 先清除取消的超时任务cancelingTimers_, 再逐个执行超时回调.
    // 可由getExpired()获取的所有超时任务.
    callingExpiredTimers_ = true;
    cancelingTimers_.clear();
    // safe to callback outside critical section
    for (const Entry& it : expired)
    {
        it.second->run(); // 通过Timer::run()回调超时处理函数
    }
    callingExpiredTimers_ = false;
    // 重置所有已超时任务
    reset(expired, now);
}
```

getExpired以参数时间点now为界限，查找set timers_中所有超时定时任务（Timer）。set会对timers_元素进行排序，std::set::lower_bound()会找到第一个时间点 < now时间点的定时任务。

getExpired调用reset重置所有超时的周期定时任务，释放超时的一次性任务。

```c++
/**
* 定时任务超时时, 从set timers_中取出所有的超时任务, 以vector形式返回给调用者
* @note 注意从set timers_要和从set activeTimers_同步取出超时任务, 两者保留的定时任务是相同的
* @param now 当前时间点, 用来判断从set中的定时器是否超时
* @return set timers_中超时的定时器
*/
std::vector<TimerQueue::Entry> TimerQueue::getExpired(Timestamp now)
{
    assert(timers_.size() == activeTimers_.size());
    std::vector<Entry> expired;
    Entry sentry(now, reinterpret_cast<Timer*>(UINTPTR_MAX));
    // end.key >= sentry.key, Entry.key is pair<Timestamp, Timer*>
    // in that end.key.second < sentry.key.second(MAX PTR)
    // => end.key == sentry.key is impossible
    // => end.key > sentry.key
    TimerList::iterator end = timers_.lower_bound(sentry);
    assert(end == timers_.end() || now < end->first);
    std::copy(timers_.begin(), end, back_inserter(expired));
    timers_.erase(timers_.begin(), end);

    for (const Entry& it : expired)
    {
        ActiveTimer timer(it.second, it.second->sequence());
        size_t n = activeTimers_.erase(timer);
        assert(n == 1); (void)n;
    }

    assert(timers_.size() == activeTimers_.size());
    return expired;
}

/**
* 根据指定时间now重置所有超时任务, 只对周期定时任务有效
* @param expired 所有超时任务
* @param now 指定的reset基准时间点, 新的超时时间点以此为基准
*/
void TimerQueue::reset(const std::vector<Entry> &expired, Timestamp now)
{
    Timestamp nextExpire;

    for (const Entry& it : expired)
    {
        ActiveTimer timer(it.second, it.second->sequence());
        // 只重置周期定时任务和没有取消的定时任务, 释放一次性超时的定时任务
        if (it.second->repeat()
        && cancelingTimers_.find(timer) == cancelingTimers_.end())
        {
            it.second->restart(now);
            insert(it.second);
        }
        else
        {
            // FIXME move to a free list
            delete it.second; // FIXME: no delete please
        }
    }

    // 根据最近的尚未达到的超时任务, 重置timerfd下一次超时时间
    if (!timers_.empty())
    {
        nextExpire = timers_.begin()->second->expiration();
    }

    if (nextExpire.valid())
    {
        resetTimerfd(timerfd_, nextExpire);
    }
}
```

#### 异步日志

![](https://img2022.cnblogs.com/blog/741401/202203/741401-20220306223622054-297119160.png)

采用双缓冲区（double buffering）交互技术。基本思想是准备2部分buffer：A和B，前端（front end）线程往buffer A填入数据（日志消息），后端（back end）线程负责将buffer B写入日志文件。当A写满时，交换A和B。如此往复。

实现时，在后端设置一个已满缓冲队列（Buffer1~n，2<=n<=16），用于缓存一个周期内临时要写的日志消息。

这样做到好处在于：
1）线程安全；2）非阻塞。

这样，2个buffer在前端写日志时，不必等待磁盘文件操作，也避免每写一条日志消息都触发后端线程。

异常处理：
当一个周期内，产生过多Buffer入队列，当超过队列元素上限数量值25时，直接丢弃多余部分，并记录。

##### 日志前端

其中前端主要包括：Logger, LogStream，FixedBuffer，SourceFile。

类图关系如下：
![](https://img2022.cnblogs.com/blog/741401/202203/741401-20220306223709671-1956089166.png)

##### Logger类

Logger位于Logging.h/Logging.cc，主要为用户（前端线程）提供使用日志库的接口，是一个pointer to impl的实现（即GoF 桥接模式），详细由内部类Impl实现。

Logger 可以根据用户提供的__FILE__，__LINE__等宏构造对象，记录日志的代码自身信息（所在文件、行数）；还提供构造不同等级的日志消息对象。每个Logger对象代表一个日志消息。

Logger 内部定义了日志等级（enum LogLevel），提供全局日志等级（g_logLevel）的获取、设置接口；提供访问内部LogStream对象的接口。

##### 日志等级类型LogLevel

定义：

```c++
    enum LogLevel
    {
        TRACE = 0,
        DEBUG,
        INFO,
        WARN,
        ERROR,
        FATAL,
        NUM_LOG_LEVELS
    };
```

各日志等级通常含义：

* TRACE
  指出比DEBUG粒度更细的一些信息事件（开发过程中使用）
* DEBUG
  指出细粒度信息事件对调试应用程序是非常有帮助的（开发过程中使用）
* INFO
  表明消息在粗粒度级别上突出强调应用程序的运行过程。
* WARN
  系统能正常运行，但可能会出现潜在错误的情形。
* ERROR
  指出虽然发生错误事件，但仍然不影响系统的继续运行。
* FATAL
  指出每个严重的错误事件将会导致应用程序的退出。

muduo默认级别为INFO，开发过程中可以选择TRACE或DEBUG。低于指定级别日志不会被输出。

##### 用户接口

Logging.h中，还定义了一系列LOG_开头的宏，便于用户以C++风格记录日志：

```c++
#define LOG_TRACE if (muduo::Logger::logLevel() <= muduo::Logger::TRACE) \
  muduo::Logger(__FILE__, __LINE__, muduo::Logger::TRACE, __func__).stream()
#define LOG_DEBUG if (muduo::Logger::logLevel() <= muduo::Logger::DEBUG) \
  muduo::Logger(__FILE__, __LINE__, muduo::Logger::DEBUG, __func__).stream()
#define LOG_INFO if (muduo::Logger::logLevel() <= muduo::Logger::INFO) \
  muduo::Logger(__FILE__, __LINE__).stream()
#define LOG_WARN muduo::Logger(__FILE__, __LINE__, muduo::Logger::WARN).stream()
#define LOG_ERROR muduo::Logger(__FILE__, __LINE__, muduo::Logger::ERROR).stream()
#define LOG_FATAL muduo::Logger(__FILE__, __LINE__, muduo::Logger::FATAL).stream()
#define LOG_SYSERR muduo::Logger(__FILE__, __LINE__, false).stream()
#define LOG_SYSFATAL muduo::Logger(__FILE__, __LINE__, true).stream()
```

例如，用户可以用这样的方式使用日志：

```c++
LOG_TRACE << "trace" << 1;
```

##### 构造函数

不难发现，每个宏定义都构造了一个Logger临时对象，然后通过stream()，来达到写日志的功能。
选取参数最完整的Logger构造函数，构造Logger临时对象：

```c++
muduo::Logger(__FILE__, __LINE__, muduo::Logger::TRACE, __func__)

__FILE__ 是一个宏, 表示当前代码所在文件名（含路径）
__LINE__ 是一个宏, 表示当前代码所在文件的行数
muduo::Logger::TRACE 日志等级TRACE
__func__ 是一个宏, 表示当前代码所在函数名
```

对应原型：

```c++
Logger(SourceFile file, int line, LogLevel level, const char* func);
```

这里SourceFile也是一个内部类，用来对构造Logger对象的代码所在文件名进行了包装，只记录基本的文件名（不含路径），以节省日志消息长度。

##### 输出位置，冲刷日志

一个应用程序，通常只有一个全局Logger。Logger类定义了2个函数指针，用于设置日志的输出位置（g_output），冲刷日志（g_flush）。
类型：

```c++
typedef void (*OutputFunc)(const char* msg, int len);
typedef void (*FlushFunc)();
```

Logger默认向stdout输出、冲刷：

```c++
void defaultOutput(const char* msg, int len)
{
    size_t n = fwrite(msg, 1, static_cast<size_t>(len), stdout);
    //FIXME check n
    (void)n;
}
void defaultFlush()
{
    fflush(stdout);
}

Logger::OutputFunc g_output = defaultOutput;
Logger::FlushFunc g_flush = defaultFlush;
```

Logger也提供2个static函数来设置g_output和g_flush。

```c++
static void setOutput(OutputFunc);
static void setFlush(FlushFunc);
```

用户代码可以这两个函数修改Logger的输出位置（需要同步修改）。一种典型的应用，就是将g_output重定位到后端AsyncLogging::append()，这样后端线程就能在缓冲区满或定时，从缓冲区取出数据并（与前端线程异步）写到日志文件。

```c++
muduo::AsyncLogging* g_asyncLog = NULL;

void asyncOutput(const char* msg, int len)
{
  g_asyncLog->append(msg, len);
}

void func()
{
    muduo::Logger::setOutput(asyncOutput);
    LOG_INFO << "123456";
}

int main()
{
  char name[256] = { '\0' };
  strncpy(name, argv[0], sizeof name - 1);
  muduo::AsyncLogging log(::basename(name), kRollSize);
  log.start();
  g_asyncLog = &log;

  func();
}
```

##### 日志等级，时区

还定义了2个全局变量，用于存储日志等级（g_logLevel），时区（g_logTimeZone）。当前日志消息等级，如果低于g_logLevel，就不会进行任何操作，几乎0开销；只有不低于g_logLevel等级的日志消息，才能被记录。这是通过LOG_xxx宏定义 的if语句实现的。

```c++
#define LOG_TRACE if (muduo::Logger::logLevel() <= muduo::Logger::TRACE) \
...
```

g_logTimeZone 会影响日志记录的时间是用什么时区，默认UTC时间（GMT时区）。例如：

```bash
20220306 07:37:08.031441Z  3779 WARN  Hello - Logging_test.cpp:75
```

这里面的“20220306 07:37:08.031441Z”会受到日志时区的影响。

根据g_logTimeZone产生日志记录的时间，位于formatTime()。由于不是核心功能，这里不详述，后续有时间再研究，通常采用默认的GMT时区即可。

##### 析构函数

前面说过，Logger是一个桥接模式，具体实现交给Impl。Logger析构代码如下：

```c++
Logger::~Logger()
{
  impl_.finish();                     // 往Small Buffer添加后缀 文件名:行数
  const LogStream::Buffer& buf(stream().buffer());
  g_output(buf.data(), buf.length()); // 回调保存的g_output, 输出Small Buffer到指定文件流
  if (impl_.level_ == FATAL)          // 发生致命错误, 输出log并终止程序
  {
    g_flush();                        // 回调冲刷
    abort();
  }
}
```

析构函数中，Logger主要完成工作：为LogStream对象stream_中的log消息加上后缀（文件名:行号，LF指换行符'\n'），将stream_缓存的log消息通过g_output回调写入指定文件流。另外，如果有致命错误（FATAL级别log），就终止程序。

缓冲Small Buffer 大小默认4KB，实际保存每条log消息，具体参见LogStream描述。

##### Impl类

Logger::Impl是Logger的内部类，负责Logger主要实现，提供组装一条完整log消息的功能。

下面是3条完整log：

```bash
20220306 09:15:44.681220Z  4013 WARN  Hello - Logging_test.cpp:75
20220306 09:15:44.681289Z  4013 ERROR Error - Logging_test.cpp:76
20220306 09:15:44.681296Z  4013 INFO  4056 - Logging_test.cpp:77
```

格式：日期 + 时间 + 微秒 + 线程id + 级别 + 正文 + 原文件名 + 行号

```bash
日期      时间     微秒     线程  级别  正文     源文件名:       行号
20220306 09:15:44.681220Z  4013 WARN  Hello - Logging_test.cpp:75
...
```

##### Impl的数据结构

```c++
    class Impl
    {
    public:
        typedef Logger::LogLevel LogLevel;
        Impl(LogLevel level, int old_errno, const SourceFile& file, int line);
        void formatTime();  // 根据时区格式化当前时间字符串, 也是一条log消息的开头
        void finish();      // 添加一条log消息的后缀

        Timestamp time_;    // 用于获取当前时间
        LogStream stream_;  // 用于格式化用户log数据, 提供operator<<接口, 保存log消息
        LogLevel level_;    // 日志等级
        int line_;          // 源代码所在行
        SourceFile basename_; // 源代码所在文件名(不含路径)信息
    };
```

包含了需要组装成一条完整log信息的所有组成部分。当然，正文部分是由用户线程直接通过Logstream::operator<<，传递给stream_的。

##### Impl构造函数

除了对各成员进行初始构造，还生成线程tid、格式化时间字符串等，并通过stream_加入Samall Buffer。

```c++
Logger::Impl::Impl(LogLevel level, int savedErrno, const SourceFile &file, int line)
        : time_(Timestamp::now()),
          stream_(),
          level_(level),
          line_(line),
          basename_(file)
{
    formatTime();
    CurrentThread::tid();
    stream_ << T(CurrentThread::tidString(), static_cast<unsigned int>(CurrentThread::tidStringLength()));
    stream_ << T(LogLevelName[level], kLogLevelNameLength); // 6
    if (savedErrno != 0) // 发生系统调用错误
    {
        stream_ << strerror_tl(savedErrno) << " (errno=" << savedErrno << ") "; // 自定义函数strerror_tl将错误号转换为字符串, 相当于strerror_r(3)
    }
}
```

##### LogStream类实现

LogStream 主要提供operator<<操作，将用户提供的整型数、浮点数、字符、字符串、字符数组、二进制内存、另一个Small Buffer，格式化为字符串，并加入当前类的Small Buffer。

##### Small Buffer存放log消息

Small Buffer，是模板类FixedBuffer<>的一个具现，i.e.FixedBuffer，默认大小4KB，用于存放一条log消息。为前端类LogStream持有。
相对的，还有Large Buffer，也是FixedBuffer的一个具现，FixedBuffer，默认大小4MB，用于存放多条log消息。为后端类AsyncLogging持有。

```c++
const int kSmallBuffer = 4000;
const int kLargeBuffer = 4000 * 1000;

class LogStream : noncopyable
{
    ...
    typedef detail::FixedBuffer<detail::kSmallBuffer> Buffer; // Small Buffer Type
    ...
    Buffer buffer_;  // 用于存放log消息的Small Buffer
}

class AsyncLogging: noncopyable
{
    ...
    typedef muduo::detail::FixedBuffer<muduo::detail::kLargeBuffer> Buffer; // Large Buffer Type
    ...
}
```

模板类FixedBuffer，内部是用数组char data_[SIZE]存储，用指针char* cur_表示当前待写数据的位置。对FixedBuffer<>的各种操作，实际上是对data_数组和cur_指针的操作。

```c++
template<int SIZE>
class FixedBuffer : noncopyable
{
public:
    ...
private:
    ...
    char data_[SIZE];
    char* cur_;
}
```

例如，往FixedBuffer加入数据FixedBuffer<>::append()：

```c++
    void append(const char* buf, size_t len)
    {
        // FIXME: append partially
        if (implicit_cast<size_t>(avail()) > len) // implicit_cast隐式转换int avail()为size_t
        {
            memcpy(cur_, buf, len);
            cur_ += len;
        }
    }
    int avail() const { return static_cast<int>(end() - cur_); } // 返回Buffer剩余可用空间大小
    const char* end() const { return data_ + sizeof(data_); }    // 返回Buffer内部数组末尾指针
```

##### operator<<格式化数据

针对不同类型数据，LogStream重载了一系列operator<<操作符，用于将数据格式化为字符串，并存入LogStream::buffer_。

```c++
{
    typedef LogStream self;
public:
        ...
    self& operator<<(bool v)

    self& operator<<(short);
    self& operator<<(unsigned short);
    self& operator<<(int);
    self& operator<<(unsigned int);
    self& operator<<(long);
    self& operator<<(unsigned long);
    self& operator<<(long long);
    self& operator<<(unsigned long long);
    self& operator<<(const void*);
    self& operator<<(float v);
    self& operator<<(double);
    self& operator<<(char v);
    self& operator<<(const char* str);
    self& operator<<(const unsigned char* str);
    self& operator<<(const string& v);
    self& operator<<(const StringPiece& v);
    self& operator<<(const Buffer& v);
    ...
}
```

1）对于字符串类型参数，operator<<本质上是调用buffer_对应的FixedBuffer<>::append()，将其存放当到Small Buffer中。

```c++
    self& operator<<(const char* str)
    {
        if (str)
        {
            buffer_.append(str, strlen(str));
        }
        else
        {
            buffer_.append("(null)", 6);
        }
        return *this;
    }
```

2）对于字符类型，跟参数是字符串类型区别是长度只有1，并且无需判断指针是否为空。

```c++
    self& operator<<(char v)
    {
        buffer_.append(&v, 1);
        return *this;
    }
```

3）对于十进制整型，如int/long，则是通过模板函数formatInteger()，将转换为字符串并直接填入Small Buffer尾部。

formatInteger() 并没有用snprintf对整型数据进行格式转换，而是用到了Matthew Wilson提出的高效的转换方法convert()。基本思想是：从末尾开始，对待转换的整型数，由十进制位逐位转换为char类型，然后填入缓存，直到剩余待转数值已为0。

注意：将int等整型转换为string，muduo并没有使用std::to_string，而是使用了效率更高的自定义函数formatInteger()。

```c++
template<typename T>
void LogStream::formatInteger(T v)
{
    if (buffer_.avail() >= kMaxNumericSize) // Small Buffer剩余空间够用
    {
        size_t len = convert(buffer_.current(), v);
        buffer_.add(len);
    }
}

const char digits[] = "9876543210123456789";
const char* zero = digits + 9; // zero pointer to '0'
static_assert(sizeof(digits) == 20, "wrong number of digits");

/* Efficient Integer to String Conversions, by Matthew Wilson. */
template<typename T>
size_t convert(char buf[], T value)
{
    T i = value;
    char* p = buf;

    do {
        int lsd = static_cast<int>(i % 10);
        i /= 10;
        *p++ = zero[lsd];
    } while (i != 0);

    if (value < 0)
    {
        *p++ = '-';
    }
    *p = '\0';
    std::reverse(buf, p);

    return static_cast<size_t>(p - buf);
}
```

4）对于double类型，使用库函数snprintf转换为const char*，并直接填入Small Buffer尾部。

```c++
LogStream::self &LogStream::operator<<(double v)
{
    if (buffer_.avail() >= kMaxNumericSize)
    {
        int len = snprintf(buffer_.current(), kMaxNumericSize, "%.12g", v ); // 将v转换为字符串, 并填入buffer_当前尾部. %g 自动选择%f, %e格式, 并且不输出无意义0. %.12g 最多保留12位小数
        buffer_.add(static_cast<size_t>(len));
    }
    return *this;
}
```

5）对于二进制数，原理同整型数，不过并不以10进制格式存放到Small Buffer，而是以16进制字符串（非NUL结尾）形式，在每个数会加上前缀"0x"。
将二进制内存转换为16进制数的核心函数convertHex，使用了类似于convert的高效转换算法。

```c++
LogStream::self &LogStream::operator<<(const void* p)
{
    uintptr_t v = reinterpret_cast<uintptr_t>(p); // uintptr_t 位数与地址位数相同, 便于跨平台使用
    if (buffer_.avail() >= kMaxNumericSize)       // Small Buffer剩余空间够用
    {
        char* buf = buffer_.current();
        buf[0] = '0';
        buf[1] = 'x';
        size_t len = convertHex(&buf[2], v);
        buffer_.add(len + 2);
    }
    return *this;
}

const char digitsHex[] = "0123465789ABCDEF";
static_assert(sizeof(digitsHex) == 17, "wrong number of digitsHex");

size_t convertHex(char buf[], uintptr_t value)
{
    uintptr_t i = value;
    char* p = buf;

    do
    {
        int lsd = static_cast<int>(i % 16); // last digit for hex number
        i /= 16;
        *p++ = digitsHex[lsd];
    } while (i != 0);

    *p = '\0';
    std::reverse(buf, p);
    return static_cast<size_t>(p - buf);
}
```

注意：uintptr_t 位数跟平台地址位数相同，在64位系统中，占64位；在32位系统中，占32位。使用uintptr_t是为了提高可移植性。

6）对于其他类型，都是转换为以上基本类型，然后再转换为字符串，添加到Small Buffer末尾。

##### staticCheck()静态检查

在operator<<(char void*)和formatInteger(T v)中分别对二进制内存数据、整型数进行格式化时，都有一个判断：Small Buffer剩余空间是否够用，这里面有一个静态常量kMaxNumericSize（默认48）。那么，如何对kMaxNumericSize进行取值呢？48是否合理，如何验证？

这就可以用到staticCheck()进行验证了。目的是为了确保kMaxNumericSize取值，能满足Small Buffer剩余空间一定能存放下要格式化的数据。取数据位较长的double、long double、long、long long，进行static_assert断言。

```c++
void LogStream::staticCheck()
{
    static_assert(kMaxNumericSize - 10 > std::numeric_limits<double>::digits10,
            "kMaxNumericSize is large enough");
    static_assert(kMaxNumericSize - 10 > std::numeric_limits<long double>::digits10,
            "kMaxNumericSize is large enough");
    static_assert(kMaxNumericSize - 10 > std::numeric_limits<long>::digits10,
            "kMaxNumericSize is large enough");
    static_assert(kMaxNumericSize - 10 > std::numeric_limits<long long>::digits10,
            "kMaxNumericSize is large enough");
}
```

std::numeric_limits::digits10 返回T类型的十进制数的有效数字的位数，比如float有效位数6位，double是15位，int是9位。kMaxNumericSize-10 是为了确保kMaxNumericSize足够大，能让Small Buffer可用空间比要转换的位数最长的类型还要多出10byte（1byte容纳一个位数）。

##### 日志后端

前端主要实现异步日志中的日志功能，为用户提供将日志内容转换为字符串，封装为一条完整的log消息存放到RAM中；
而实现异步，核心是通过专门的后端线程，与前端线程并发运行，将RAM中的大量日志消息写到磁盘上。

后端主要包括：AsyncLogging, LogFile, AppendFile，MutexLock。

AsyncLogging 提供后端线程，定时将log缓冲写到磁盘，维护缓冲及缓冲队列。

LogFile 提供日志文件滚动功能，写文件功能。

AppendFile 封装了OS提供的基础的写文件功能。

类图关系如下：
![](https://img2022.cnblogs.com/blog/741401/202203/741401-20220307164814022-1094756529.png)

##### AsyncLogging类

AsyncLogging 主要职责：提供大缓冲Large Buffer（默认4MB）存放多条日志消息，缓冲队列BufferVector用于存放多个Large Buffer，为前端线程提供线程安全的写Large Buffer操作；提供专门的后端线程，用于定时或缓冲队列非空时，将缓冲队列中的Large Buffer通过LogFile提供的日志文件操作接口，逐个写到磁盘上。

##### 数据成员

```c++
/**
* Provide async logging function. backend.
* Background thread (just only one) call this module to write log to file.
*/
class AsyncLogging : noncopyable
{
        ...
private:

    typedef muduo::detail::FixedBuffer<muduo::detail::kLargeBuffer> Buffer; // Large Buffer Type
    typedef std::vector<std::unique_ptr<Buffer>> BufferVector;              // 已满缓冲队列类型
    typedef BufferVector::value_type BufferPtr;   

    const int flushInterval_;                    // 冲刷缓冲数据到文件的超时时间, 默认3秒
    std::atomic<bool> running_;                  // 后端线程loop是否运行标志
    const string basename_;                      // 日志文件基本名称
    const off_t rollSize_;                       // 日志文件滚动大小
    muduo::Thread thread_;                       // 后端线程
    muduo::CountDownLatch latch_;                // 门阀, 同步调用线程与新建的后端线程
    muduo::MutexLock mutex_;                     // 互斥锁, 功能相当于std::mutex
    muduo::Condition cond_ GUARDED_BY(mutex_);   // 条件变量, 与mutex_配合使用, 等待特定条件满足
    BufferPtr currentBuffer_ GUARDED_BY(mutex_); // 当前缓冲
    BufferPtr nextBuffer_ GUARDED_BY(mutex_);    // 空闲缓冲
    BufferVector buffers_ GUARDED_BY(mutex_);    // 已满缓冲队列
}
```

AsyncLogging数据按功能主要分为3部分：1）维护存放log消息的大缓冲Large Buffer；2）后端线程；3）传递给其他类对象的参数，如basename_，rollSize_；

##### LargeBuffer 存放大量log消息

Large Buffer（FixedBuffer[muduo::detail::kLargeBuffer]()）默认大小4MB，用于存储多条log消息；相对的，还有Small Buffer（FixedBuffer[muduo::detail::kSmallBuffer]()）默认大小4KB，用于存储一条log消息。

当前端线程通过调用LOG_XXX << "..."时，如何将log消息传递给后端呢？
可以通过调用AsyncLogging::append()

```c++
void AsyncLogging::append(const char *logline, int len)
{
    muduo::MutexLockGuard lock(mutex_);
    if (currentBuffer_->avail() > len)
    { // current buffer's free space is enough to fill C string logline[0..len-1]
        currentBuffer_->append(logline, static_cast<size_t>(len));
    }
    else
    { // current buffer's free space is not enough
        buffers_.push_back(std::move(currentBuffer_));

        if (nextBuffer_)
        {
            currentBuffer_ = std::move(nextBuffer_);
        }
        else
        {
            currentBuffer_.reset(new Buffer); // rarely happens
        }

        currentBuffer_->append(logline, static_cast<size_t>(len));
        cond_.notify();
    }
}
```

append()可能会被多个前端线程调用，因此必须考虑线程安全，可以用mutex_加锁。
append()基本思路：当前缓冲（currentBuffer_）剩余空间（avail()）足够存放新log消息大小（len）时，就直接存放到当前缓冲；当前缓冲剩余空间不够时，说明当前缓冲已满（或者接近已满），就将当前缓冲move到已满缓冲队列（buffers_），将空闲缓冲move到当前缓冲，再把新log消息存放到当前缓冲中（此时当前缓冲为空，剩余空间肯定够用），最后唤醒等待中的后端线程。

注意：Large Buffer是通过std::unique_ptr指向的，move操作后，原来的 std::unique_ptr就会值为空。

问题：
1）为什么最后要通过cond_唤醒后端线程？
因为没有log消息要记录时，后端线程很可能阻塞等待log消息，当有缓冲满时，及时唤醒后端将已满缓冲数据写到磁盘上，能有效改善新能；否则，短时间内产生大量log消息，可能造成数据堆积，甚至丢失，而后端线程一直休眠（直到3秒超时唤醒）。

2）为什么调用notify()而不是notifyAll()，只唤醒一个线程，而不是唤醒所有线程？
因为一个应用程序通常只有一个日志库后端，而一个后端通常只有一个后端线程，也只会有一个后端线程在该条件变量上等待，因此唤醒一个线程足以。

##### 后端线程 异步写数据到log文件

后端线程的创建就是启动，是在start()中，通过调用Thread::start()完成。门阀latch_目的在于让调用start()线程等待线程函数启动完成，而线程函数中调用latch_.countDown()表示启动完成，当然，前提是latch_计数器初值为1。

```c++
    // Control background thread

    void start()
    {
        running_ = true;
        thread_.start();
        latch_.wait();
    }

    void stop() NO_THREAD_SAFETY_ANALYSIS
    {
        running_ = false;
        cond_.notify();
        thread_.join();
    }
```

而AsyncLogging::stop()用于关闭后端线程，通常是在析构函数中，调用AsyncLogging::stop() 停止后端线程。

```c++
    ~AsyncLogging()
    {
        if (running_)
        {
            stop();
        }
    }
```

后端线程函数threadFunc，会构建1个LogFile对象，用于控制log文件创建、写日志数据，创建2个空闲缓冲区buffer1、buffer2，和一个待写缓冲队列buffersToWrite，分别用于替换当前缓冲currentBuffer_、空闲缓冲nextBuffer_、已满缓冲队列buffers_，避免在写文件过程中，锁住缓冲和队列，导致前端无法写数据到后端缓冲。

threadFunc中，提供了一个loop，基本流程是这样的：
1）每次当已满缓冲队列中有数据时，或者即使没有数据但3秒超时，就将当前缓冲加入到已满缓冲队列（即使当前缓冲没满），将buffer1移动给当前缓冲，buffer2移动给空闲缓冲（如果空闲缓冲已移动的话）。
2）然后，再交换已满缓冲队列和待写缓冲队列，这样已满缓冲队列就为空，待写缓冲队列就有数据了。
3）接着，将待写缓冲队列的所有缓冲通过LogFile对象，写入log文件。
4）此时，待写缓冲队列中的缓冲，已经全部写到LogFile指定的文件中（也可能在内核缓冲中），擦除多余缓冲，只用保留两个，归还给buffer1和buffer2。
5）此时，待写缓冲队列中的缓冲没有任何用处，直接clear即可。
6）将内核高速缓存中的数据flush到磁盘，防止意外情况造成数据丢失。

后端线程函数threadFunc，会构建1个LogFile对象，用于控制log文件创建、写日志数据，创建2个空闲缓冲区buffer1、buffer2，和一个待写缓冲队列buffersToWrite，分别用于替换当前缓冲currentBuffer_、空闲缓冲nextBuffer_、已满缓冲队列buffers_，避免在写文件过程中，锁住缓冲和队列，导致前端无法写数据到后端缓冲。

threadFunc中，提供了一个loop，基本流程是这样的：
1）每次当已满缓冲队列中有数据时，或者即使没有数据但3秒超时，就将当前缓冲加入到已满缓冲队列（即使当前缓冲没满），将buffer1移动给当前缓冲，buffer2移动给空闲缓冲（如果空闲缓冲已移动的话）。
2）然后，再交换已满缓冲队列和待写缓冲队列，这样已满缓冲队列就为空，待写缓冲队列就有数据了。
3）接着，将待写缓冲队列的所有缓冲通过LogFile对象，写入log文件。
4）此时，待写缓冲队列中的缓冲，已经全部写到LogFile指定的文件中（也可能在内核缓冲中），擦除多余缓冲，只用保留两个，归还给buffer1和buffer2。
5）此时，待写缓冲队列中的缓冲没有任何用处，直接clear即可。
6）将内核高速缓存中的数据flush到磁盘，防止意外情况造成数据丢失。

```c++
void AsyncLogging::threadFunc()
{
    assert(running_ == true);
    latch_.countDown();
    LogFile output(basename_, rollSize_, false); // only called by this thread, so no need to use thread safe
    BufferPtr newBuffer1(new Buffer);
    BufferPtr newBuffer2(new Buffer);
    newBuffer1->bzero();
    newBuffer2->bzero();
    BufferVector buffersToWrite;
    static const int kBuffersToWriteMaxSize = 25;

    buffersToWrite.reserve(16); // FIXME: why 16?
    while (running_)
    {
        // ensure empty buffer
        assert(newBuffer1 && newBuffer1->length() == 0);
        assert(newBuffer2 && newBuffer2->length() == 0);
        // ensure buffersToWrite is empty
        assert(buffersToWrite.empty());

        { // push buffer to vector buffersToWrite
            muduo::MutexLockGuard lock(mutex_);
            if (buffers_.empty())
            { // unusual usage!
                cond_.waitForSeconds(flushInterval_); // wait condition or timeout
            }
            // not empty or timeout

            buffers_.push_back(std::move(currentBuffer_));
            currentBuffer_ = std::move(newBuffer1);
            buffersToWrite.swap(buffers_);
            if (!nextBuffer_)
            {
                nextBuffer_ = std::move(newBuffer2);
            }
        }

        // ensure buffersToWrite is not empty
        assert(!buffersToWrite.empty());

        if (buffersToWrite.size() > kBuffersToWriteMaxSize) // FIXME: why 25? 25x4MB = 100MB, 也就是说, 从上次loop到本次loop已经堆积超过100MB, 就丢弃多余缓冲
        {
            char buf[256];
            snprintf(buf, sizeof(buf), "Dropped log message at %s, %zd larger buffers\n",
                     Timestamp::now().toFormattedString().c_str(),
                     buffersToWrite.size() - 2);
            fputs(buf, stderr);
            output.append(buf, static_cast<int>(strlen(buf)));
            buffersToWrite.erase(buffersToWrite.begin() + 2, buffersToWrite.end()); // keep 2 buffer
        }

        // append buffer content to logfile
        for (const auto& buffer : buffersToWrite)
        {
            // FIXME: use unbuffered stdio FILE? or use ::writev ?
            output.append(buffer->data(), buffer->length());
        }

        if (buffersToWrite.size() > 2)
        {
            // drop non-bzero-ed buffers, avoid trashing
            buffersToWrite.resize(2);
        }

        // move vector buffersToWrite's last buffer to newBuffer1
        if (!newBuffer1)
        {
            assert(!buffersToWrite.empty());
            newBuffer1 = std::move(buffersToWrite.back());
            buffersToWrite.pop_back();
            newBuffer1->reset(); // reset buffer
        }

        // move vector buffersToWrite's last buffer to newBuffer2
        if (!newBuffer2)
        {
            assert(!buffersToWrite.empty());
            newBuffer2 = std::move(buffersToWrite.back());
            buffersToWrite.pop_back();
            newBuffer2->reset(); // reset buffer
        }

        buffersToWrite.clear();
        output.flush();
    }
    output.flush();
}
```

**异常处理：**
当已满缓冲队列中的数据堆积（默认缓冲数超过25），就会丢弃多余缓冲，只保留最开始2个。
为什么保留2个？个人觉得2个~16个都是可以的，不过，为了有效减轻log导致的负担，丢弃多余的也未尝不可。

**25的含义：**
25个缓冲，每个4MB，共100MB。也就是说，上次处理周期到本次，已经堆积了超过100MB数据待处理。
假设磁盘的写速度100MB/S，要堆积100MB有2种极端情况：
1）1S内产生200MB数据；
2）25秒内，平均每秒产生104MB数据；

不论哪种情况，都是要超过磁盘的处理速度。而实际应用中，只有产生数据速度不到磁盘写速度的1/10，应用程序性能才不会受到明显影响。

##### LogFile类

LogFile 主要职责：提供对日志文件的操作，包括滚动日志文件、将log数据写到当前log文件、flush log数据到当前log文件。

##### 构造函数

```c++
LogFile::LogFile(const std::string &basename,
                 off_t rollSize,
                 bool threadSafe,    // 线程安全控制项, 默认为true. 当只有一个后端AsnycLogging和后端线程时, 该项可置为false
                 int flushInterval,
                 int checkEveryN)
        : basename_(basename),  // 基础文件名, 用于新log文件命名
        rollSize_(rollSize),    // 滚动文件大小
        flushInterval_(flushInterval), // 冲刷时间限值, 默认3 (秒)
        checkEveryN_(checkEveryN),     // 写数据次数限值, 默认1024
        count_(0),                     // 写数据次数计数, 超过限值checkEveryN_时清除, 然后重新计数
        mutex_(threadSafe ? new MutexLock : NULL), // 互斥锁指针, 根据是否需要线程安全来初始化
        startOfPeriod_(0),             // 本次写log周期的起始时间(秒)
        lastRoll_(0),                  // 上次roll日志文件时间(秒)
        lastFlush_(0)                  // 上次flush日志文件时间(秒)
{
    assert(basename.find('/') == string::npos); // basename不应该包含'/', 这是路径分隔符
    rollFile();
}
```

重新启动时，可能并没有log文件，因此在构建LogFile对象时，直接调用rollFile()以创建一个全新的日志文件。

##### 滚动日志文件

当日志文件接近指定的滚动限值（rollSize）时，需要换一个新文件写数据，便于后续归档、查看。调用LogFile::rollFile()可以实现文件滚动。

```c++
bool LogFile::rollFile()
{
    time_t now = 0;
    string filename = getLogFileName(basename_, &now);
    time_t start = now / kRollPerSeconds_ * kRollPerSeconds_;

    if (now > lastRoll_)
    { // to avoid identical roll by roll time
        lastRoll_ = now;
        lastFlush_ = now;
        startOfPeriod_ = start;
        // create new log file with new filename
        file_.reset(new FileUtil::AppendFile(filename));
        return true;
    }
    return false;
}
```

滚动日志文件操作的关键是：1）取得新log文件名，文件名全局唯一；2）创建并打开一个新log文件，用指向LogFile对象的unique_ptr指针file_表示。

**异常处理：**
滚动操作会新建一个文件，而为避免频繁创建新文件，rollFile会确保上次滚动时间到现在如果不到1秒，就不会滚动。

注意：是否滚动日志文件的条件判断，并不在rollFile，而是在写数据到log文件的LogFile::append_unlocked()中，因为写新数据的时候，是判断当前log文件大小是否足够大的最合适时机。而rollFile只用专门负责如何滚动log文件即可。

##### 日志文件名

getLogFileName根据调用者提供的基础名，以及当前时间，得到一个全新的、唯一的log文件名。或许叫nextLogFileName更合适。

```c++
string LogFile::getLogFileName(const string &basename, time_t *now) // static
{
    string filename;
    filename.reserve(basename.size() + 64); // extra 64 bytes for timestamp etc.
    filename = basename;

    char timebuf[32];
    struct tm tmbuf;
    *now = time(NULL);
    gmtime_r(now, &tmbuf); // FIXME: localtime_r ?
    strftime(timebuf, sizeof(timebuf), ".%Y%m%d-%H%M%S.", &tmbuf);
    filename += timebuf;

    filename += ProcessInfo::hostname();

    char pidbuf[32];
    snprintf(pidbuf, sizeof(pidbuf), ".%d", ProcessInfo::pid());
    filename += pidbuf;
    filename += ".log";

    return filename;
}
```

gmtime_r获取的是gmt时区时间，localtime_r获取的是本地时间。

新log文件名格式：

```bash
basename + now + hostname + pid + ".log"

basename 基础名, 由用户指定, 通常可设为应用程序名
now 当前时间, 格式: "%Y%m%d-%H%M%S"
hostname 主机名
pid 进程号, 通常由OS提供, 通过getpid获取
".log" 固定后缀名, 表明这是一个log文件
各部分之间, 用"."连接
```

如下面是一个根据basename为"test_log_mt"生成的log文件名：

```bash
test_log_mt.20220218-134000.ubuntu.12426.log
```

##### 写日志文件操作

LogFile提供了2个接口，用于向当前日志文件file_写入数据。append本质上是通过append_unlocked完成对日志文件写操作，但多了线程安全。用户只需调用第一个接口即可，append会根据线程安全需求，自行判断是否需要加上；第二个是private接口。

```c++
void append(const char *logline, int len);
void append_unlocked(const char *logline, int len);
```

append_unlocked 会先将log消息写入file_文件，之后再判断是否需要滚动日志文件；如果不滚动，就根据append_unlocked的调用次数和时间，确保1）一个log文件超时（默认1天），就创建一个新的；2）flush文件操作，不会频繁执行（默认间隔3秒）。

```c++
void LogFile::append_unlocked(const char *logline, int len)
{
    file_->append(logline, len);

    if (file_->writtenBytes() > rollSize_)
    { // written bytes to file_ > roll threshold (rollSize_)
        rollFile();
    }
    else
    {
        ++count_;
        if (count_ >= checkEveryN_)
        {
            count_ = 0;
            time_t now = ::time(NULL);
            time_t thisPeriod_ = now / kRollPerSeconds_ * kRollPerSeconds_;

            if (thisPeriod_ != startOfPeriod_)
            { // new period, roll file for log
                rollFile();
            }
            else if (now - lastFlush_ > flushInterval_)
            { // timeout ( flushInterval_ = 3 seconds)
                lastFlush_ = now;
                file_->flush();
            }
        }
    }
}
```

append如何根据需要选择是否线程安全地调用append_unlocked？
可以根据mutex_是否为空。因为构造时，根据用户传入的threadSafe实参，决定了mutex_是否为空。

```c++
void LogFile::append(const char *logline, int len)
{
    if (mutex_)
    {
        MutexLockGuard lock(*mutex_);
        append_unlocked(logline, len);
    }
    else
    {
        append_unlocked(logline, len);
    }
}
```

##### flush日志文件

flush操作往往与write文件操作配套。LogFile::flush实际上是通过AppendFile::flush()，完成对日志文件的冲刷。与LogFile::append()类似，flush也能通过mutex_指针是否为空，自动选择线程安全版本，还是非线程安全版本。

```c++
void LogFile::flush()
{
    if (mutex_)
    {
        MutexLockGuard lock(*mutex_);
        file_->flush();
    }
    else
    {
        file_->flush();
    }
}
```

##### AppendFile类

AppendFile位于FileUtil.h/.cc，封装了OS提供的，底层的创建/打开文件、写文件、关闭文件等操作接口，并没有专门考虑线程安全问题。线程安全由上一层级调用者，如LogFile来保证。

##### 数据结构

AppendFile的数据结构较简单，

```c++
// not thread safe
class AppendFile : public noncopyable
{
public:
    explicit AppendFile(StringArg filename);
    ~AppendFile();
    void append(const char* logline, size_t len);  // 添加log消息到文件末尾
    void flush();                                  // 冲刷文件到磁盘
    off_t writtenBytes() const { return writtenBytes_; } // 返回已写字节数
private:
    size_t write(const char* logline, size_t len);       // 写数据到文件

    FILE* fp_;                                // 文件指针
    char buffer_[ReadSmallFile::kBufferSize]; // 文件操作的缓冲区
    off_t writtenBytes_;                      // 已写字节数
};
```

##### RAII方式打开、关闭文件

AppendFile采用RAII方式管理文件资源，构建对象即打开文件，销毁对象即关闭文件。

```c++
AppendFile::AppendFile(StringArg filename)
: fp_(::fopen(filename.c_str(), "ae")), // 'e' for O_CLOEXEC
writtenBytes_(0)
{
    assert(fp_);
    ::setbuffer(fp_, buffer_, sizeof(buffer_)); // change stream fp_'s buffer to buffer_
#if 0
    // optimization for predeclaring an access pattern for file data
    struct stat statbuf;
    fstat(fd_, &statbuf);
    ::posix_fadvise(fp_, 0, statbuf.st_size, POSIX_FADV_DONTNEED);
#endif
}

AppendFile::~AppendFile()
{
    ::fclose(fp_);
}
```

为posix_fadvise(2)指定POSIX_FADV_DONTNEED选项，告诉内核在近期不会访问文件的指定数据，以便内核对其进行优化。

##### 写数据到文件

AppendFile有个两个接口：append和write。其中，append()是供用户调用的public接口，确保将指定数据附加到文件末尾，实际的写文件操作是通过write()来完成的；write通过非线程安全的glibc库函数fwrite_unlocked()来完成写文件操作，而没有选择线程安全的fwrite()，主要是出于性能考虑。

一个后端通常只有一个后端线程，一个LogFile对象，一个AppendFile对象，这样，也就只会有一个线程写同一个log文件。

```c++
void AppendFile::append(const char *logline, size_t len)
size_t AppendFile::write(const char *logline, size_t len);
```

append和write实现：

```c++
void AppendFile::append(const char *logline, size_t len)
{
    size_t written = 0;

    /* write len byte to fp_ unless complete writing or error occurs */
    while (written != len)
    {
        size_t remain = len - written;
        size_t n = write(logline + written, remain);
        if (n != remain)
        {
            int err = ferror(fp_);
            if (err)
            {
                fprintf(stderr, "AppendFile::append() failed %s\n", strerror_tl(err));
                clearerr(fp_); // clear error indicators for fp_
                break;
            }
        }
        written += n;
    }
    writtenBytes_ += written;
}

size_t AppendFile::write(const char *logline, size_t len)
{
    // not thread-safe
    return ::fwrite_unlocked(logline, 1, len, fp_);
}
```

可以看出，append是通过一个循环来确保所有数据都写到磁盘文件上，除非发生错误。

#### Thread

##### Thread类

类图
![image](https://img2020.cnblogs.com/blog/2318601/202106/2318601-20210609230212540-1292511011.png)

这里涉及到一个**线程标识符tid**
Linux中，每个进程有一个pid，类型pid_t，由getpid()取得。Linux下的POSIX线程也有一个id，类型 pthread_t，由pthread_self()取得，该id由线程库维护，其id空间是各个进程独立的（即不同进程中的线程可能有相同的id）。Linux中的POSIX线程库实现的线程其实也是一个进程（LWP），只是该进程与主进程（启动线程的进程）共享一些资源而已，比如代码段，数据段等。
有时候我们可能需要知道线程的真实pid。比如进程P1要向另外一个进程P2中的某个线程发送信号时，既不能使用P2的pid，更不能使用线程的pthread id，而只能使用该线程的真实pid，称为tid。
有一个函数gettid()可以得到tid，但glibc并没有实现该函数，只能通过Linux的系统调用syscall来获取。
return syscall(SYS_gettid)

```cpp
class Thread : noncopyable
{
 public:
  typedef std::function<void ()> ThreadFunc;  //线程函数

  explicit Thread(ThreadFunc, const string& name = string()); //构造
  ~Thread(); //析构

  void start();
  int join(); // return pthread_join()

  bool started() const { return started_; }
  pid_t tid() const { return tid_; }   //线程真实pid
  const string& name() const { return name_; } //线程名字

  static int numCreated() { return numCreated_.get(); } //已经创建的线程数

 private:
  void setDefaultName();  //设置默认name_

  bool       started_;  
  bool       joined_;
  pthread_t  pthreadId_;
  pid_t      tid_;     //线程标识符
  ThreadFunc func_;
  string     name_;   
  CountDownLatch latch_; //计数+条件变量，计数等于0时条件变量做通知

  static AtomicInt32 numCreated_; //创建线程数，原子性操作
};
```

##### 构造函数

主要对数据成员进行初始化，还没创建线程执行入口函数。

```cpp
Thread::Thread(ThreadFunc func, const string& n)
  : started_(false),
    joined_(false),
    pthreadId_(0),
    tid_(0),
    func_(std::move(func)),
    name_(n),
    latch_(1)
{
  setDefaultName();
}

void Thread::setDefaultName()
{
  int num = numCreated_.incrementAndGet();
  if (name_.empty()) //没有设置名字，使用默认的name
  {
    char buf[32];
    snprintf(buf, sizeof buf, "Thread%d", num);
    name_ = buf;
  }
}
```

##### start()方法

将数据封装在ThreadData类，执行pthread_create创建线程。

```cpp
void Thread::start()
{
  assert(!started_);
  started_ = true;
  // FIXME: move(func_)
  detail::ThreadData* data = new detail::ThreadData(func_, name_, &tid_, &latch_);
  if (pthread_create(&pthreadId_, NULL, &detail::startThread, data))
  {
    started_ = false;
    delete data; // or no delete?
    LOG_SYSFATAL << "Failed in pthread_create";
  }
  else
  {
    latch_.wait(); //主线程等待子线程初始化完毕才开始工作，在runInThread()中
    assert(tid_ > 0);
  }
}
```

线程执行函数
使用了CountDownLatch类，这个类主要成员是计数值和条件变量，当计数值为0，条件变量发出通知。
既可以用于所有子线程等待主线程发起 “起跑” ；
也可以用于主线程等待子线程初始化完毕才开始工作。

```cpp
//线程入口函数
//
void* startThread(void* obj)
{
  ThreadData* data = static_cast<ThreadData*>(obj);
  data->runInThread();
  delete data;
  return NULL;
}

 void runInThread()
 {
    *tid_ = muduo::CurrentThread::tid();//Thread类获取tid
    tid_ = NULL;
    latch_->countDown();//计数减一，因为初始化为1，为零时条件变量通知主线程初始化完成
    latch_ = NULL;

    muduo::CurrentThread::t_threadName = name_.empty() ? "muduoThread" : name_.c_str();
    ::prctl(PR_SET_NAME, muduo::CurrentThread::t_threadName);
    try
    {
      func_();//执行用户函数
      muduo::CurrentThread::t_threadName = "finished";
    }
    catch (const Exception& ex) //捕捉异常
    {
      muduo::CurrentThread::t_threadName = "crashed";
      fprintf(stderr, "exception caught in Thread %s\n", name_.c_str());
      fprintf(stderr, "reason: %s\n", ex.what());
      fprintf(stderr, "stack trace: %s\n", ex.stackTrace());
      abort();
    }
    catch (const std::exception& ex)
    {
      muduo::CurrentThread::t_threadName = "crashed";
      fprintf(stderr, "exception caught in Thread %s\n", name_.c_str());
      fprintf(stderr, "reason: %s\n", ex.what());
      abort();
    }
    catch (...)
    {
      muduo::CurrentThread::t_threadName = "crashed";
      fprintf(stderr, "unknown exception caught in Thread %s\n", name_.c_str());
      throw; // rethrow
    }
  }
};
```

##### tid的获取

在ThreadData::runInThread()中调用了CurrentThread::tid()给Thread获取线程tid。
前面说到，有一个函数gettid()可以得到tid，但glibc并没有实现该函数，只能通过Linux的系统调用syscall来获取。
muduo中使用系统调用获取tid后保存在__thread变量中，减少多次系统调用。
CurrentThread并是不一个类，该命名空间中的是线程自己保留的变量，用__thread关键字修饰。
__thread变量是每个线程都有一份独立实体，各个线程的变量值互不干扰。

```cpp
  extern __thread int t_cachedTid;
  extern __thread char t_tidString[32];
  extern __thread int t_tidStringLength;
  extern __thread const char* t_threadName;
```

CurrentThread::tid()

```cpp
//CurrentThread.h
inline int tid()
 {
    if (__builtin_expect(t_cachedTid == 0, 0))
    {
      cacheTid();
    }
    return t_cachedTid;
 }

//Thread.h
void CurrentThread::cacheTid()
{
  if (t_cachedTid == 0)
  {
    t_cachedTid = detail::gettid();
    t_tidStringLength = snprintf(t_tidString, sizeof t_tidString, "%5d ", t_cachedTid);
  }
}
//Thread.h
//namespace detail

pid_t gettid()
{ 
  //线程的真实tid
  return static_cast<pid_t>(::syscall(SYS_gettid));
}
```

#### Timestamp

##### Timestamp类设计

![](https://img2022.cnblogs.com/blog/741401/202202/741401-20220227201122493-1521296483.png)

由于时间戳希望在不同变量之间赋值、拷贝，因此设计成值语义的，继承自copyable class。

数据成员：
成员变量microSecondsSinceEpoch_，用来来表示从 Epoch时间到目前为止的微妙数，初值0（也表示无效值）。
microSecondsSinceEpoch_的数据类型为什么是int64_t，而不是int32_t或者uint64_t？因为32位连一年的微妙数都不能表示，而int64_t可以表示290余年的微妙数（一年按365*24*3600*100000计算），未来还能表示一百余年，也就是说，其范围满足目前日常需求。而有符号的int64_t可以用来让2个时间戳进行差值计算，从而表示先后顺序。当然，时间戳本身为负数没有意义。

##### 构造函数（ctor）

可以像这样定义Timstamp及其构造函数：

```c++
/**
* Time stamp in UTC, in microseconds resolution.
*/
class Timestamp : public copyable
{
public:
    /**
     * Constructs an invalid Timestamp
     */
    Timestamp() : microSecondsSinceEpoch_(0)
    { }

    /**
     * Constructs a Timestamp at specific time
     * @param microSecondsSinceEpochArg
     */
    explicit Timestamp(int64_t microSecondsSinceEpochArg)
            : microSecondsSinceEpoch_(microSecondsSinceEpochArg)
    {
    }
    ...

private:
    int64_t microSecondsSinceEpoch_;
};
```

1）继承自copyable，表明这是一个值语义的class，其对象能够进行copy操作；

2）default ctor（构造函数），存储时间戳变量microSecondsSinceEpoch_初值0，0和负数都表示无效值。同时，也提供单一参数版本ctor，给调用者构造指定时间戳值的Timestamp对象的机会。

##### 对象有效性

至于microSecondsSinceEpoch_符号，我们可以定义成员函数valid()判断其有效性，通过invalid()构造一个无效的Timestamp对象。

```c++
// Timestamp.h
    bool valid() const
    { return microSecondsSinceEpoch_ > 0; }

    static Timestamp invalid()
    {
        return Timestamp();
    }
```

1）为什么需要invalid()？
有时，我们需要一个临时的Timestamp对象，并不会用它来表示真实的时间，而是表示无用Timestamp对象。此时，用invalid()类函数自然是没问题的。当然，也可以用default ctor来构造一个临时Timestamp对象也是可以的，invalid()实现也是这么做的，但invalid()语义更清晰，而且不依赖于default ctor实现。假设哪天修改了microSecondsSinceEpoch_含义，那么用default ctor来代表无效Timestamp对象，也就失效了，然而invalid()接口却可以不变，客户端不用修改代码。

2）为什么invalid()是static？
因为要构造一个Timestamp对象，也就是说，此时还没有Timestamp对象，也就无法通过对象的成员函数来构造自身对象。

##### 获得当前时间（时刻）

Timestamp的一个重要意义，在于捕获当前时间，转换为时间戳反馈给调用者。

```c++
// Timestamp.h
    /**
     * Get time of now
     */
    static Timestamp now();
    static const int kMicroSecondsPerSecond = 1000 * 1000; // 1s = 1e6 us

// Timestamp.cc
Timestamp Timestamp::now()
{
    struct timeval tv;
    gettimeofday(&tv, NULL);
    int64_t seconds = tv.tv_sec;
    return Timestamp(seconds * kMicroSecondsPerSecond + tv.tv_usec);
}
```

1）用gettimeofday()获取当前时刻，转化为微秒，并构造一个Timestamp临时对象。1换算公式：sec = 1e6 usec

##### 时间换算

如何将由time(2)获得的自Epoch时间（1970-01-01 00:00:00 +0000 (UTC).）以来的秒数（time_t类型），转化为Timestamp类型对象？
可以定义fromUnixTime来完成这个工作：

```c++
// Timestamp.h
    static Timestamp fromUnixTime(time_t t)
    {
        return fromUnixTime(t, 0);
    }
    static Timestamp fromUnixTime(time_t t, int microseconds)
    {
        return Timestamp(static_cast<int64_t>(t) * kMicroSecondsPerSecond + microseconds);
    }
```

1）第一个重载版本，只转换提供的秒数，微秒数默认0；第二个版本，提供了秒数和微秒数的设置

##### 对象交换

有时为了避免对象数据成员的拷贝，会利用swap对对象进行交换操作。

```c++
// Timestamp.h
    void swap(Timestamp& that)
    {
        std::swap(microSecondsSinceEpoch_, that.microSecondsSinceEpoch_);
    }
```

1）就目前的设计来说，完全可以用std::swap来交换2个对象，而不用定义Timestamp::swap()。这里是为了以后方便扩展，自定义swap行为。

##### 获取时间戳

获取从Epoch时间，到目前为止的时间戳数值

```c++
// Timestamp.h
    int64_t microSecondsSinceEpoch() const { return microSecondsSinceEpoch_;}; // 微秒数
    time_t secondsSinceEpoch() const // 秒数
    {
        return static_cast<time_t>(microSecondsSinceEpoch_ / kMicroSecondsPerSecond);
    }
```

1）获取微秒数；
2）获取秒数；

##### 获取可打印字符串

```c++
// Timestamp.h
    std::string toString() const;
    std::string toFormattedString(bool showMicroseconds = true) const;

#ifndef __STDC_FORMAT_MACROS // PRId64, for printf data in cross platform
#define __STDC_FORMAT_MACROS
#endif

#include <inttypes.h>

string Timestamp::toString() const
{
    char buf[32] = {0};
    int64_t seconds = microSecondsSinceEpoch_ / kMicroSecondsPerSecond;
    int64_t microseconds = microSecondsSinceEpoch_ % kMicroSecondsPerSecond;
    snprintf(buf, sizeof(buf), "%" PRId64 ".%06" PRId64 "", seconds, microseconds);
    return buf;
}
string Timestamp::toFormattedString(bool showMicroseconds) const
{
    char buf[64] = {0};
    time_t seconds = static_cast<time_t>(microSecondsSinceEpoch_ / kMicroSecondsPerSecond);
    struct tm tm_time;
    gmtime_r(&seconds, &tm_time); // convert seconds since Epoch to UTC time (struct tm)

    if (showMicroseconds)
    {
        int microseconds = static_cast<int>(microSecondsSinceEpoch_ % kMicroSecondsPerSecond);
        snprintf(buf, sizeof(buf), "%4d%02d%02d %02d:%02d:%02d.%06d",
                 tm_time.tm_year + 1990, tm_time.tm_mon + 1, tm_time.tm_mday,
                 tm_time.tm_hour, tm_time.tm_min, tm_time.tm_sec,
                 microseconds);
    }
    else
    {
        snprintf(buf, sizeof(buf), "%4d%02d%02d %02d:%02d:%02d",
                 tm_time.tm_year + 1990, tm_time.tm_mon + 1, tm_time.tm_mday,
                 tm_time.tm_hour, tm_time.tm_min, tm_time.tm_sec);
    }
    return buf;
}
```

1）toString() 将秒数、微秒数转换为可打印的std::string类型，用PRId64跨平台输出64bit数据到string缓存；
2）toFormattedString() 将时间戳转换为人类可理解的格式化时间字符串，形如"yyyymmdd hh:mm:ss.zzzzzz"。

##### 辅助函数（非class member函数）

常需要比较2个时间先后顺序，计算这2个时刻之间的时间差，一个时刻加上一段时间来得到另外一个时刻，可以通过定义helper函数来实现：

```c++
inline bool operator<(Timestamp lhs, Timestamp rhs)
{
    return lhs.microSecondsSinceEpoch() < rhs.microSecondsSinceEpoch();
}

/**
* Gets time difference of two timestamps, result in seconds.
* @param high
* @param low
* @return (high - low) in seconds.
* @c double has 52-bit precision, enough for one-microsecond
* resolution for next 100 years.
*/
inline double timeDifference(Timestamp high, Timestamp low)
{
    int64_t diff = high.microSecondsSinceEpoch() - low.microSecondsSinceEpoch();
    return static_cast<double>(diff) / Timestamp::kMicroSecondsPerSecond;
}

/**
* Add @c seconds to given timestamp.
* @param timestamp given basic timestamp
* @param seconds given seconds to be added to timestamp
* @return timestamp + seconds as Timestamp
*/
inline Timestamp addTime(Timestamp timestamp, double seconds)
{
    int64_t delta = static_cast<int64_t>(seconds * Timestamp::kMicroSecondsPerSecond);
    return Timestamp(timestamp.microSecondsSinceEpoch() + delta);
}
```

1）operator< 除了比较2个时间戳大小关系（代表的先后顺序），也是实现等价关系判断的重要条件；
2）timeDifference() 计算2个时间戳差值，精确到1usec，用小数表示，而整数部分表示1sec；
3）addTime() 利用一个基准时间戳timestamp + 时间段seconds(秒数)，得到新的Timestamp对象。

### 事件分发

事件分发部分类图如下图 4-? 所示：
![img](./image/%E4%BA%8B%E4%BB%B6%E5%88%86%E5%8F%91%E7%B1%BB%E5%9B%BE.png)

#### EventLoop

基于 3.2.3 节中关于 EventLoop 功能的讨论和图 4-? 中 EventLoop 类图，其设计和实现可分为四个部分：

+ 提供事件循环。
+ 运行定时任务。
+ 处理激活通道事件。
+ 线程安全。

##### 事件循环

事件循环主要有以下四个接口，loop()、quit()、wakeup()、handleRead()。

loop() 是EventLoop 实现事件循环的核心函数。其利用 Poller::poll() 函数将激活事件填入 Channel 队列 activeChannels_，然后排队执行每个 Channel 的 handleEvent，从而调用对应激活事件的回调函数。

quit() 函数通过修改 quit_ 的值提供了退出事件循环的接口。

wakeup() 函数用于通过向类内自创建的 wakeupFd_ 文件描述符写入信息来唤醒被阻塞的事件循环。

handleRead() 函数用于读取 wakeupFd_ 文件描述符中的信息来完成可读事件。

##### 处理激活通道事件

处理激活通道事件的接口有以下三个：updateChannel()、removeChannel()、hasChannel()。

updateChannel()、removeChannel() 利用 Poller::updateChannel 和 Poller::removeChannel 更新或移除 Poller 监听的事件。

hasChannel() 来判断 Poller 是否正在监听某个 Channel。

##### 定时任务

定时任务的接口有以下四个：runAt()、runAfter()、runEvery()、cancel()。其中前三个接口利用 TimerQueue::addTimer() 函数添加一次性定时任务或周期定时任务，最后一个接口调用 TimerQueue::cancel() 函数取消定时任务。

##### 线程安全

一个 EventLoop 的实例的接口函数可能会出现多个线程并发执行的情况，这使得保证类私有数据的线程安全就格外重要。所以 EventLoop 在设计上采用“one loop per thread”的设计思想——即一个事件循环（EventLoop）对应一个线程，这在大部分情况是一个非常好的线程模型，大大减少了因数据竞争而产生的并发问题。[10]

同时在具体实现和使用时，本软件使用了以下四种种方法来实现以上的设计思想：

+ 在每个子线程中设置其自己拥有的全局 EventLoop 变量 t_loopInThisThread，用以保证每个子线程中都运行唯一的 EventLoop 实例。
+ 实现 isInLoopThread、assertInLoopThread 函数用以判断调用函数线程和 EventLoop 事件循环线程是否是同一线程，以保证事件循环函数始终在同一线程中执行。
+ 实现 runInLoop、queueInLoop 函数，用以处理来自不同线程的用户任务，具体分为以下两种情况：
  + 若当前线程是 EventLoop 事件循环线程，则立即执行用户任务。
  + 反之，则加入用户任务队列 pendingFunctors_ 中，唤醒事件循环后排队执行。
+ 用户任务队列 pendingFunctors_ 使用互斥锁保证不同线程之间使用用户任务队列的线程安全。

#### EventLoopThread

EventLoopThread 的结构比较简单，对外只提供启动 I/O 线程的接口 startLoop()。

##### startLoop

startLoop() 接口过程就是启动在初始化时创建的 I/O 线程，然后等待 I/O 线程函数执行 threadFunc() 函数完成初始化，并将 loop_ 同步值返回给函数调用者。

##### I/O 线程函数 threadFunc

I/O 线程函数主要工作是创建 EventLoop 局部对象，并将其值同步至 EventLoop 的 loop_，然后运行事件循环。

##### 线程安全

从以上两节介绍中可以看出 EventLoopThread 中的 loop_ 指针在创建时（startLoop()中），会存在调用线程和子线程函数 threadFunc 并发读、写 loop_ 的情况。为保证 loop_ 的数据线程安全，在实现时采用了以下两种方式：

+ 采用互斥锁在读写时保护 loop_，同时使用条件变量来同步 loop_ 的值。
+ I/O 线程函数退出时，线程中事件循环已不在运行，这时应清空 loop_ 的值，否则可能会导致析构函数重复调用 loop_ 的 quit() 函数，让 I/O 线程循环重复退出。

#### EventLoopThreadPool

EventLoopThreadPool 线程池类通常由 main 函数线程创建，绑定 main 函数线程创建的 EventLoop 到 baseLoop_ 中。

##### start

线程池在创建后，通过调用 start() 接口来启动线程池。其主要工作如下：

+ 保证 baseLoop_ 所属线程调用 start() 接口。
+ 创建用户指定数量的线程（其数量由用户调用 setThreadNum() 接口确定），并启动线程，记录子线程对应 EventLoop。
+ 若没有指定线程数量（或为指定 0），调用用户指定的线程初始化回调函数。

##### getNextLoop

getNextLoop() 接口从 I/O 线程池维护的 EventLoop 数组 loops_ 中轮询取得一个 EventLoop 对象，每次调用取下一个 EventLoop 对象（若取到最后一个，则下一个是数组中第一个 EventLoop 对象），利用此分配策略以达到负载均衡的目的。

### 网络协议

#### TCP

muduo使用TcpConnection类来管理TCP连接，使用接受器Acceptor来接受连接，连接器Connector发起连接。TcpServer管理accept获得TcpConnection，生命周期由用户控制。

下图是TcpServer新建连接的相关函数调用顺序。当Channel::handleEvent()的触发条件是listening socket可读时，表明有新连接请求达到。TcpServer为新连接创建对应的TcpConnection对象。

![](https://img2022.cnblogs.com/blog/741401/202203/741401-20220324083202877-1291752519.png)

##### Acceptor类

Acceptor是TcpServer的一个内部类，主要职责是用来获得新连接的fd。保存用户提供的Connection-Callback和MessageCallback，新建TcpConnection对象（newConn()）的时候直接传递给TcpConnection的构造函数。

```c++
/**
* TCP连接接受器
* 基础调用为accept(2)/accept4(2)
*/
class Acceptor : private noncopyable
{
public:
    typedef std::function<void(int sockfd, const InetAddress &)> NewConnectionCallback;

    Acceptor(EventLoop* loop, const InetAddress& listenAddr, bool reuseport);
    ~Acceptor();

    /* 设置新连接回调 */
    void setNewConnectionCallback(const NewConnectionCallback& cb)
    { newConnectionCallback_ = cb; }

    /* 监听本地端口 */
    void listen();

    /* 判断当前是否正在监听端口 */
    bool listening() const { return listening_; }

private:
    void handleRead();      // 处理读事件

    EventLoop *loop_;        // 所属EventLoop
    Socket acceptSocket_;    // 专门用于接受连接的套接字(sock fd)
    Channel acceptChannel_;  // 专门接受连接通道, 监听conn fd
    NewConnectionCallback newConnectionCallback_; // 新建连接回调
    bool listening_;         // 监听状态
    int idleFd_;             // 空闲fd, 用于fd资源不够用时, 可以空一个出来作为新建连接conn fd
};
```

如果fd资源不够用了，导致accept(2)/accept4(2)创建连接失败，比如达到系统上限，怎么办？
Accetor用了这样一种技术：先申请一个空闲的fd（idleFd_），等到发生由于fd资源不够用时，就把这个备用fd暂时用于accept接收连接，然后再马上关闭，以防止不断产生可读事件（连接请求），从而回调相同的失败代码。及早建立连接后并关闭连接，让程序不会频繁响应同一个连接请求。

##### Acceptor构造与析构

Acceptor构造时，创建sockfd（套接字），待后续交给TcpServer来start监听套接字。
空闲fd指向文件"/dev/null"，用来解决服务器fd资源耗尽问题。

```c++
// Acceptor.cc
Acceptor::Acceptor(EventLoop *loop, const InetAddress &listenAddr, bool reuseport)
: loop_(loop),
  acceptSocket_(sockets::createNonblockingOrDie(listenAddr.family())),
  acceptChannel_(loop, acceptSocket_.fd()),
  listening_(false),
  idleFd_(::open("/dev/null", O_RDONLY | O_CLOEXEC)) // 申请空闲fd
{
    assert(idleFd_ >= 0);
    acceptSocket_.setReuseAddr(true);
    acceptSocket_.setReusePort(reuseport);
    acceptSocket_.bindAddress(listenAddr);
    acceptChannel_.setReadCallback(
            std::bind(&Acceptor::handleRead, this));
}

Acceptor::~Acceptor()
{
    acceptChannel_.disableAll(); // disable all event of the channel
    acceptChannel_.remove(); // remove the channel from poller
    ::close(idleFd_);
}
```

##### Acceptor监听

Acceptor包含2类监听：1）监听套接字，即本地ip地址&端口。2）监听通道事件，读事件。

* 为什么不在构造时，就调用listen监听sockfd呢？
  将非必要资源的初始化，延迟到需要时，用户可以通过调用TcpSever::start()来启动。这样，用户可以更灵活控制资源的申请和释放。

```c++
// Acceptor.cc
/**
* 监听本地sock fd, 使能监听Channel读事件
*/
void Acceptor::listen()
{
    loop_->assertInLoopThread();
    listening_ = true;
    acceptSocket_.listen();         // 使能监听本地sock fd(ip, port)
    acceptChannel_.enableReading(); // 使能监听通道读事件
}
```

##### Acceptor接受连接

Acceptor内部有一个Channel成员，当Poller监听到有Tcp连接请求时，就通过Channel的可读事件，在loop线程，来回调Acceptor::handleRead()。从而将conn fd和IP地址传递给上一层TcpServer，用于创建TcpConnection对象管理Tcp连接。

```c++
// Acceptor.cc
/**
* 处理读Channel事件, accept连接
* @note 先accept, 然后将相关资源通过回调交由上一层的TcpServer进行处理(管理)
*/
void Acceptor::handleRead()
{
    loop_->assertInLoopThread();
    InetAddress peerAddr;
    // FIXME loop until no more
    int connfd = acceptSocket_.accept(&peerAddr); // 获取连接fd及对端ip地址
    if (connfd >= 0)
    {
        if (newConnectionCallback_)
        { // 创建新连接回调
            newConnectionCallback_(connfd, peerAddr);
        }
        else
        {
            sockets::close(connfd);
        }
    }
    else
    { // 错误
        LOG_SYSERR << "in Acceptor::handleRead";
        /*
         * Read the section named "The special problem of
         * accept()ing when you can't" in libev's doc.
         * By Marc Lehmann, author of libev.
         *
         * The per-process limit of open file descriptors has been reached.
         */
        if (errno == EMFILE)
        { // 文件描述符资源耗尽错误
            ::close(idleFd_);
            idleFd_ = ::accept(acceptSocket_.fd(), NULL, NULL);
            ::close(idleFd_);
            // reopen /dev/null, it dose not matter whether it succeeds or fails.
            idleFd_ = ::open("/dev/null", O_RDONLY | O_CLOEXEC);
        }
    }
}
```

##### TcpServer类

TcpServer类管理TcpConnection，供用户直接使用，生命周期由用户控制。接口如下，用户只需要设置好callback，然后调用start()即可。

```c++
// TcpServer.h
/**
* Tcp Server, 支持单线程和thread-poll模型.
* 接口类, 因此不要暴露太多细节.
*/
class TcpServer : private noncopyable
{
public:
    typedef std::function<void (EventLoop*)> ThreadInitCallback;
    enum Option
    enum Option
    {
        kNoReusePort, // 不允许重用本地端口
        kReusePort,   // 允许重用本地端口
    };

//    TcpServer(EventLoop* loop, const InetAddress& listenAddr);
    TcpServer(EventLoop* loop,
              const InetAddress& listenAddr,
              const std::string& nameArg,
              Option option = kNoReusePort);
    ~TcpServer(); // force out-line dtor, for std::unique_ptr members.

    /**
     * 如果没有监听, 就启动服务器(监听).
     * 多次调用没有副作用.
     * 线程安全.
     */
    void start();

    /**
     * 设置连接回调.
     * 非线程安全.
     */
    void setConnectionCallback(const ConnectionCallback& cb)
    { connectionCallback_ = cb; }

    /**
     * 设置消息回调.
     * 非线程安全.
     */
    void setMessageCallback(const MessageCallback & cb)
    { messageCallback_ = cb; }

    /**
     * 设置写完成回调.
     * 非线程安全.
     */
    void setWriteCompleteCallback(const WriteCompleteCallback& cb)
    { writeCompleteCallback_ = cb; }
```

注意到并没有连接关闭回调，这是由TcpServer::removeConnection()负责的，进而把工作转交给TcpConnection::connectDestroyed()，用户不可更改设置。

##### TcpServer的构造与析构

TcpServer构造函数主要工作是为成员申请资源，为各回调设置回调函数

```c++
TcpServer::TcpServer(EventLoop* loop,
                     const InetAddress& listenAddr,
                     const std::string& nameArg,
                     Option option)
: loop_(CHECK_NOTNULL(loop)), // 确保loop非空
  ipPort_(listenAddr.toIpPort()),  // 将Ip, port转换为字符串
  name_(nameArg), // 名称
  acceptor_(new Acceptor(loop, listenAddr, option == kReusePort)),
  threadPool_(new EventLoopThreadPool(loop, name_)), // 初始化事件循环线程池
  connectionCallback_(defaultConnectionCallback),    // 连接回调为默认连接回调
  messageCallback_(defaultMessageCallback),          // 消息回调为默认消息回调
  nextConnId_(1)  // 连接id
{
    acceptor_->setNewConnectionCallback(
            std::bind(&TcpServer::newConnection, this, _1, _2)); // 设置新建连接时的回调
}
```

同样是连接回调，TcpServer::newConnection()和connectionCallback_有何区别？
前者是Acceptor发生连接请求事件时，回调，用来新建一个Tcp连接；后者是在TcpServer内部新建连接即调用TcpServer::newConnection()时，回调connectionCallback_。

TcpServer析构工作内容很简单，主要销毁ConnectionMap中所有Tcp连接，而每个Tcp连接用的是一个TcpConnection对象来管理的。

```c++
/**
* 析构TcpServer对象, 销毁ConnectionMap中所有连接
*/
TcpServer::~TcpServer()
{
    loop_->assertInLoopThread();
    LOG_TRACE << "TcpServer::~TcpServer [" << name_ << "] destructing";


    // reset all connection of @c connections_
    for (auto& item : connections_)
    {
        TcpConnectionPtr conn(item.second); // shared_ptr manage TcpConnection
        item.second.reset();
        conn->getLoop()->runInLoop(
                std::bind(&TcpConnection::connectDestroyed, conn));
    }
}
```

TcpServer启动Tcp服务器，主要完成1）线程池的启动；2）Acceptor监听Tcp连接请求。

线程池需要指定其初始数量，当然，这需要在start()之前调用TcpServer::setThreadNum()设置。

```c++
/**
* 启动TcpServer, 初始化线程池, 连接接受器Accept开始监听(Tcp连接请求)
*/
void TcpServer::start()
{
    if (started_.getAndSet(1) == 0)
    {
        threadPool_->start(threadInitCallback_);


        assert(!acceptor_->listening());
        loop_->runInLoop(
                std::bind(&Acceptor::listen, get_pointer(acceptor_)));
    }
}
```

* TcpServer如何获得新连接的conn fd（accept返回值）？
  TcpServer内部用Acceptor，保存用户提供的Connection-Callback和MessageCallback，新建TcpConnection对象（newConn()）的时候直接传递给TcpConnection的构造函数。
* 如何创建TcpConnection对象？
  新连接请求到达时，Acceptor回调newConnection()，通过TcpServer::newConnection创建一个新TcpConnection对象，用于管理一个Tcp连接。
  即，TcpServer::newConnection回调顺序 EventLoop => Channel => Acceptor => TcpServer

下面是用于创建TcpConnection对象的函数TcpServer::newConnectio()

```c++
// TcpServer.cc
/**
* 新建一个TcpConnection对象, 用于连接管理.
* @details 新建的TcpConnection对象会加入内部ConnectionMap.
* @param sockfd accept返回的连接fd (accepted socket fd)
* @param peerAddr 对端ip地址信息
* @note 必须在所属loop线程运行
*/
void TcpServer::newConnection(int sockfd, const InetAddress &peerAddr)
{
    loop_->assertInLoopThread();
    /* 从EventLoop线程池中，取出一个EventLoop对象构造TcpConnection对象，便于均衡各EventLoop负责的连接数　*/
    EventLoop* ioLoop = threadPool_->getNextLoop(); // next event loop from the event loop thread pool

    /* 设置连接对象名称, 包含基础名称+ip地址+端口号+连接Id
     * 因为要作为ConnectionMap的key, 要确保运行时唯一性 */
    char buf[32];
    snprintf(buf, sizeof(buf), "-%s#%d", ipPort_.c_str(), nextConnId_);
    ++nextConnId_;
    std::string connName = name_ + buf;

    LOG_INFO << "TcpServer::newConnection [" << name_
    << "] - new connection [" << connName
    << "] from " << peerAddr.toIpPort();
  
    InetAddress localAddr(sockets::getLocalAddr(sockfd)); // 本地ip地址信息
    // FIXME poll with zero timeout to double confirm the new connection
    // FIXME use make_shared if necessary
    /* 新建TcpConnection对象,　并加入ConnectionMap */
    TcpConnectionPtr conn(new TcpConnection(ioLoop, connName, sockfd, localAddr, peerAddr));
    connections_[connName] = conn;
    /* 为新建TcpConnection对象设置各种回调 */
    conn->setConnectionCallback(connectionCallback_); // 连接回调
    conn->setMessageCallback(messageCallback_);       // 消息回调
    conn->setWriteCompleteCallback(writeCompleteCallback_); // 写完成回调
    conn->setCloseCallback( // 关闭连接回调
            std::bind(&TcpServer::removeConnection, this, _1)); // FIXME: unsafe
    /* 确认连接是否已建立, 并初始化连接建立后的状态 */
    ioLoop->runInLoop(std::bind(&TcpConnection::connectEstablished, conn));
}
```

##### TcpConnection类

TcpConnection类是muduo最核心的类，唯一默认用shared_ptr来管理的类，唯一继承自enable_shared_from_this的类。这是因为其生命周期模糊：可能在连接断开时，还有其他地方持有它的引用，贸然delete会造成空悬指针。只有确保其他地方没有持有该对象的引用的时候，才能安全地销毁对象。

```c++
// TcpConnection.h
/**
* Tcp连接, 为服务器和客户端使用.
* 接口类, 因此不要暴露太多细节.
* 
* @note 继承自std::enable_shared_from_this的类, 可以用getSelf返回用std::shared_ptr管理的this指针
*/
class TcpConnection : noncopyable,
        public std::enable_shared_from_this<TcpConnection> // std::shared_ptr<TcpConnection> getSelf()
{
public:
```

TcpConnection不提供用户使用的函数，所提供的接口主要给TcpServer/TcpClient使用。TcpConnection的状态有4个：kDisconnected, kConnecting, kConnected, kDisconnecting。TcpConnection使用Channel获得socket上IO事件，它会自己处理writable事件，把readable事件通过MessageCallback传递给客户。TcpConnection拥有TCP socket（Socket类），后者析构函数会close(fd)。

```c++
// TcpConnection.h
private:
    enum StateE { kDisconnected, kConnecting, kConnected, kDisconnecting }; // TCP连接状态定义
    void handleRead(Timestamp receiveTime); // 处理读事件
    void handleWrite();                     // 处理写事件
    void handleClose();                     // 处理关闭连接事件
    void handleError();                     // 处理错误事件

    /* loop线程中排队发送消息 */
//    void sendInLoop(std::string&& message); // C++11 // add by martin
    void sendInLoop(const StringPiece& message);
    void sendInLoop(const char* message, size_t len);

    /* loop线程中排队关闭写连接 */
    void shutDownInLoop();
//    void shutDownAndForceCloseInLoop(double seconds); // add by martin
    /* loop线程中排队关闭连接 */
    void forceCloseInLoop();

    /*　设置TcpConnection状态 */
    void setState(StateE s) { state_ = s; }

    /* 将状态转换为字符串 */
    const char* stateToString() const;

    /* loop线程中排队开始监听读事件 */
    void startReadInLoop();
    /* loop线程中排队关闭监听读事件 */
    void stopReadInLoop();

    EventLoop* loop_;     // 所属　EventLoop
    std::string name_;    // 名称
    StateE state_; // FIXME: use atomic varaible　//　状态
    bool reading_; // whether the connection is reading // 连接是否正在监听读事件
    // we don't expose those classes to client.
    std::unique_ptr<Socket> socket_;              // 连接套接字, 用于对连接进行底层操作
    std::unique_ptr<Channel> channel_;            // 通道, 用于绑定要监听的事件
    InetAddress localAddr_;                       // 本地IP地址
    InetAddress peerAddr_;                        // 对端IP地址
    ConnectionCallback connectionCallback_;       // 连接回调
    MessageCallback messageCallback_;             // 收到消息回调
    WriteCompleteCallback writeCompleteCallback_; // 写完成回调
    HighWaterMarkCallback highWaterMarkCallback_; // 高水位回调
    CloseCallback closeCallback_;                 // 关闭连接回调
    size_t highWaterMark_;                        // 高水位阈值
    Buffer inputBuffer_;                          // 输入缓冲区
    Buffer outputBuffer_; // FIXME: use list<Buffer> as output buffer.　//　输出缓冲区
    boost::any contex_;                           // 用户自定义参数
    // FIXME: createTime_, lastReceiveTime_
    //        bytesReceived_, bytesSent_
};
```

TcpConnection表示的是“一次Tcp连接”，不可再生，一旦连接断开，该TcpConnection对象就没用了。TcpConnection没用发起连接的功能，构造函数参数是已经建立好连接的socket fd，初始状态是kConnecting。连接可以是TcpServer或TcpClient发起。

接收到消息时，通过Channel::handleEvent会将可读事件转交给TcpConnection::handleRead处理，而TcpConnection::handleRead又会通过messageCallback_将可读事件转交给TcpServer::messageCallback_，进而传递给用户。

```c++
// TcpConnection.cc
/**
* 从输入缓存inputBuffer_读取数据, 交给回调messageCallback_处理
* @param receiveTime 接收到读事件的时间点
* @details 通常是TcpServer/TcpClient运行回调messageCallback_, 将处理机会传递给用户
*/
void TcpConnection::handleRead(Timestamp receiveTime)
{
    loop_->assertInLoopThread();
    int savedErrno = 0;
    ssize_t n = inputBuffer_.readFd(channel_->fd(), &savedErrno); // 从指定fd读取数据到内部缓冲
    if (n > 0)
    {
        messageCallback_(shared_from_this(), &inputBuffer_, receiveTime);
    }
    else if (n == 0)
    {
        handleClose();
    }
    else
    {
        errno = savedErrno;
        LOG_SYSERR << "TcpConnection::handleRead";
        handleError();
    }
}
```

##### 断开Tcp连接

##### 断开连接方式

muduo中有2种关闭连接的方式：
1）被动关闭：即对端先关闭连接，本地read(2)返回0，触发关闭逻辑，调用handleClose。
2）主动关闭：利用forceClose()或forceCloseWithDelay()成员函数调用handleClose，强制关闭或强制延时关闭连接。

被动关闭流程见下图，图中“X”表示TcpConnection对象通常在此析构。

![](https://img2022.cnblogs.com/blog/741401/202203/741401-20220324082607844-1314411437.png)

##### Channel与断开连接

Channel中有关关闭连接的事件回调CloseCallback，由Channel::handleEvent()调用，从而触发TcpConnection::handleClose()：

调用链路：
Poller::poll()检测到Channel事件就绪 => EventLoop::loop() =>Channel::handleEvent() => Channel::closeCallback_ => TcpConnection::handleClose()

```c++
// Channel.cc
void Channel::handleEvent(Timestamp recevieTime)
{
    std::shared_ptr<void> guard;
    if (tied_)
    {
        guard = tie_.lock();
        if (guard)
        {
            handleEventWithGuard(recevieTime);
        }
    }
    else
    {
        handleEventWithGuard(recevieTime);
    }
}

/**
* 根据不同的激活原因, 调用不同的回调函数
*/
void Channel::handleEventWithGuard(Timestamp receiveTime)
{
    eventHandling_ = true; // 正在处理事件
    LOG_TRACE << reventsToString(); // 打印fd及就绪事件
    if ((revents_ & POLLHUP) && !(revents_ & POLLIN))
    { // fd挂起(套接字已不在连接中), 并且没有数据可读
        if (logHup_)
        { // 打印挂起log
            LOG_WARN << "fd = " << fd_ << " Channel::handle_event() POLLHUP";
        }
        // 调用关闭回调
        if (closeCallback_) closeCallback_();
    }
    ...
}
```

##### TcpConnection与断开连接

包含CloseCallback事件回调，不过是给TcpServer和TcpClient用的，用于通知它们移除所持有的TcpConnectionPtr，而非给普通用户直接用的，普通用户应使用ConnectionCallback。

与断开连接有关的部分：

```c++
// TcpConnection.h
public:
    /* 关闭写半连接 */
    void shutdown(); // NOT thread safe, no simultaneous calling
//    void shutdownAndForceCloseAfter(double seconds); // NOT thread safe, no simultaneous calling

    /* 强制关闭连接 */
    void forceClose();
    /* 强制延时关闭连接 */
    void forceCloseWithDelay(double seconds);

    ...

    /* Internal use only */
    void setCloseCallback(const CloseCallback& cb)
    { closeCallback_ = cb; }

    // called when TcpServer accepts a new connection
    void connectEstablished(); // should be called only once per connection
    // called when TcpServer has removed me from its map
    void connectDestroyed(); // should be called only once per connection

private:
    void handleClose();                     // 处理关闭连接事件
```

* 被动关闭连接
  当收到对端FIN分节时，本地read返回0,，Tcp连接被动关闭，会触发调用本地TcpConnection::handleClose()，其定义如下：

```c++
/**
* 处理Tcp连接关闭
* @details 更新状态为kDisconnected, 清除所有事件通道监听
* @note 必须在所属loop线程中运行.
*/
void TcpConnection::handleClose()
{
    loop_->assertInLoopThread(); // 确保在所属loop线程中运行
    LOG_TRACE << "fd = " << channel_->fd() << " state = " << stateToString();
    assert(state_ == kConnected || state_ == kDisconnecting);
    // we don't close fd, leave it to dtor, so we can find leaks easily.
    setState(kDisconnected); // 更新Tcp连接状态
    channel_->disableAll();     // 停止监听所有通道事件(读写事件)

    TcpConnectionPtr guardThis(shared_from_this());
    connectionCallback_(guardThis); // 连接回调
    // must be the last line
    closeCallback_(guardThis);      //　关闭连接回调
}
```

closeCallback_在TcpServer::newConnection()为新连接新建TcpConnection时，已设为TcpServer::removeConnection()，而removeConnection()最终会调用TcpConnection::connectDestroyed()来销毁连接资源。

```c++
/**
* 主动销毁当前tcp连接, 移除通道事件
* @note 只有处于已连接状态(kConnected)的tcp连接, 才需要先更新状态, 关闭通道事件监听
*/
void TcpConnection::connectDestroyed()
{
    loop_->assertInLoopThread();
    if (state_ == kConnected) // 只有kConnected的连接, 才有必要采取断开连接动作
    {
        setState(kDisconnected);
        channel_->disableAll(); // 关闭通道事件监听

        connectionCallback_(shared_from_this()); // 调用连接回调
    }
    channel_->remove(); // 从EventLoop和Poller中移除监听通道事件
}
```

* 主动关闭连接
  按连接方向，有2类关闭方式：1）强制close连接；2）关闭一个方向的连接（读或写方向）。

```c++
/**
* 强制关闭连接, 只对连接为kConnected或kDisconnecting状态才有效
* @details 为防止意外, 动作应该放到loop末尾去做
*/
void TcpConnection::forceClose()
{
    // FIXME: use compare and swap
    if (state_ == kConnected || state_ == kDisconnecting)
    {
        setState(kDisconnecting);
        loop_->queueInLoop(std::bind(&TcpConnection::forceCloseInLoop, shared_from_this()));
    }
}

/**
* 在所属loop循环中强制关闭连接, 只对连接为kConnected或kDisconnecting状态才有效
*/
void TcpConnection::forceCloseInLoop()
{
    loop_->assertInLoopThread();
    if (state_ == kConnected || state_ == kDisconnecting)
    {
        // as if we received 0 byte in handleRead()
        handleClose();
    }
}
```

可以看到，除了状态更新，对于关闭连接的真正操作，被动、主动关闭连接都是由handleClose来完成的。

假设想延迟一段时间再关闭连接，可以调用forceCloseWithDelay()，区别在于交给EventLoop:runAfter()延时运行，而不是交给EventLoop::queueInLoop()。

```c++
void TcpConnection::forceCloseWithDelay(double seconds)
{
    if (state_ == kConnected || state_ == kDisconnecting)
    {
        setState(kDisconnecting);
        loop_->runAfter(seconds,
                        makeWeakCallback(shared_from_this(),
                                          &TcpConnection::forceClose)); // not forceCloseInLoop to avoid race condition
    }
}
```

关闭连接写方向，相当于库函数shutdown(2)

```c++
/**
* 关闭连接写方向, 只有已连接状态才有效
*/
void TcpConnection::shutdown()
{
    // FIXME: use compare and swap
    if (state_ == kConnected)
    {
        setState(kDisconnecting);
        // FIXME: shared_from_this()?
        loop_->runInLoop(std::bind(&TcpConnection::shutDownInLoop, shared_from_this()));
    }
}
```

##### TcpServer与断开连接

当新建一个tcp连接时，TcpServer会调用newConnection创建一个新TcpConnection对象管理Tcp连接，并将对象加入自己的ConnectionMap进行管理。

而当tcp连接断开时，需要调用removeConnection进行移除工作，而removeConnection会将工作转交给removeConnectionInLoop, 确保在所属loop线程中执行。

```c++
/**
* 转交给removeConnectionInLoop, 在所属loop线程中执行
*/
void TcpServer::removeConnection(const TcpConnectionPtr &conn)
{
    // FIXME: unsafe
    loop_->runInLoop(std::bind(&TcpServer::removeConnectionInLoop, this, conn));
}
```

removeConnectionInLoop要做的工作是将要移除的tcp连接对应TcpConnection对象，从ConnectionMap移除，然后销毁该对象。

```c++
/**
* 在所属loop线程循环中, 排队移除指定tcp连接 conn
* @param conn 指向待移除tcp连接对应TcpConnection对象
*/
void TcpServer::removeConnectionInLoop(const TcpConnectionPtr &conn)
{
    loop_->assertInLoopThread();
    LOG_INFO << "TcpServer::removeConnectionInLoop [" << name_
    << "] - connection " << conn->name();
    size_t n = connections_.erase(conn->name()); // 从ConnectionMap中擦除待移除TcpConnection对象
    (void)n;
    assert(n == 1);
    EventLoop* ioLoop = conn->getLoop();
    ioLoop->queueInLoop(
            std::bind(&TcpConnection::connectDestroyed, conn)); // 在所属loop线程中排队销毁TcpConnection对象
}
```

##### 错误处理

Channel在处理通道事件handleEvent时，如果发生错误（既不是读取数据，也不是写数据完成），

调用链路：
Channel::handleEvent() => 检测到错误，调用Channel::errorCallback_ => TcpConnection::handleError()

```c++
/**
* 处理tcp连接错误, 打印错误log
* @details 从tcp协议栈获取错误信息
*/
void TcpConnection::handleError()
{
    int err = sockets::getSocketError(channel_->fd());
    LOG_ERROR << "TcpConnection::handleError [" << name_
    << "] - SO_ERROR = " << err << " " << strerror_tl(err);
}
```

##### 发送数据

用于发送数据的TcpConnection::send()重载了下面几个版本

```c++
public:
    ...
    /* 发送消息给连接对端 */
//    void send(std::string&& mesage); // C++11
    void send(const void* message, int len);
    void send(const StringPiece& message);
//    void send(Buffer&& message);
    void send(Buffer* message); // this one will swap data

3个send() 重载版本，最终都会转交给sendInLoop(const char*, int)，在所属loop线程中执行发送工作。


// 转发给send(const StringPiece&), 最终转交给sendInLoop(const char*, int)
void TcpConnection::send(const void *message, int len)
{
    send(StringPiece(static_cast<const char*>(message), len));
}

/**
* 转交给 sendInLoop(const char*, int)
* 发送消息给对端, 允许在其他线程调用
* @param message 要发送的消息. StringPiece兼容C/C++风格字符串, 二进制缓存, 提供统一字符串接口
*/
void TcpConnection::send(const StringPiece& message)
{
    if (state_ == kConnected)
    {
        if (loop_->isInLoopThread())
        { // 当前线程是所属loop线程
            sendInLoop(message);
        }
        else
        { // 当前线程并非所属loop线程
            void (TcpConnection::*fp)(const StringPiece& message);
            fp = &TcpConnection::sendInLoop;
            loop_->runInLoop(
                    std::bind(fp,
                              this, // FIXME
                              message.as_string()));
        }
    }
}

// 转交给sendInLoop(const char*, int)
void TcpConnection::sendInLoop(const StringPiece &message)
{
    sendInLoop(message.data(), message.size());
}

// 转交给sendInLoop(const char*, int)
// FIXME efficiency!!!
void TcpConnection::send(Buffer *buf)
{
    if (state_ == kConnected)
    {
        if (loop_->isInLoopThread())
        {
            // send all readable bytes
            sendInLoop(buf->peek(), buf->readableBytes());
            buf->retrieveAll();
        }
    }
}
```

sendInLoop定义如下，主要是向对端发送一次数据，如果发送完一次，就进行一次回调；如果待发送数据超高水位，就进行高水位回调；如果发生错误，就进行错误回调。

```c++
/**
* 在所属loop线程中, 发送data[len]
* @param data 要发送的缓冲区首地址
* @param len　要发送的缓冲区大小(bytes)
* @details 发生write错误, 如果发送缓冲区未满,　对端已发FIN/RST分节 表明tcp连接发生致命错误(faultError为true)
*/
void TcpConnection::sendInLoop(const char *data, size_t len)
{
    loop_->assertInLoopThread();
    ssize_t nwrote = 0;
    size_t remaining = len;
    bool faultError = false;
    if (state_ == kDisconnected) // 如果已经断开连接(kDisconnected), 就无需发送, 打印log(LOG_WARN)
    {
        LOG_WARN << "disconnected, give up writing";
        return;
    }

    // write一次, 往对端发送数据, 后面再看是否发生错误, 是否需要高水位回调
    // if no thing output queue, try writing directly
    if (!channel_->isWriting() && outputBuffer_.readableBytes() == 0)
    { // 如果通道没有使能监听写事件, 并且outputBuffer　没有待发送数据, 就直接通过socket写
        nwrote = sockets::write(channel_->fd(), data, len);
        if (nwrote >= 0)
        {
            remaining = len - nwrote;
            if (remaining == 0 && writeCompleteCallback_)
            {
                loop_->queueInLoop(std::bind(writeCompleteCallback_, shared_from_this()));
            }
        }
        else // nwrote < 0, error
        {
            nwrote = 0;
            if (errno != EWOULDBLOCK) // EWOULDBLOCK: 发送缓冲区已满, 且fd已设为nonblocking
            { // O_NONBLOCK fd, write block but return EWOULDBLOCK error
                LOG_SYSERR << "TcpConnection::sendInLoop";
                if (errno == EPIPE || errno == ECONNRESET) // FIXME: any others?
                { // EPIPE: reading end is closed; ECONNRESET: connection reset by peer
                    faultError = true;
                }
            }
        }
    }

    // 处理剩余待发送数据
    assert(remaining <= len);
    if (!faultError && remaining > 0) // 没有故障, 并且还有待发送数据, 可能是发送太快, 对方来不及接收
    { // no error and data remaining to be written
        size_t oldLen = outputBuffer_.readableBytes(); // Buffer中待发送数据量

        if (oldLen + remaining >= highWaterMark_ // Buffer及当前要发送的数据量之和 超 高水位(highWaterMark)
        && oldLen < highWaterMark_ // 单独的Buffer中待发送数据量 未超 高水位
        && highWaterMarkCallback_)
        {
            loop_->queueInLoop(std::bind(highWaterMarkCallback_, shared_from_this(), oldLen + remaining));
        }
        // append data to be written to the output buffer
        outputBuffer_.append(static_cast<const char*>(data) + nwrote, remaining);
        // enable write event for channel_
        if (!channel_->isWriting())
        { // 如果没有在监听通道写事件, 就使能通道写事件
            channel_->enableWriting();
        }
    }
}
```

##### TcpConnection回调

##### TcpConnection回调有哪些？

TcpConnection为应用程序提供了几个用于处理连接及数据的回调接口，主要包括 ConnectionCallback（连接建立回调）、MessageCallback（接收到消息回调）、WriteCompleteCallback（写完成回调）、HighWaterMarkCallback（高水位回调）、CloseCallback（连接关闭回调）。

通过以下几个成员实现存储、判断：

```c++
    ConnectionCallback connectionCallback_;       // 连接回调
    MessageCallback messageCallback_;             // 收到消息回调
    WriteCompleteCallback writeCompleteCallback_; // 写完成回调
    HighWaterMarkCallback highWaterMarkCallback_; // 高水位回调
    CloseCallback closeCallback_;                 // 关闭连接回调

    size_t highWaterMark_;                        // 高水位阈值
    Buffer inputBuffer_;                          // 应用层接收缓冲区
    Buffer outputBuffer_; // FIXME: use list<Buffer> as output buffer.　//　应用层发送缓冲区
    boost::any contex_;                           // 绑定一个用户自定义类型的上下文参数
```

connectionCallback_存放用户注册的连接回调， 会在以下几种情形回调：
1）Tcp连接确认建立时：TcpServer/TcpClient在Tcp连接建立时，通过newConenction新建TcpConnection对象时，回调TcpConnection::connectEstablished，进而回调connectionCallback_。
2）Tcp连接确认销毁时：TcpServer/TcpClient在移除Tcp连接时，通过removeConnectionInLoop销毁连接，回调connectionCallback_。
3）强制关闭Tcp连接时：TcpConnection强制销毁时，调用forceClose()/forceClose()，或者从sockfd read到数据量为0（对端关闭连接），进而调用handleClose，回调connectionCallback_。

messageCallback_ 存放用户注册的接收到消息回调，会在Channel调用handleRead处理读事件时，调用cpConnection::handleRead，从sockfd read到数据量>0，进而回调messageCallback_。

writeCompleteCallback_ 存放用户注册的写完成回调，会在数据发送完毕回调函数，所有的用户数据都已拷贝，outputBuffer_被清空也会回调该函数。

highWaterMarkCallback_ 存放用户注册的高水位回调，用户send数据时，会在应用发送缓存堆积数据过多（>highWaterMark_）时调用。

##### 回调注册

注册回调接口：

```c++
    void setConnectionCallback(const ConnectionCallback& cb)
    { connectionCallback_ = cb; }

    void setMessageCallback(const MessageCallback& cb)
    { messageCallback_ = cb; }

    void setWriteCompleteCallback(const WriteCompleteCallback& cb)
    { writeCompleteCallback_ = cb; }

    void setHighWaterMarkCallback(const HighWaterMarkCallback& cb, size_t highWaterMark)
    { highWaterMarkCallback_ = cb; highWaterMark_ = highWaterMark; }
```

##### 何时调用connectionCallback_？

调用connectionCallback_ 分为多种情况。connectionCallback_ 通常不应该为空，如果用户没有特殊设置，应该设为默认的defaultConnectionCallback。

1）Tcp连接建立时

```c++
/**
* TcpServer/TcpClient 调用newTcpConnection创建TcpConnection对象后, 用来做一些连接以建立的事后工作
*/
void TcpConnection::connectEstablished()
{
    loop_->assertInLoopThread();
    assert(state_ == kConnecting);
    setState(kConnected);
    channel_->tie(shared_from_this()); // tie this TcpConnection object to channel_
    channel_->enableReading(); // 使能监听读事件, 连接上有读事件发生时, 如收到对端数据, 会触发channel_::handleRead


    // FIXME: add by Martin
    assert(connectionCallback_ != nullptr); // connectionCallback_通常不允许为空, TcpServer/TcpClient可以设置为缺省值defaultConnectionCallback


    connectionCallback_(shared_from_this()); // 回调connectionCallback_
}
```

2）Tcp连接销毁时，通常由TcpServer/TcpClient主动移除通道事件触发

```c++
/**
* TcpServer/TcpClient移除通道事件时, 调用该函数销毁tcp连接
* @note 只有处于已连接状态(kConnected)的tcp连接, 才需要先更新状态, 关闭通道事件监听
*/
void TcpConnection::connectDestroyed()
{
    loop_->assertInLoopThread();
    if (state_ == kConnected) // 只有kConnected的连接, 才有必要采取断开连接动作
    {
        setState(kDisconnected);
        channel_->disableAll(); // 关闭通道事件监听


        connectionCallback_(shared_from_this()); // 调用连接回调
    }
    channel_->remove(); // 从EventLoop和Poller中移除监听通道事件
}
```

3）对端关闭连接（从sockfd read返回0），本地调用handleClose被动关闭

```c++
/**
* 处理Tcp连接关闭
* @details 更新状态为kDisconnected, 清除所有事件通道监听.
* @note 必须在所属loop线程中运行.
*/
void TcpConnection::handleClose()
{
    loop_->assertInLoopThread(); // 确保在所属loop线程中运行
    LOG_TRACE << "fd = " << channel_->fd() << " state = " << stateToString();
    assert(state_ == kConnected || state_ == kDisconnecting);
    // we don't close fd, leave it to dtor, so we can find leaks easily.
    setState(kDisconnected); // 更新Tcp连接状态
    channel_->disableAll();     // 停止监听所有通道事件(读写事件)

    TcpConnectionPtr guardThis(shared_from_this());
    connectionCallback_(guardThis); // 连接回调
    // must be the last line
    closeCallback_(guardThis);      //　关闭连接回调
}

void TcpConnection::handleRead(Timestamp receiveTime)
{
    loop_->assertInLoopThread();
    int savedErrno = 0;
    ssize_t n = inputBuffer_.readFd(channel_->fd(), &savedErrno); // 从指定fd读取数据到内部缓冲
    if (n > 0)
    {
        messageCallback_(shared_from_this(), &inputBuffer_, receiveTime);
    }
    else if (n == 0)
    {
        handleClose();
    }
    ...
}
```

4）本地主动强制关闭（forceClose）Tcp连接，当然，connectionCallback_回调跟3）一样，还是在handleClose中发生的

```c++
/**
* 在所属loop循环中强制关闭连接, 只对连接为kConnected或kDisconnecting状态才有效
*/
void TcpConnection::forceCloseInLoop()
{
    loop_->assertInLoopThread();
    if (state_ == kConnected || state_ == kDisconnecting)
    {
        // as if we received 0 byte in handleRead()
        handleClose();
    }
}

/**
* 强制关闭连接, 只对连接为kConnected或kDisconnecting状态才有效
* @details 为防止意外, 动作应该放到loop末尾去做
*/
void TcpConnection::forceClose()
{
    // FIXME: use compare and swap
    if (state_ == kConnected || state_ == kDisconnecting)
    {
        setState(kDisconnecting);
        loop_->queueInLoop(std::bind(&TcpConnection::forceCloseInLoop, shared_from_this()));
    }
}
```

##### 何时调用messageCallback_？

前面在handleRead()中已经看到，当从sockfd read返回>0时，就会回调messageCallback_，交给应用层处理从对端接收来的数据（存放到inputBuffer_中）。

```c++
void TcpConnection::handleRead(Timestamp receiveTime)
{
    loop_->assertInLoopThread();
    int savedErrno = 0;
    ssize_t n = inputBuffer_.readFd(channel_->fd(), &savedErrno); // 从指定fd读取数据到内部缓冲
    if (n > 0)
    {
        messageCallback_(shared_from_this(), &inputBuffer_, receiveTime);
    }
    ...
}
```

##### 何时调用writeCompleteCallback_，highWaterMarkCallback_？

TcpConnection::send()发送数据时，会转为在IO线程中调用TcpConnection::sendInLoop。将要发送的数据写到内核缓冲区，然后回调writeCompleteCallback_；如果一次没有写完，剩余数据量较大，超过highWaterMark_（高水位），就回调highWaterMarkCallback_。

调用顺序：
send() => sendInLoop() => writeCompleteCallback_(), highWaterMarkCallback_() => handleWrite()

```c++
void TcpConnection::sendInLoop(const char *data, size_t len)
{
    string s;
    std::reverse(s.begin(), s.end());
    loop_->assertInLoopThread();
    ssize_t nwrote = 0;
    size_t remaining = len;
    bool faultError = false;
    if (state_ == kDisconnected) // 如果已经断开连接(kDisconnected), 就无需发送, 打印log(LOG_WARN)
    {
        LOG_WARN << "disconnected, give up writing";
        return;
    }

    // write一次, 往对端发送数据, 后面再看是否发生错误, 是否需要高水位回调
    // if no thing output queue, try writing directly
    if (!channel_->isWriting() && outputBuffer_.readableBytes() == 0)
    { // 如果通道没有使能监听写事件, 并且outputBuffer　没有待发送数据, 就直接通过socket写
        nwrote = sockets::write(channel_->fd(), data, len);
        if (nwrote >= 0)
        {
            remaining = len - nwrote;
            // 写完了data[len]到到内核缓冲区, 就回调writeCompleteCallback_
            if (remaining == 0 && writeCompleteCallback_)
            {
                loop_->queueInLoop(std::bind(writeCompleteCallback_, shared_from_this()));
            }
        }
        else // nwrote < 0, error
        {
            nwrote = 0;
            if (errno != EWOULDBLOCK) // EWOULDBLOCK: 发送缓冲区已满, 且fd已设为nonblocking
            { // O_NONBLOCK fd, write block but return EWOULDBLOCK error
                LOG_SYSERR << "TcpConnection::sendInLoop";
                if (errno == EPIPE || errno == ECONNRESET) // FIXME: any others?
                { // EPIPE: reading end is closed; ECONNRESET: connection reset by peer
                    faultError = true;
                }
            }
        }
    }

    // 处理剩余待发送数据
    // 没有错误, 并且还有没有写完的数据, 说明内涵发送缓冲区满, 要将未写完的数据添加到output buffer中
    assert(remaining <= len);
    if (!faultError && remaining > 0) // 没有故障, 并且还有待发送数据, 可能是发送太快, 对方来不及接收
    { // no error and data remaining to be written
        size_t oldLen = outputBuffer_.readableBytes(); // Buffer中待发送数据(readable空间数据)

        // 如果readable空间数据 + 未写完数据, 超过highWaterMark_(高水位), 回调highWaterMarkCallback_
        if (oldLen + remaining >= highWaterMark_
        && oldLen < highWaterMark_
        && highWaterMarkCallback_)
        {
            loop_->queueInLoop(std::bind(highWaterMarkCallback_, shared_from_this(), oldLen + remaining));
        }
        // append data to be written to the output buffer
        outputBuffer_.append(static_cast<const char*>(data) + nwrote, remaining);
        // enable write event for channel_
        if (!channel_->isWriting())
        { // 如果没有在监听通道写事件, 就使能通道写事件
            channel_->enableWriting();
        }
    }
}

/**
* 内核发送缓冲区有空间了, 回调该函数.
* @details 在send()之后触发. send()通常由用户主动调用, handleWrite由epoll_wait/poll监听,
* 由EventLoop对应IO线程回调.
*/
void TcpConnection::handleWrite()
{
    loop_->assertInLoopThread();
    if (channel_->isWriting())
    { // 只有通道在监听写事件时, 处理write事件才有意义
        // 继续往内核缓冲区写readable空间的待发送数据
        ssize_t n = sockets::write(channel_->fd(),
                                   outputBuffer_.peek(),
                                   outputBuffer_.readableBytes());
        if (n > 0)
        {
            outputBuffer_.retrieve(n);
            if (outputBuffer_.readableBytes() == 0)
            { // 发送缓冲区已清空
                channel_->disableWriting();
                if (writeCompleteCallback_)
                { // 应用层发送缓冲区已被清空, 就回调writeCompleteCallback_
                    loop_->queueInLoop(std::bind(writeCompleteCallback_, shared_from_this()));
                }
                if (state_ == kDisconnecting)
                { // 发送缓冲区已清空, 并且连接状态是kDisconnecting, 要关闭连接
                    shutDownInLoop(); // 关闭连接 写方向
                }
            }
        }
        else
        {
            LOG_SYSERR << "TcpConnection::handleWrite";
        }
    }
}
```

##### 上下文数据传递

input buffer、output buffer是Tcp连接中，伴随着一次或几次请求、应答的数据，是与对端进行沟通的数据。如果想要一下数据，伴随着整个Tcp连接的声明周期，该怎么办？
可以使用TcpConnection::contex_，在连接建立时（回调onConnection），创建boost::any对象（可变类型），然后让TcpConnection::contex_指向该对象。这样，在整个TcpConnection生命周期内，都可以直接通过contex_访问该对象；直到沟通完成，或者连接关闭时，reset contex_。

```c++
public:
    ...
    void setContext(const boost::any& contex)
    { contex_ = contex; }

    const boost::any& getContext() const
    { return contex_; }

    boost::any* getMutableContext()
    { return &contex_; }
    ...
private:
    boost::any contex_;                           // 用户自定义参数
    ...
```

比如，对于一个HttpSever，可以中连接回调onConnection中存放一个HttpContext对象；在收到对端消息，回调onMessage时，可以获得该HttpContext对象；在确认解析完HTTP请求后，可以reset 重置context上下文。

```c++
void HttpServer::onConnection(const TcpConnectionPtr &conn)
{
    if (conn->connected()) // 确认已处于连接建立状态
    {
        conn->setContext(HttpContext()); // 让contex_ 指向一个HttpContext对象
    }
}

void HttpServer::onMessage(const TcpConnectionPtr &conn, Buffer *buf, Timestamp receivedTime)
{
    // 从TcpConnection获取contex_ 上下文，然后将内容转型为所需的HttpContext类型
    HttpContext* context = boost::any_cast<HttpContext>(conn->getMutableContext()); 

    if (!context->parseRequest(buf, receivedTime))
    { // parse request failure
        conn->send("HTTP/1.1 400 Bad Request\r\n\r\n");
        conn->shutdown();
    }

    if (context->gotAll())
    {
        onRequest(conn, context->request());
        context->reset();
    }
}
```

#### HTTP

客户端发送请求，通过muduo库之后服务端收到的数据存放于Buffer中，之后解析成HttpRequest请求对象，再创建一个HttpResponse响应对象并格式化成Buffer返回给客户端。服务端解析客户端请求的Buffer使用HttpContext类。

1、HttpRequest 类
发送端构造一个HttpRequest，调用成员函数设置请求头、请求体等。调用成员函数请求头字段，使用 const char* start, const char* end 来传递一个字符串。主要是因为基于TCP的请求数据都保存在Buffer中，通过解析Buffer中数据进行传递HTTP报文信息。

class HttpRequest : public muduo::copyable
{
 public:
  enum Method  { kInvalid, kGet, kPost, kHead, kPut, kDelete };  //设计支持的请求类型
  enum Version { kUnknown, kHttp10, kHttp11 };  // HTTP版本

  HttpRequest() : method_(kInvalid), version_(kUnknown) {}  //默认构造函数

  void setVersion(Version v) { version_ = v; }      // 版本
  Version getVersion() const { return version_; }

  bool setMethod(const char* start, const char* end)  // 根据字符串设定请求方法
  {
    assert(method_ == kInvalid);
    string m(start, end);
    if      (m == "GET")  {  method_ = kGet;  }
    else if (m == "POST") {  method_ = kPost; }
    else if (m == "HEAD") {  method_ = kHead; }
    else if (m == "PUT")  {  method_ = kPut;  }
    else if (m == "DELETE"){ method_ = kDelete; }
    else                   { method_ = kInvalid;}
    return method_ != kInvalid;
  }
  Method method() const { return method_; }

  const char* methodString() const  // 请类型字符串
  {
    const char* result = "UNKNOWN";
    switch(method_)
    {
      case kGet:    result = "GET";    break;
      case kPost:   result = "POST";   break;
      case kHead:   result = "HEAD";   break;
      case kPut:    result = "PUT";    break;
      case kDelete: result = "DELETE"; break;
      default: break;
    }
    return result;
  }
  // 请求行的URL
  void setPath(const char* start, const char* end) { path_.assign(start, end); }
  const string& path() const { return path_; }

  void setQuery(const char* start, const char* end) { query_.assign(start, end);  }
  const string& query() const { return query_; }

  void setReceiveTime(Timestamp t) { receiveTime_ = t; }
  Timestamp receiveTime() const  { return receiveTime_; }

  // 请求头的添加键值对
  void addHeader(const char* start, const char* colon, const char* end)
  {
    string field(start, colon);  // 要求冒号前无空格
    ++colon;
    while (colon < end && isspace(*colon)) {  // 过滤冒号后的空格
       ++colon;
    }
    string value(colon, end);
    while (!value.empty() && isspace(value[value.size()-1])){
      value.resize(value.size()-1);
    }
    headers_[field] = value;
  }
  // 请求头部查找键的值
  string getHeader(const string& field) const
  {
    string result;
    std::map<string, string>::const_iterator it = headers_.find(field);
    if (it != headers_.end()){
      result = it->second;
    }
    return result;
  }

  const std::map<string, string>& headers() const { return headers_; }

  void swap(HttpRequest& that)  // 交换
  {
    std::swap(method_, that.method_);
    std::swap(version_, that.version_);
    path_.swap(that.path_);
    query_.swap(that.query_);
    receiveTime_.swap(that.receiveTime_);
    headers_.swap(that.headers_);
  }

 private:
  Method method_;						// 请求行 - 请求方法
  Version version_;						// 请求行 - HTTP版本
  string path_;							// 请求行 - URL
  string query_;						// 请求体
  Timestamp receiveTime_;				// 请求事件
  std::map<string, string> headers_;	// 请求头部
};

添加请求头键值对的字符串中，冒号前不能有空格，例如"key:value"或"key: value"是正确的，"key :value"就是错误的。

请求行中的URL中可能带有请求参数，以问号"?"分割，后面的请求参数使用键值对方式，并作为请求体保存。

2、HttpResponse 类
主要用于构造一个HttpResponse，调用成员函数设置响应头部、响应体，调用appendToBuffer()格式化到Buffer中，回复给客户端。

class HttpResponse : public muduo::copyable
{
 public:
  enum HttpStatusCode{
    kUnknown,
    k200Ok = 200,					// 正常
    k301MovedPermanently = 301,	    // 资源不可访问，重定向
    k400BadRequest = 400,			// 请求错误（域名不存在、请求不正确）
    k404NotFound = 404,				// 通常是URL不正确（或者因为服务不再提供）
  };

  explicit HttpResponse(bool close) : statusCode_(kUnknown), closeConnection_(close){}

  void setStatusCode(HttpStatusCode code) { statusCode_ = code; }

  void setStatusMessage(const string& message) { statusMessage_ = message; }

  void setCloseConnection(bool on) { closeConnection_ = on; }

  bool closeConnection() const { return closeConnection_; }

  void setContentType(const string& contentType) { addHeader("Content-Type", contentType); }

  // FIXME: replace string with StringPiece
  void addHeader(const string& key, const string& value) { headers_[key] = value; }

  void setBody(const string& body) { body_ = body; }

  void appendToBuffer(Buffer* output) const;  // 将整个HttpRespose对象按照协议输出到Buffer中

 private:
  std::map<string, string> headers_;   	// 响应头部，键值对
  HttpStatusCode statusCode_;		   	// 响应行 - 状态码
  // FIXME: add http version
  string statusMessage_;				// 响应行 - 状态码文字描述
  bool closeConnection_;				// 是否关闭连接
  string body_;  						// 响应体
};

函数HttpResponse::appendToBuffer(Buffer* output)默认使用HTTP1.1版本，按照HTTP协议对HttpResponse对象进行格式化输出到Buffer中。

void HttpResponse::appendToBuffer(Buffer* output) const
{
  char buf[32];
  // 响应行
  snprintf(buf, sizeof buf, "HTTP/1.1 %d ", statusCode_);
  output->append(buf);
  output->append(statusMessage_);
  output->append("\r\n");

  // 响应头部
  if (closeConnection_){
    output->append("Connection: close\r\n");
  } else{
    snprintf(buf, sizeof buf, "Content-Length: %zd\r\n", body_.size());
    output->append(buf);
    output->append("Connection: Keep-Alive\r\n");
  }

  for (const auto& header : headers_){
    output->append(header.first);
    output->append(": ");
    output->append(header.second);
    output->append("\r\n");
  }

  output->append("\r\n");  // 空行
  output->append(body_);   // 响应体
}

注意添加请求头时，需要删除键值对的字符串左侧和右侧的空字符，保证解析正常。因为解析请求头时，对一行字符串用冒号“:”进行分割解析。

3、HttpContext 类
服务端接收客户请求通过HttpContext解析，解析后数据封装到HttpRequest中。

class HttpContext : public muduo::copyable
{
 public:
  enum HttpRequestParseState
  {
    kExpectRequestLine,		// 请求行
    kExpectHeaders,			// 请求头
    kExpectBody,		    // 请求体
    kGotAll,
  };
  // 构造函数，默认从请求行开始解析
  HttpContext() : state_(kExpectRequestLine) { }

  // default copy-ctor, dtor and assignment are fine

  // return false if any error
  bool parseRequest(Buffer* buf, Timestamp receiveTime); // 解析请求Buffer

  bool gotAll() const { return state_ == kGotAll; }

  void reset()  // 为了复用HttpContext
  {
    state_ = kExpectRequestLine;
    HttpRequest dummy;
    request_.swap(dummy);
  }

  const HttpRequest& request() const { return request_; }
  HttpRequest& request() { return request_; }

 private:
  bool processRequestLine(const char* begin, const char* end);

  HttpRequestParseState state_; // 解析状态
  HttpRequest request_;
};

主要关注如何解析Buffer中的文本（请求行、请求头）部分、请求体（可能是二进制或其他文本格式）。一个正常的请求，一般至少是有请求行的，默认解析状态为kExpectRequestLine。

3.1、请求解析 parseRequest()
函数原型为bool HttpContext::parseRequest(Buffer* buf, Timestamp receiveTime)，传入需要解析的Buffer对象和接收时间，根据期望解析的部分进行处理。

bool HttpContext::parseRequest(Buffer* buf, Timestamp receiveTime)
{
  bool ok = true;
  bool hasMore = true;
  while (hasMore)
  {
    if (state_ == kExpectRequestLine)  // 解析请求行
    { // 查找Buffer中第一次出现“\r\n”的位置
      const char* crlf = buf->findCRLF();
    if (crlf)
      { // 若找到“\r\n”，说明至少有一行数据，进行实际解析
        // buf->peek()为数据开始部分，crlf为结束部分
        ok = processRequestLine(buf->peek(), crlf);
        if (ok){ // 解析成功
          request_.setReceiveTime(receiveTime);
          buf->retrieveUntil(crlf + 2); // buffer->peek()向后移2字节，到下一行
          state_ = kExpectHeaders;
        }
        else { hasMore = false;}
      }
      else {  hasMore = false; }
    }
    else if (state_ == kExpectHeaders)  // 解析请求头
    {
      const char* crlf = buf->findCRLF(); // 找到“\r\n”位置
      if (crlf)
      {
        const char* colon = std::find(buf->peek(), crlf, ':'); // 定位分隔符
        if (colon != crlf){
          request_.addHeader(buf->peek(), colon, crlf); // 键值对解析
        }
        else{
          // empty line, end of header
          // FIXME:
          state_ = kGotAll;
          hasMore = false;
        }
        buf->retrieveUntil(crlf + 2); // 后移2个字符
      }
      else{ hasMore = false; }
    }
    else if (state_ == kExpectBody)  // 解析请求体，未实现
    {
      // FIXME:
    }
  }
  return ok;
}

请求体解析，可以根据请求体协议的定义，两个CRLF后（最后一个请求体的CRLF，空行的CRLF）的数据就是请求体部分。修改部分代码如下

    else if (state_ == kExpectHeaders)
    {
      const char* crlf = buf->findCRLF();
      if (crlf){
        const char* colon = std::find(buf->peek(), crlf, ':');
        if (colon != crlf){
          request_.addHeader(buf->peek(), colon, crlf);
        }
        else{
          // empty line, end of header
          state_ = kExpectBody;  //继续查找后续数据
        }
        buf->retrieveUntil(crlf + 2);
      }
      else{
        hasMore = false;
      }
    }
    else if (state_ == kExpectBody){
      if(buf->readableBytes()){  //如果还有数据，就是请求体
        request_.setQuery(buf->peek(), buf->beginWrite());
      }
      state_ = kGotAll;
      hasMore = false;
    }

3.1、请求行的解析 processRequestLine()
请求行有固定格式Method URL Version CRLF，URL中可能带有请求参数。根据空格符进行分割成三段字符。URL可能带有请求参数，使用"?”分割解析。

bool HttpContext::processRequestLine(const char* begin, const char* end)
{
  bool succeed = false;
  const char* start = begin;
  const char* space = std::find(start, end, ' ');
  // 第一个空格前的字符串，请求方法
  if (space != end && request_.setMethod(start, space))
  {
    start = space+1;
    space = std::find(start, end, ' ');
    if (space != end)
    {  // 第二个空格前的字符串，URL
      const char* question = std::find(start, space, '?');
      if (question != space){  // 如果有"?"，分割成path和请求参数
        request_.setPath(start, question);
        request_.setQuery(question, space);
      }
      else{
        request_.setPath(start, space); // 仅path
      }
      start = space+1;
      // 最后一部分，解析HTTP协议
      succeed = end-start == 8 && std::equal(start, end-1, "HTTP/1.");
      if (succeed)
      {
        if (*(end-1) == '1'){  // 1.1版本
          request_.setVersion(HttpRequest::kHttp11);
        }
        else if (*(end-1) == '0'){  // 1.0版本
          request_.setVersion(HttpRequest::kHttp10);
        }
        else{
          succeed = false;
        }
      }
    }
  }
  return succeed;
}

## 系统测试

### 测试环境

### 服务器单元测试

### 服务器集成测试

### 系统性能测试

### 测试结果分析

## 总结

### 全文总结

### 未来工作展望

## 致谢

## 参考文献

[1] Nancy J. Yeager; Robert E. McGrath (1996). *Web Server Technology*. ISBN "ISBN (identifier)" 1-55860-376-X. Archived from the original on 20 January 2023. Retrieved 22 January 2021.

[2] [字节跳动在 Go 网络库上的实践](https://www.cloudwego.io/zh/blog/2020/05/24/%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8%E5%9C%A8-go-%E7%BD%91%E7%BB%9C%E5%BA%93%E4%B8%8A%E7%9A%84%E5%AE%9E%E8%B7%B5/)

[3] [Netty Project. Netty Project Community. [2019-01-31].](https://web.archive.org/web/20190130065314/https://netty.io/)

[4] Linux 多线程服务端编程（使用muduo C++网络库）.陈硕 .电子工业出版社 .2021.04

[5] Silberschatz, Abraham; Galvin, Peter Baer; Gagne, Greg. Operating System Concepts. Hoboken, NJ: John Wiley & Sons. 2008. ISBN 978-0-470-12872-5

[6] [RFC 793](https://datatracker.ietf.org/doc/html/rfc793)

[7] [什么是事件驱动的架构？](https://www.ibm.com/cn-zh/topics/event-driven-architecture)

[8] [如何深刻理解Reactor和Proactor？ - 小林coding的回答 - 知乎](https://www.zhihu.com/question/26943938/answer/1856426252)

[9] [Efficient IO with io_uring](https://kernel.dk/io_uring.pdf)

[10] [INTERACTION WITH OTHER PROGRAMS, LIBRARIES OR THE ENVIRONMENT](http://pod.tst.eu/http://cvs.schmorp.de/libev/ev.pod#THREADS_AND_COROUTINES)

[11] [Java线程池实现原理及其在美团业务中的实践](https://tech.meituan.com/2020/04/02/java-pooling-pratice-in-meituan.html)

[12] [eventfd(2) — Linux manual page](https://man7.org/linux/man-pages/man2/eventfd.2.html)

[13] [select(2) — Linux manual page](https://man7.org/linux/man-pages/man2/select.2.html)

[14] [poll(2) — Linux manual page](https://man7.org/linux/man-pages/man2/poll.2.html)

[15] [epoll(7) — Linux manual page](https://man7.org/linux/man-pages/man7/epoll.7.html)
