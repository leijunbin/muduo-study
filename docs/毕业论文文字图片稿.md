# 毕业论文文字图片稿

## 摘要和关键词

### 中文

### 英文

### 关键词

## 绪论

### 课题背景

### 课题主要研究内容

### 主要工作内容

### 论文结构

## 网络服务器软件相关技术

### TCP 协议

### HTTP 协议

### Socket

Socket 套接字是通信的基石，是支持 TCP/IP 协议的路通信的基本操作单元。可以将套接字看作不同主机间的进程进行双间通信的端点，它构成了单个主机内及整个网络间的编程界面。套接字存在于通信域中，通信域是为了处理一般的线程通过套接字通信而引进的一种抽象概念。套接字通常和同一个域中的套接字交换数据(数据交换也可能穿越域的界限，但这时一定要执行某种解释程序)，各种进程使用这个相同的域互相之间用 Internet 协议簇来进行通信。[18]

Socket 套接字的工作原理是通过网络连接来传输数据。当一个应用程序需要发送数据时，它会创建一个 Socket 套接字，并将数据发送到套接字。套接字将数据传输到网络上，然后将数据传输到目标计算机的套接字。目标计算机的套接字将数据传输到目标应用程序中。[20]

Socket 套接字有三种类型：流套接字、数据报套接字、原始套接字。流套接字是一种面向连接的套接字，它提供了可靠的、有序的、基于字节流的数据传输；数据报套接字是一种无连接的套接字，它提供了不可靠的、无序的、固定大小的数据传输；原始套接字可以直接接收本机网卡上的数据帧或者数据包。[19]

### 事件驱动模式

事件驱动模式是一种围绕事件的发布、捕获、处理和存储(或持久化)而设计软件架构和模式。事件驱动模式在网络服务器领域支持了应用和服务之间的松散耦合，它们可以通过发布和消费事件相互通信。[7]而其中应用最为广泛的是以下两种事件驱动模式：Reactor 模式和 proactor 模式。

#### Reactor 模式

Reactor 模式是非阻塞同步网络模式，捕获就绪的可读写事件。由于 Reactor 模式的灵活多变，使得其存在多种方案可供选择。[8]这里将着重介绍单 Reactor 多线程方案，其方案流程图如图 2-1 所示：

![img](./image/Reactor%E6%A8%A1%E5%BC%8F.png)

+ Reactor 监听事件，收到事件后通过 dispatch 进行分发——若是建立连接事件，则交由 Acceptor 获取连接并创建 Handle 对象分发至子线程中来处理后续响应事件；如果不是连接建立事件，则交由当前连接对应的 Handler 对象来进行响应。
+ Handler 对象负责 I/O 操作和业务处理。

#### Proactor 模式

Proactor 模式是异步网络模式，捕获已完成的读写事件，其方案流程图如图 2-2 所示：

![img](./image/Proactor%E6%A8%A1%E5%BC%8F.png)

+ Proactor Initiator 负责创建 Proactor 和 Handler 对象，并将 Proactor 和 Handler 都通过 Asynchronous Operation Processor 注册到内核。
+ Asynchronous Operation Processor 负责处理注册请求，并处理 I/O 操作。
+ Asynchronous Operation Processor 完成 I/O 操作后通知 Proactor。
+ Proactor 根据不同的事件类型回调不同的 Handler 进行业务处理。
+ Handler 完成业务处理。

### 多路 I/O 复用

传统的网络编程模型是阻塞式的，每个连接需要创建一个线程或进程来处理请求，这种方式在连接数量较少的情况下还可以接受，但是当连接数量增多时，线程或进程的创建和切换会占用大量的 CPU 资源，导致服务器性能下降。为解决传统网络编程模型的弊病，多路 I/O 复用技术诞生了——其可以在单线程或少数线程的情况下监听多个网络请求，并等待这些请求中的任何一个有数据到达。一旦有数据到达，线程或进程就会被唤醒，可以立即处理该请求并返回数据。这种方式可以有效地降低服务器的 CPU 开销，同时可以处理更多的连接请求。

在 Linux 中，实现多路 I/O 复用的系统调用有 select[13]、poll[14]、epoll[15]，它们的区别在于实现方式和适用场景不同，但对于网络服务器场景，epoll 更加适合，占据了几乎所有 Linux 高性能网络服务器的多路 I/O 复用部分。下面将详细分析 select、poll 在网络服务器场景下的缺点和 epoll 的优势。

#### select 和 poll 的缺点

select 在网络服务器场景下的缺点如下：

+ 单个进程能够监听的文件描述符数量存在最大限制，通常是1024[13]。虽然可以更改数量，但由于 select 采用轮询的方式扫描文件描述符，文件描述符数量越多，性能越差。
+ 内核、用户空间内存拷贝问题，select 需要复制大量的句柄数据结构（文件描述符），产生巨大的开销。
+ select 返回的是含有整个句柄的数组，应用程序需要遍历整个数组才能发现哪些句柄发生了事件。

相比 select，poll 使用链表保存文件描述符，因此没有监听文件描述符数量的限制，但其他两个缺点依然存在。[16]

#### epoll 的优势

epoll 的设计和实现与 select、poll 完全不同。epoll 把原先的 select、poll 调用分成以下 3 个部分：

+ 调用 epoll_create() 建立一个 epoll 对象。
+ 调用 epoll_ctl() 向 epoll 对象中添加连接的 Socket。
+ 调用 epoll_wait() 收集发生的事件的文件描述符资源。

而 epoll 对象中对于大量 Socket 采用红黑树的数据结构进行管理，使得其查找、插入、删除 Socket 的效率显著高于 select 和 poll 的遍历方式。
同时，epoll_wait 的效率也非常高——在调用 epoll_wait 时，不会将全部 Socket 再从内核态复制到用户态，而是仅将活跃的 Socket 复制到用户态，显著减少了复制的开销。[16]

#### 水平触发和边缘触发

多路 I/O 复用的系统调用存在两种触发方式——边缘触发和水平触发。下面将介绍这两种触发方式：

+ 水平触发：当被监控的文件描述符上有可读写事件发生时，会通知用户程序去读写，如果用户一次读写没取完数据，他会一直通知用户。
+ 边缘触发：当被监控的文件描述符上有可读写事件发生时，会通知用户程序去读写，它只会通知用户进程一次，这需要用户一次性将内容读写完成。如果用户未能一次性将数据读完，再次请求时，不会立即返回，需要等待下一次的新数据到来时才会返回，这次返回的内容包括上次未读完的数据。

而在现实的网络编程中，水平触发的使用更多，主要有以下三点原因：

+ 高可靠性：其不会丢失数据或消息，应用没有读取完数据，内核会不断上报。
+ 低延时处理：每次读数据只需要一次系统调用，照顾了多个连接的公平性，不会因为某个连接上的数据量过大而影响其他连接处理消息。
+ 跨平台处理：其像 select 一样可以跨平台使用。[16]

### timerfd

timerfd 是 Linux 为用户程序提供的一个定时器接口。这个接口基于文件描述符，通过文件描述符的可读事件进行超时通知，因此可以配合多路 I/O 复用系统调用使用。其主要有以下几个接口：

+ timerfd_create() 创建一个定时器对象，同时返回一个与之关联的文件描述符。
+ timerfd_settime() 用于设置新的超时时间，并开始计时，能够启动和停止定时器。
+ timerfd_gettime() 函数获取距离下次超时剩余的时间。

### eventfd

eventfd 是 Linux 特有的专用于事件通知的文件描述符。利用 eventfd() 创建 eventfd 后，可用 read(2) 读取 eventfd，若 eventfd 是阻塞的，read(2) 可能阻塞线程；若 eventfd 设置了 EFD_NONBLOCK，read(2) 返回 EAGIAN 错误。直到另外一个线程对 eventfd 进行 write(2) 后才可正常返回。[12]

### 线程池技术

在并发环境下，系统不能够确定在任意时刻中，有多少任务需要执行，有多少资源需要投入。这种不确定性将带来以下若干问题：

+ 频繁申请、销毁资源和调度资源，将带来额外的消耗，可能会非常巨大。
+ 对资源无限申请缺少抑制手段，易引发系统资源耗尽的风险。
+ 系统无法合理管理内部的资源分布，会降低系统的稳定性。

为解决以上问题，线程池应运而生。线程池维护多个线程，等待监督管理者分配可并发执行的任务。这种做法，相比于一般的线程管理方式有以下四点优势：

+ 降低资源消耗：通过池化技术重复利用已创建的线程，降低线程创建和销毁造成的损耗。
+ 提高响应速度：任务到达时，无需等待线程创建即可立即执行。
+ 提高线程的可管理性：线程是稀缺资源，如果无限制创建，不仅会消耗系统资源，还会因为线程的不合理分布导致资源调度失衡，降低系统的稳定性。使用线程池可以进行统一的分配、调优和监控，保证了对内核的充分有效的利用。
+ 提供更多更强大的功能：线程池具备可拓展性，允许开发人员向其中增加更多的功能。如延时定时线程池，就允许任务延期执行或定期执行。[11]

## 整体架构设计

### 网络服务器软件需求分析

结合网络服务器软件的现实需求、行业标准、开源社区相关讨论等，一个优秀的网络服务器软件应该满足以下需求[1]：

+ 网络服务器软件能够在操作系统的底层能力支持下，完成以下两个主要任务：
  + 接收并管理来自于不同网络地址，且携带不同网络信息的网络连接，同时为它们分配操作系统资源并协调它们之间的工作；
  + 最大限度地满足各类用户的网络服务要求，及时的响应用户服务请求，为互联网服务用户及时的提供所需要的服务信息，包括但不限于请求的回复报文、静态存储的文件、操作系统的状态等。
+ 网络服务器软件在架构上应该足够抽象，以达成可以利用配置文件、中间件或者插件、软件提供相关配置选项的 API 、甚至于将网络服务器软件作为底层，在上层编写相关代码来实现完成特定网络服务的网络服务器软件等不同方式来适应不同的操作系统、多样的网络协议和网络服务需求、日益增长的网络服务用户数量等任务。
+ 网络服务器软件应该为软件开发者提供友好的编程接口和符合人体工程学的编程范式，同时提供详细且易于理解的使用样例及相关文档。
+ 网络服务器软件在现实中可能出现的各种异常情况下应该通过良好的异常处理机制——包括但不限于日志记录、告警处理、自动恢复等方式来保证网络服务不会被可恢复异常中断，且在发生不可恢复异常时能够提供相关信息以帮助软件开发者快速定位问题，降低不可恢复异常所造成的损失。

### 系统架构设计

基于 3.1 节的需求分析，同时结合了其他优秀的网络服务器软件开源项目的架构（Netpoll[2]、Netty[3]、muduo[4] 等），设计了本网络服务器软件的架构。网络服务器软件整体架构图如图 4-1 所示。网络服务器软件整体架构分为操作系统、公共组件、事件分发、网络协议、用户五层，下面将介绍每一层和相关模块的主要功能。

![img](./image/%E6%9E%B6%E6%9E%84%E5%9B%BE.png)

#### 操作系统

操作系统为计算机硬件提供了抽象和管理，为用户和应用程序提供了接口与服务[5]，但在不同类型的操作系统上相同功能的实现细节、所需的系统调用以及编程上的最佳实践都千差万别，这种差异使得软件的跨平台适配大多都极其繁琐。考虑到网络服务器领域的现实需求和个人能力，本网络服务器软件在架构和实现上都为类 UNIX 系统的适配留下相应接口，但只对 Linux 系统实现完整的网络服务器软件功能。

具体到本服务器软件，类 UNIX 操作系统应当能够为其提供以下接口与服务支持：

+ 网络通信的基础支持，包括网络协议栈、Socket 接口等功能。
+ I/O 多路复用机制。
+ 线程和进程管理机制。
+ 文件系统能够访问并修改磁盘或其他存储设备上的数据。
+ 接收时钟的定时中断，获取时钟的时间信息。

#### 公共组件

公共组件将操作系统提供的接口与服务封装成功能模块，以便于网络服务器软件上层能够忽略底层操作系统的系统调用细节，通过更友好的编程接口来使用操作系统所提供的接口和服务支持，同时为上层应用提供如异步日志等通用功能组件的接口支持。

公共组件各个功能模块的具体功能介绍如下：

+ 网络：对常用网络编程系统接口的封装，包括 Socket、InetAddress、Buffer 部分。

  + Socket 部分是对 Socket 文件描述符及其各种常用操作的封装，如创建 Socket、绑定地址、监听、接受连接等。
  + InetAddress 部分是对网络地址及其相关操作的封装，可以获取本端和对端的 ip 地址和端口信息，也可以传入 ip 地址和端口信息用来设置 Socket 文件描述符所绑定的地址。
  + Buffer 部分是网络服务器软件数据缓冲区的实现。由于网络服务器软件所使用到的网络协议（主要是 TCP 协议）大多是无边界的字节流协议[6]，可能会发生读取或写入数据时不能通过一次系统调用完成的情况，这时就需要缓冲区来承接这部分数据以保证程序不会因此阻塞——例如服务器发送 100 字节的数据，但使用系统调用后只写入 80 字节，在缓冲区存在的情况下，服务器软件可以将剩余数据放入缓冲区，并将 Socket 是否可写列入文件描述符监听队列，交出线程控制权，直至 Socket 可写后激活线程并发送缓冲区数据。
+ I/O 复用：对多路 I/O 复用系统调用的封装。基于 2.4 节的对于多路 I/O 复用的介绍，本网络服务器软件最终选用了水平触发的 epoll 系统调用来实现多路 I/O 复用。包括 Channel、Poller、EpollPoller 部分。

  + Channel 部分是对文件描述符的封装，每个 Channel 对象与一个文件描述符对应，其中定义了该文件描述符的可监听的事件、事件的回调函数等。
  + Poller 部分抽象了 I/O 多路复用的操作，如注册事件、删除事件等；并提供实现接口，为在其它类 UNIX 系统下实现本网络服务器软件可用的 I/O 多路复用功能给出了符合人体工程学的编程范式。
  + EpollPoller 部分是对 Linux 下 epoll 相关的多路 I/O 复用系统调用的封装，继承自 Poller 类，补全了接口函数，实现了 Linux 下本网络服务器软件可用的 epoll 多路 I/O 复用功能。
+ Timestamp：本网络服务器软件的微秒级时间戳部分，具有以下功能：

  + 通过操作系统获取当前微秒级时间戳。
  + 可比较时间戳先后顺序、计算时间戳之间的差值、计算某个时间戳在一定时间后的另一个时间戳等。
  + 将时间戳转换成为形如“2023/04/24 23:41:31”格式的时间字符串。
+ Thread：对操作系统线程管理能力的封装，有创建、销毁线程等功能。
+ 定时器：对操作系统接收定时中断能力的封装，且能够在收到定时中断后处理对应事件。包括 Timer、TimerQueue 部分。

  + Timer 部分定义了定时任务的属性以及对其进行读取和修改的函数方法。
  + TimerQueue 部分是定时器队列类，实现了对定时器队列的管理，如添加、删除定时事件等。
+ 异步日志：将代码运行时的重要信息进行保存，方便故障诊断和追踪，包括日志前端和日志后端。

  + 日志前端主要为开发人员提供异步日志接口，能够使其按照等级写入日志并在控制台输出日志信息。包括 Logger、LogStream 部分。
    + Logger 部分为用户提供日志库的接口，能够设置日志等级、控制日志输出流等。
    + LogStream 部分主要提供日志流操作，将用户提供的整型数、浮点数、字符、字符串、字符数组、二进制内存、另一个日志缓冲区，格式化为字符串，并加入当前流的日志缓冲区。
  + 日志后端将前端写下的日志写入文件，放入计算机存储设备中使其持久化。包括 FileUtil、LogFile、AsyncLogging 部分。
    + FileUtil 部分封装操作系统提供的底层的创建、打开文件，写文件，关闭文件等操作接口。
    + LogFile 部分提供对日志文件的操作，包括滚动日志文件、将日志数据写到当前日志文件、刷新日志数据到当前日志文件；同时保证底层操作的线程安全。
    + AsynccLogging 部分提供缓冲存放多条日志信息，为前端线程提供线程安全的写日志操作；提供专门的后端线程，用于定时或缓冲非空时，将缓冲中日志信息逐个写到磁盘上。

#### 事件分发

Linux 在 5.1 版本才支持可应用于网络编程的异步 I/O 系统调用—— io_uring，此前仅支持同步阻塞或非阻塞的网络 I/O 系统调用[8]。故基于 2.4 节的介绍和 3.1.1 节的操作系统取舍，本网络服务器软件将采用 Reactor 模式作为事件分发部分的事件驱动模式。

事件分发部分作为网络服务器软件的核心，其在内部实现了 Reactor 模式，向上层代码提供了一组符合直觉的事件驱动调用接口，开发人员可以基于该接口适配其他网络协议。事件分发包括 EventLoop、EventLoopThread、EventLoopThreadPool 三部分。

+ EventLoop 部分是事件循环类，负责接收 Channel 对象上的事件并将其分发到对应的事件处理函数中。
+ EventLoopThread 部分是封装了一个新线程，并在该线程上创建一个 EventLoop 对象，用于处理事件。
+ EventLoopThreadPool 部分是封装了一个线程池，用于创建一组 EventLoopThread 对象，以此提高并发处理能力和操作系统资源利用率。

#### 网络协议

网络协议基于事件分发部分的接口，实现特定网络通信协议的特性，为应用程序提供网络协议支持。开发人员也可以在本部分适配其他的网络协议或特性。本网络服务器软件实现了 TCP 协议和 HTTP 协议。

+ TCP 部分实现对 TCP 连接的管理。包括 TcpConnection、TcpServer、Acceptor 部分。

  + TcpConnection 部分是对一条 TCP 连接的封装，包括读写数据、关闭连接等。
  + TcpServer 部分是对 TCP 服务器的封装，负责监听并处理新的 TCP 连接。
  + Acceptor 部分用于接收来自其他网络地址的 TCP 网络连接并将其分发至线程池中经过分配策略所决定的工作线程。
+ HTTP 部分实现 HTTP 协议中的 GET 和 POST 请求， 实现了静态文件传输和下载的功能。包括 HttpServer、HttpContext、HttpResponse、HttpRequest 部分。

  + HttpServer 部分是对 HTTP 服务器的封装，负责监听并处理新的 HTTP 连接。
  + HttpContext 部分是对 HTTP 报文解析的封装，负责解析收到的 HTTP 报文信息。
  + HttpResponse 部分是对 HTTP 请求类的封装，负责管理 HttpContext 解析完成的报文信息。
  + HttpRequest 部分是对 HTTP 响应类的封装，负责管理服务器响应报文的数据并将其格式化为字符串报文。

#### 用户

用户部分使得使用该系统的各类用户，包括软件开发人员和网络服务的使用者，它们可以通过软件开发人员在这一部分实现的业务代码，从而使网络服务的使用者得到其所需要的服务数据。

### 处理流程

本网络服务器软件存在三个业务处理流程：网络连接处理流程、异步日志处理流程、定时器处理流程。

#### 网络连接处理流程

网络服务器软件的本质是处理四个事件[4]，即：

+ 建立连接。
+ 断开连接。
+ 读取网络信息。
+ 发送网络消息。

故网络协议之间细节可能不同，但流程都大致相似，故下文将以 TCP 协议为例介绍本网络服务器软件的网络连接处理流程。图 4-2 为本网络服务器软件在 TCP 协议下的处理流程图。

![img](./image/TCP%E7%BD%91%E7%BB%9C%E8%BF%9E%E6%8E%A5%E6%B5%81%E7%A8%8B%E5%9B%BE.png)

##### 建立连接

使用 Socket 来创建一个简单的 TCP 服务器时，建立一个连接通常需要四个步骤：

+ 调用 socket 函数建立监听 socket。
+ 调用 bind 函数绑定需要监听的地址和端口。
+ 调用 listen 函数监听端口。
+ 调用 accept 函数返回新建立连接的文件描述符。

而在本网络服务器软件中，也基本遵循了以上步骤：

+ 构造一个事件循环器 EventLoop。
+ 建立对应的业务服务器 TcpServer。在构造 TcpServer 时，会创建 Acceptor 对象，在 Acceptor 对象的构造函数中，会执行 socket 函数和 bind 函数。
+ 然后 main 函数执行 TcpServer 的 TcpServer::start() 方法完成以下两个任务——启动线程池；调用 listen 函数，并将 listen 所监听的文件描述符添加到可读事件的监听队列中。
+ 设置 TcpServer 的对应的回调函数。
+ main 函数会执行 EventLoop::loop() 方法来启动事件循环。在事件循环中，当有连接到来后，会执行 Acceptor 中 handleRead 回调函数，进而调用 TcpServer 中的 newConnection 回调函数，执行 accept 函数并生成对应的文件描述符并将其绑定至一个 TcpConnection 实例中，并注册相应的回调函数。随后将文件描述符分发至线程池中的特定子线程的事件循环中。

##### 读取网络信息

读取网络信息主要有以下两个步骤：

+ 在子线程的事件循环中，当连接中有数据送达时，就会调用 TcpConnection 中的 handleRead 回调函数，将连接送达的数据放入请求数据缓冲区（inputBuffer）中。
+ 调用用户注册的 onMessage 回调函数，执行网络服务的业务逻辑。

##### 发送网络信息

在用户注册的 onMessage 回调中会调用 TcpConnection::send() 方法来发送网络信息，这个方法会调用 TcpConnection::sendInLoop() ，其处理过程如下：

+ 若响应数据缓冲区（outputBuffer） 为空，则直接向网络连接文件描述符写数据。
+ 若向网络连接文件描述符写的数据未写完，则记录剩余的字节数。
+ 若此时响应数据缓冲区（outputBuffer）中的旧数据的个数和未写完字节个数和大于高水位表记值（highWaterMark） ，则调用回调函数 highWaterMarkCallback。否则将剩余数据写入响应数据缓冲区（outputBuffer）。
+ 若向网络连接文件描述符写的数据没有写完，则最后需要为其注册可写事件。当其可写事件发生时，调用对应的回调 TcpConnection::handleWrite()，尽可能将数据从响应数据缓冲区（outputBuffer）中向网络连接文件描述符中写数据。
+ 如果响应数据缓冲区（outputBuffer）中的数据都写完了，将网络连接文件描述符的写事件移除，并调用写完成回调 writeCompleteCallback。

##### 断开连接

断开连接分为被动断开和主动断开。主动断开和被动断开的处理流程基本一致，所以被动断开为例介绍断开连接的流程。

被动断开即客户端断开了连接，服务端需要感知到这个断开的状态，然后进行的相关的处理。其中感知远程断开这一步是在 TCP 连接的可读事件处理函数 TcpConnection::handleRead() 中进行的：当对网络连接文件描述符进行 read 操作时，返回值为 0，则说明此时连接已断开。然后调用 TcpConnection::handleClose() 函数。其执行过程如下：

+ 将网络连接文件描述符从监听队列中移除。
+ 调用用户的设置的 onConnection() 函数。
+ 将网络连接文件描述符对应的 TcpConnection 对象从 TcpServer 中移除。
+ 关闭网络连接文件描述符。

#### 异步日志处理流程

![](https://img2022.cnblogs.com/blog/741401/202203/741401-20220306223622054-297119160.png)

采用双缓冲区（double buffering）交互技术。基本思想是准备2部分buffer：A和B，前端（front end）线程往buffer A填入数据（日志消息），后端（back end）线程负责将buffer B写入日志文件。当A写满时，交换A和B。如此往复。

实现时，在后端设置一个已满缓冲队列（Buffer1~n，2<=n<=16），用于缓存一个周期内临时要写的日志消息。

这样做到好处在于：
1）线程安全；2）非阻塞。

这样，2个buffer在前端写日志时，不必等待磁盘文件操作，也避免每写一条日志消息都触发后端线程。

异常处理：
当一个周期内，产生过多Buffer入队列，当超过队列元素上限数量值25时，直接丢弃多余部分，并记录。

#### 定时器处理流程

不同定时器接口的处理流程一致，故将以 EventLoop 中 runAt() 接口流程为例介绍定时器处理流程，图 4-4 为 runAt() 的处理流程图。

![img](./image/%E5%AE%9A%E6%97%B6%E5%99%A8%E6%B5%81%E7%A8%8B%E5%9B%BE.png)

+ 当 EventLoop 被创建时，与其同生命周期的 TimerQueue 定时器队列类和其管理的 timerfd 也一并被创建。
+ 执行 runAt() 接口创建单次定时事件时，调用 TimerQueue::addTimer() 向 TimerQueue 的定时器队列中添加定时任务，同时向 EventLoop 中的 Poller 注册监听 timerfd 事件。
+ 当 timerfd 到达预定时间被唤醒时，执行其回调函数 TimerQueue::handleRead()，执行定时任务。若其为循环任务，则重新添加定时任务，注册监听 timerfd；反之则删除定时任务，为下一个定时任务注册监听 timerfd。

## 各模块的设计与实现

### 网络

网络部分类图如下图 4-? 所示：

![img](./image/%E7%BD%91%E7%BB%9C%E7%B1%BB%E5%9B%BE.png)

#### InetAddress

InetAddress 类对地址信息进行了包装，是 sockaddr_in 的包装类，提供 sockaddr_in 的常用方法。

+ toIpPort(), toIp(), toPort()：将 IP 地址、Port 信息由 sockaddr 对象，转换为字符串或整形。
+ getLocalAddr()：从连接获取本地ip地址（包括端口号）。
+ getPeerAddr()：获取连接对端的ip地址（包括端口号）。

#### Socket

Socket 类是 Socket 文件描述符的一个轻量级封装，提供操作系统 Socket 文件描述符的常用方法。

+ setReuseAddr(), setReusePort()：设置是否开启复用 IP 地址和端口的特性。
+ setKeepAlive()：是否设置心跳检测以保持 TCP 长连接。
+ setTcpNoDelay()：用于设置 TCP_NODELAY 选项，以禁用 Nagle 算法，从而不会等到收到 ACK 才进行下一次数据发送，而是 TCP 协议栈缓冲中有数据就立即发送。
+ createNonblockingOrDie()：创建非阻塞 Socket 文件描述符。
+ bindAddress()：绑定 Socket 文件描述符与本地地址。
+ listen()：监听本地 Socket 文件描述符。
+ accept()：接受连接请求。
+ shutdownWrite()：关闭连接写方向。
+ getSocketError()：获取tcp协议栈错误。通常在处理连接的读写事件时调用，检查是否发生错误。

* isSelfConnect()：检查是否为自连接。利用了 InetAddress 类的 getLocalAddr() 和 getPeerAddr() 函数，检查 IP 地址是否相同，来判断连接对端地址信息是否为本机。

#### Buffer

##### Buffer 数据结构

![img](https://img2022.cnblogs.com/blog/741401/202204/741401-20220412171109103-1465773870.png)

readIndex、writeIndex 把 buffer_ 内容分为三部分：prependable、readable、writable。灰色部分是 Buffer 的有效载荷，prependable 能让程序以极低代价在数据前添加几个字节，从而简化客户代码。

Buffer中有两个个常数：kCheapPrepend，kInitialSize，分别定义了 prependable 初始大小，writable 的初始大小。readable 初始大小 0。

初始化完成后，Buffer 数据结构如下：
![img](https://img2022.cnblogs.com/blog/741401/202204/741401-20220412173407579-1717880116.png)

##### 基本 I/O 操作

Buffer 初始化完成后，向 Buffer 写入 200 byte，其布局是：

![img](https://img2022.cnblogs.com/blog/741401/202204/741401-20220412173502458-783290308.png)

可以看到，writeIndex 向后移动了 200，readIndex 保持不变。readable、writable 都有变化。

如果从 Buffer 读入 50 byte，其布局是：
![img](https://img2022.cnblogs.com/blog/741401/202204/741401-20220412173532217-1169824755.png)

可以看到，readIndex 向后移动 50，writeIndex 保持不变，readable 和 writable 的值也有变化。

经过若干次读写，readIndex 移到较靠后的位置，留下了很大的 prependable 空间，如下图已由初始 8 byte，变成 532 byte。
![img](https://img2022.cnblogs.com/blog/741401/202204/741401-20220412173616065-1397427603.png)

此时，若想写入 300 byte，Buffer 不会重新分配内存，而是先把已有的数据移到前面去，减小多余 prependable 空间，为 writable 腾出空间。
![img](https://img2022.cnblogs.com/blog/741401/202204/741401-20220412173620685-816517251.png)

这样，writable 变成 724 byte，就可以写入 300 byte 数据了。

Buffer 长度不是固定的，可以自动增长，因为底层存储利用的是 vector。

假设当前可写空间 writable 为 624 byte，现客户代码一次写入 1000 byte，那么 buffer 会自动增长，如下图。

增长前：
![img](https://img2022.cnblogs.com/blog/741401/202204/741401-20220412173542958-1236803670.png)

增长后：
![img](https://img2022.cnblogs.com/blog/741401/202204/741401-20220412173546598-1288273385.png)

readable 由 350 增长为 1350，writable 由 624 减为 0。另外，readIndex 由 58 回到了初始位置 8，保证prependable 等于 kCheapPrependable。Buffer 没有缩小功能，下次写入 1350 byte 就不会重新分配内存，一方面避免浪费内存，另一方面避免反复分配内存。

##### 重要函数

+ retrieve：retrieve 系列函数从 readable 空间取走数据，只关心移动 readableIndex_，改变 readable 空间大小，通常不关心读取数据具体内容，除非有指定具体的返回值，如 retrieveAllAsString()。其会改变 readable 空间大小，但通常不会改变 writable 空间大小，除非 retrieveAll() 取完所有 readable 空间的数据，readable 空间将会合并到 writable 空间。
+ peek：peek 系列函数只从 readable 空间头部读取数据，而不取走数据，不会导致 readable 空间变化。
+ makeSpace()：makeSpace() 生产足够大小的 writable 空间，以写入新的数据.
+ readFd()：readFd() 从指定连接对应文件描述符读取数据，当读取的数据超过内部缓冲区 writable 空间大小时，采用的策略是先用一个 64K 栈缓存 extrabuf 临时存储，然后根据需要合并 prependable 空间到 writable 空间，或者重新分配 buffer_ 大小。
+ writeFd(): readFd() 向指定连接对应文件描述符读取数据。
+ findCRLF()：对 readable 空间中的 CRLF 字符进行专门的查找。

### I/O 复用

I/O 复用部分类图如下图 4-? 所示：

![img](./image/IO%E5%A4%8D%E7%94%A8%E7%B1%BB%E5%9B%BE.png)

#### Channel

Channel 成员函数主要包括以下八类：

+ 设置事件处理的回调函数，如 setReadCallback()。
+ 使能文件描述符关心的事件 events_ 会注册到 Poller 中进行监听，如 enableReading()。
+ 关闭文件描述符关心的事件 events_，更新该文件描述符在 Poller 中监听的事件，如 disableReading()。
+ 关闭文件描述符关心的所有事件 events_，更新该文件描述符在 Poller 中监听的事件，如 disableAll()。
+ 删除对文件描述符的监听，会将其从 Poller 的 ChannelMap 中移除,如 remove()。
+ handleEvent() 处理激活的 Channel 事件，由 Poller 更新激活的 Channel 列表，EventLoop::loop() 根据激活 Channel 列表，逐个执行 Channel 中已注册好的相应回调。实际事件处理工作，由 handleEventWithGuard 完成。其中 tie 变量保证了执行事件处理动作时，所需对象不会被释放。handleEventWithGuard() 根据不同的激活原因，调用不的回调函数。这些回调函数，需要进行事件监听的类中进行设置，比如 TcpConnection，EventLoop，Acceptor，TimerQueue 等。
+ update() 通过 EventLoop 对象，传递给 Poller 对象，然后更新其监听的通道列表中对应通道。支持 ADD、MOD 操作。
+ remove() 通过 EventLoop 传递给 Poller 对象，将当前通道从 Poller 的事件列表中删除。支持 DEL 操作。

#### Poller 和 EPollPoller

EPollPoller 以 epoll 为核心，实现了基类 Poller 的 virtual 函数，在其中调用了 epoll_create、epoll_ctl、epoll_wait 等接口。poll() 返回后，会将就绪的文件描述符添加到激活队列 activeChannels 中管理。

### 定时器

定时器部分类图如下图 4-? 所示：

![img](./image/%E5%AE%9A%E6%97%B6%E5%99%A8%E7%B1%BB%E5%9B%BE.png)

#### Timer 和 TimerId

Timer 类代表一个超时任务，但并不直接绑定 Channel。Timer 主要包含超时时刻（expiration_），超时回调（callback_），周期时间值（interval_），全局唯一id（sequence_）。每当创建一个新 Timer 对象时，原子变量 s_numCreated_ 就会自增一，作为全剧唯一序列号 sequence_，用来标识该 Timer 对象。

在创建 Timer 时，超时时刻 when 决定了回调超时事件时间点，而 interva 决定了 Timer 是一次性的，还是周期性的。如果是周期性的，会在 TimerQueue::reset() 中，调用 Timer::restart()，在当前时间点基础上，重启定时器。

restart() 重启 Timer，根据 Timer 是否为周期类型，分为两种情况：

+ 周期 Timer，restart() 将重置超时时刻 expiration_ 为当前时间加上周期间隔时间。
+ 非周期 Timer，即一次性 Timer，restart() 将 expiration_ 置为无效时间，默认为 0。

TimerId 类来主要用来作为 Timer 的唯一标识，用于取消 Timer。

#### TimerQueue

定时器队列 TimerQueue 类是定时功能的核心，其包含 3 个 Timer 集合：

+ timers_ 定时器集合：包含用户添加的所有 Timer 对象，std::set 会用 AVL 搜索树，对集合元素按时间戳（Timestamp）从小到大顺序。
+ activeTimers_ 激活定时器集合：包含激活的 Timer 对象，与 timers_ 包含的 Timer 对象相同，个数也相同，std::set 会根据 Timer 指针大小，对元素进行排序。
+ cancelingTimers_ 取消定时器集合：包含所有取消的 Timer 对象，与 activeTimers_ 相对。

构造 TimerQueue 对象时，就会绑定 TimerQueue 创建 TimerQueue 的 EventLoop 对象。同时调用 Channel::enableReading() 将 timerfd 加入 Poller 的监听列表中。

在析构 TimerQueue 对象有两点需要注意：

+ 在 remove() 绑定的 timerfd 前，要先 disableAll() 停止监听所有 timerfd 事件。
+ timers_ 中 Timer 对象是在 TimerQueue::addTimer() 中使用 new 关键字创建出来的，需要手动 delete。

addTimer() 会在构造一个 Timer 对象后，将其添加到 timers_ 的工作转交给 addTimerInLoop() 完成调用——调用定时器函数的线程，可能并非在 TimerQueue 所在的 loop 线程，而修改 TimerQueue 数据成员时，必须在所属 loop 线程中进行，因此需要通过 loop_->runInLoop() 将工作转交给所属 loop 线程。

addTimerInLoop() 的主要工作由两个函数来完成：insert 将 Timer 插入 timers_ 和 activeTimers_，resetTimerfd 重置 timerfd 到下一次定时事件的触发时间。

TimerQueue::cancel() 可以清除一个尚未到期的定时器。其类似于 addTimer()，cancel() 也可能在别的线程被调用，因此需要将其转交给 cancelInLoop() 执行。

timerfd 的可读事件回调函数 handleRead() 实现有三个要点：

+ 其必须在所在 loop_ 线程运行。
+ 可能不止一个定时任务超时，可用 getExpired() 获取。
+ 所有超时任务执行完后，重置周期定时任务，释放一次性定时任务。

getExpired() 以参数时间点 now 为界限，查找 set timers_ 中所有超时定时任务。set 会对 timers_ 元素进行排序，std::set::lower_bound() 会二分查找到第一个小于 now 时间点的定时任务，同时调用 reset 重置所有超时的周期定时任务，释放超时的一次性任务。

### 异步日志

#### FixedBuffer

FixedBuffer 类图如下图 4-? 所示：

![img](./image/FixedBuffer%E7%B1%BB%E5%9B%BE.png)

FixedBuffer 模板类内部是用数组 data_[SIZE] 存储，用指针 cur_ 表示当前待写数据的位置。对FixedBuffer 的各种操作，实际上是对 data_ 数组和 cur_ 指针的操作。

在异步日志系统中，其有以下两种具体实现：

+ Small Buffer，默认大小 4 KB，用于存放前端日志消息，为前端类 LogStream 持有。
+ Large Buffer，默认大小 4 MB，用于存放大量后端日志消息。为后端类 AsyncLogging 持有。

#### 日志前端

日志前端主要包括：Logger, LogStream，SourceFile。其类图如下图 4-? 所示：
![img](./image/%E6%97%A5%E5%BF%97%E5%89%8D%E7%AB%AF%E7%B1%BB%E5%9B%BE.png)

##### Logger

Logger 支持以下六种日志等级：

+ TRACE：指出比 DEBUG 粒度更细的一些信息事件。
+ DEBUG：指出细粒度信息事件对调试应用程序是非常有帮助的。
+ INFO：表明消息在粗粒度级别上突出强调应用程序的运行过程。
+ WARN：系统能正常运行，但可能会出现潜在错误的情形。
+ ERROR：指出虽然发生错误事件，但仍然不影响系统的继续运行。
+ FATAL：指出每个严重的错误事件将会导致应用程序的退出。

本网络服务器软件默认级别为 INFO，开发过程中可以选择 TRACE 或 DEBUG。

日志信息是由 Logger 的内部类 Logger::Impl 所提供，其包含了一条完整日志信息的所有组成部分，并通过 Logstream::operator<< 将日志信息加入到 Small Buffer 中。

Logger 的用户接口则通过定义了一系列 LOG_ 开头的宏，每个宏定义都构造了一个 Logger 临时对象，然后通过 stream() 函数来达到以 C++ 风格写日志的能力。同时定义全局变量存储当前日志等级（g_logLevel），通过宏定义的 if 语句，使得日志仅记录不低于当前日志等级的日志；低于当前日志等级的不会有任何操作，几乎 0 开销。而且用户还可以调用 setOutput()、setFlush()、setLogLevel() 接口来调整日志的输出位置（g_output），日志冲刷方式（g_flush）,当前日志等级（g_logLevel）等日志系统基本配置。

而在其析构函数中，其主要工作是为 LogStream 对象 stream_ 中的日志消息加上后缀（如文件名、行号，换行符等），将 stream_ 缓存的 log 消息通过 g_output 回调写入指定文件流。另外，如果有致命错误（FATAL级别log），就终止程序。

##### LogStream

针对不同类型数据，LogStream 重载了一系列 operator<< 操作符，用于将数据格式化为字符串，并存入LogStream::buffer_ （Small Buffer）中。

+ 对于字符串类型参数，operator<< 本质上是调用 buffer_ 对应的FixedBuffer::append()，将其存放当到 Small Buffer 中。
+ 对于字符类型，跟参数是字符串类型区别是长度只有一，并且无需判断指针是否为空。
+ 对于十进制整型，如 int/long，则是通过模板函数 formatInteger()，将转换为字符串并直接填入Small Buffer 尾部。formatInteger() 并没有用 snprintf 对整型数据进行格式转换，而是用到了 Matthew Wilson 提出的高效的转换方法 convert()[21]。基本思想是：从末尾开始，对待转换的整型数，由十进制位逐位转换为 char 类型，然后填入缓存，直到剩余待转数值已为 0。
+ 对于double类型，使用库函数 snprintf 转换为 const char*，并直接填入 Small Buffer 尾部。
+ 对于其他类型，都是转换为以上基本类型，然后再转换为字符串，添加到 Small Buffer 末尾。

#### 日志后端

日志后端主要包括：AsyncLogging, LogFile, FileUtil。其类图如下图 4-? 所示：

![img](./image/%E6%97%A5%E5%BF%97%E5%90%8E%E7%AB%AF%E7%B1%BB%E5%9B%BE.png)

##### AsyncLogging

AsyncLogging 类提供 Large Buffer 存放多条日志消息，缓冲队列 BufferVector 用于存放多个 Large Buffer，为前端线程提供线程安全的写 Large Buffer 操作；提供专门的后端线程，用于定时或缓冲队列非空时，将缓冲队列中的 Large Buffer 通过 LogFile 提供的日志文件操作接口，逐个写到磁盘上。

提供线程安全的写 Large Buffer 操作的函数是 append()，其基本思路是当前缓冲（currentBuffer_）剩余空间足够存放新日志消息大小时，就直接存放到当前缓冲；当前缓冲剩余空间不够时，说明当前缓冲已满（或者接近已满），就将当前缓冲 move 到已满缓冲队列（buffers_），将空闲缓冲 move 到当前缓冲，再把新日志消息存放到当前缓冲中，最后唤醒等待中的后端线程。同时 append() 可能会被多个前端线程调用，因此必须考虑线程安全。这里用 mutex_ 加锁来保证数据的线程安全。

后端线程的启动是在 start() 中，通过调用 Thread::start() 完成。而 stop() 用于关闭后端线程，通常是在析构函数中调用其停止后端线程。

后端线程函数 threadFunc()，会构建一个 LogFile 对象，用于控制日志文件创建和日志数据的写入；创建两个空闲缓冲区 buffer1、buffer2，和一个待写缓冲队列 buffersToWrite，分别用于替换当前缓冲 currentBuffer_、空闲缓冲 nextBuffer_、已满缓冲队列 buffers_，避免在写文件过程中，锁住缓冲和队列，导致前端无法写数据到后端缓冲。

threadFunc() 中，提供了一个执行循环，其基本流程如下：

+ 每次当已满缓冲队列中有数据时，或者即使没有数据但3秒超时，就将当前缓冲加入到已满缓冲队列（即使当前缓冲没满），将 buffer1 移动给当前缓冲，buffer2 移动给空闲缓冲（如果空闲缓冲已移动的话）。
+ 再交换已满缓冲队列和待写缓冲队列。
+ 将待写缓冲队列的所有缓冲通过 LogFile 对象，写入日志文件。
+ 待写缓冲队列中的缓冲，已全部写到 LogFile 指定的文件中（也可能在内核缓冲中）后，擦除多余缓冲，只用保留两个，归还给 buffer1 和 buffer2。
+ 清空待写缓冲队列中的缓冲。
+ 将内核高速缓存中的数据 flush 到磁盘，防止意外情况造成数据丢失。

##### LogFile

当日志文件接近指定的滚动限值（rollSize_）时，需要换一个新文件写数据，便于后续归档、查看。调用 rollFile() 函数可以实现文件滚动，其基本流程如下：

+ 利用 getLogFileName() 根据调用者提供的基础名，以及当前时间，得到一个全新的、唯一的日志文件名。
+ 创建并打开一个新日志文件，用指向 LogUtil 对象的指针 file_ 表示。

而为避免频繁创建新文件，rollFile() 会确保上次滚动时间到现在如果不超过一秒，就不会滚动。

LogFile 提供了两个个接口，用于向当前日志文件 file_ 写入数据—— append() 本质上是通过 appendInLock() 完成对日志文件写操作，但使用互斥锁 mutex_ 来保证线程安全；appendInLock() 会先将日志信息写入 file_ 文件，之后再判断是否需要滚动日志文件；如果不滚动，就根据 append_unlocked 的调用次数和时间，确保一个日志文件超时（默认1天），就创建一个新日志文件，同时调用 FileUtil::flush() 函数刷新文件操作（默认间隔3秒）。

flush() 函数实际上是通过 AppendFile::flush()，完成对日志文件的冲刷，也使用互斥锁 mutex_ 来保证线程安全。

##### FileUtil

FileUtil 类提供主要功能的有三个函数：append()、write()、flush()。其中，append() 是将指定数据附加到文件末尾，但实际的写文件操作是通过 write() 来完成的；write() 出于性能考虑通过非线程安全的 glibc 库函数 fwrite_unlocked() 来完成写文件操作，而线程安全将由上层调用 LogFile 来保证。

### Thread

Thread 类图如下图 4-? 所示：
![img](./image/Thread%E7%B1%BB%E5%9B%BE.png)

+ start() 函数用来初始化子线程，获取子线程操作系统编号，执行子线程函数。
+ join() 函数阻塞主线程，等待子线程终止后回收相关资源。

### Timestamp

Timestamp 类图如下图 4-? 所示：

![img](./image/Timestamp%E7%B1%BB%E5%9B%BE.png)

+ 成员变量 microSecondsSinceEpoch_ 用来来表示从 Epoch 时间到目前为止的微妙数。
+ valid() 函数判断 microSecondsSinceEpoch_ 有效性。
+ now() 函数用 gettimeofday() 获取当前时刻，转化为微秒，并构造一个 Timestamp 对象。
+ toString() 将秒数、微秒数转换为形如“2023/04/24 23:41:31”格式的时间字符串。
+ operator<() 比较两个个时间戳先后顺序。
+ addTime() 利用一个基准时间戳 timestamp 加上一段时间的秒数，得到新的 Timestamp 对象。

### 事件分发

事件分发部分类图如下图 4-? 所示：
![img](./image/%E4%BA%8B%E4%BB%B6%E5%88%86%E5%8F%91%E7%B1%BB%E5%9B%BE.png)

#### EventLoop

基于 3.2.3 节中关于 EventLoop 功能的讨论和图 4-? 中 EventLoop 类图，其设计和实现可分为四个部分：

+ 提供事件循环。
+ 运行定时任务。
+ 处理激活通道事件。
+ 线程安全。

##### 事件循环

事件循环主要有以下四个接口，loop()、quit()、wakeup()、handleRead()。

loop() 是EventLoop 实现事件循环的核心函数。其利用 Poller::poll() 函数将激活事件填入 Channel 队列 activeChannels_，然后排队执行每个 Channel 的 handleEvent，从而调用对应激活事件的回调函数。

quit() 函数通过修改 quit_ 的值提供了退出事件循环的接口。

wakeup() 函数用于通过向类内自创建的 wakeupFd_ 文件描述符写入信息来唤醒被阻塞的事件循环。

handleRead() 函数用于读取 wakeupFd_ 文件描述符中的信息来完成可读事件。

##### 处理激活通道事件

处理激活通道事件的接口有以下三个：updateChannel()、removeChannel()、hasChannel()。

updateChannel()、removeChannel() 利用 Poller::updateChannel 和 Poller::removeChannel 更新或移除 Poller 监听的事件。

hasChannel() 来判断 Poller 是否正在监听某个 Channel。

##### 定时任务

定时任务的接口有以下四个：runAt()、runAfter()、runEvery()、cancel()。其中前三个接口利用 TimerQueue::addTimer() 函数添加一次性定时任务或周期定时任务，最后一个接口调用 TimerQueue::cancel() 函数取消定时任务。

##### 线程安全

一个 EventLoop 的实例的接口函数可能会出现多个线程并发执行的情况，这使得保证类私有数据的线程安全就格外重要。所以 EventLoop 在设计上采用“one loop per thread”的设计思想——即一个事件循环（EventLoop）对应一个线程，这在大部分情况是一个非常好的线程模型，大大减少了因数据竞争而产生的并发问题。[10]

同时在具体实现和使用时，本软件使用了以下四种种方法来实现以上的设计思想：

+ 在每个子线程中设置其自己拥有的全局 EventLoop 变量 t_loopInThisThread，用以保证每个子线程中都运行唯一的 EventLoop 实例。
+ 实现 isInLoopThread、assertInLoopThread 函数用以判断调用函数线程和 EventLoop 事件循环线程是否是同一线程，以保证事件循环函数始终在同一线程中执行。
+ 实现 runInLoop、queueInLoop 函数，用以处理来自不同线程的用户任务，具体分为以下两种情况：
  + 若当前线程是 EventLoop 事件循环线程，则立即执行用户任务。
  + 反之，则加入用户任务队列 pendingFunctors_ 中，唤醒事件循环后排队执行。
+ 用户任务队列 pendingFunctors_ 使用互斥锁保证不同线程之间使用用户任务队列的线程安全。

#### EventLoopThread

EventLoopThread 的结构比较简单，对外只提供启动 I/O 线程的接口 startLoop()。startLoop() 接口过程就是启动在初始化时创建的 I/O 线程，然后等待 I/O 线程函数执行 threadFunc() 函数完成初始化，并将 loop_ 同步值返回给函数调用者。其中 threadFunc() 函数也叫做 I/O 线程函数 主要工作是创建 EventLoop 局部对象，并将其值同步至 EventLoop 的 loop_，然后运行事件循环。

##### 线程安全

从上文介绍可以看出 EventLoopThread 中的 loop_ 指针在创建时（startLoop()中），会存在调用线程和子线程函数 threadFunc 并发读、写 loop_ 的情况。为保证 loop_ 的数据线程安全，在实现时采用了以下两种方式：

+ 采用互斥锁在读写时保护 loop_，同时使用条件变量来同步 loop_ 的值。
+ I/O 线程函数退出时，线程中事件循环已不在运行，这时应清空 loop_ 的值，否则可能会导致析构函数重复调用 loop_ 的 quit() 函数，让 I/O 线程循环重复退出。

#### EventLoopThreadPool

EventLoopThreadPool 线程池类通常由 main 函数线程创建，绑定 main 函数线程创建的 EventLoop 到 baseLoop_ 中。

线程池在创建后，通过调用 start() 接口来启动线程池。其主要工作如下：

+ 保证 baseLoop_ 所属线程调用 start() 接口。
+ 创建用户指定数量的线程（其数量由用户调用 setThreadNum() 接口确定），并启动线程，记录子线程对应 EventLoop。
+ 若没有指定线程数量（或为指定 0），调用用户指定的线程初始化回调函数。

getNextLoop() 接口从 I/O 线程池维护的 EventLoop 数组 loops_ 中轮询取得一个 EventLoop 对象，每次调用取下一个 EventLoop 对象（若取到最后一个，则下一个是数组中第一个 EventLoop 对象），利用此分配策略以达到负载均衡的目的。

### TCP

muduo使用TcpConnection类来管理TCP连接，使用接受器Acceptor来接受连接，连接器Connector发起连接。TcpServer管理accept获得TcpConnection，生命周期由用户控制。

下图是TcpServer新建连接的相关函数调用顺序。当Channel::handleEvent()的触发条件是listening socket可读时，表明有新连接请求达到。TcpServer为新连接创建对应的TcpConnection对象。

![](https://img2022.cnblogs.com/blog/741401/202203/741401-20220324083202877-1291752519.png)

#### Acceptor类

Acceptor是TcpServer的一个内部类，主要职责是用来获得新连接的fd。保存用户提供的Connection-Callback和MessageCallback，新建TcpConnection对象（newConn()）的时候直接传递给TcpConnection的构造函数。

```c++
/**
* TCP连接接受器
* 基础调用为accept(2)/accept4(2)
*/
class Acceptor : private noncopyable
{
public:
    typedef std::function<void(int sockfd, const InetAddress &)> NewConnectionCallback;

    Acceptor(EventLoop* loop, const InetAddress& listenAddr, bool reuseport);
    ~Acceptor();

    /* 设置新连接回调 */
    void setNewConnectionCallback(const NewConnectionCallback& cb)
    { newConnectionCallback_ = cb; }

    /* 监听本地端口 */
    void listen();

    /* 判断当前是否正在监听端口 */
    bool listening() const { return listening_; }

private:
    void handleRead();      // 处理读事件

    EventLoop *loop_;        // 所属EventLoop
    Socket acceptSocket_;    // 专门用于接受连接的套接字(sock fd)
    Channel acceptChannel_;  // 专门接受连接通道, 监听conn fd
    NewConnectionCallback newConnectionCallback_; // 新建连接回调
    bool listening_;         // 监听状态
    int idleFd_;             // 空闲fd, 用于fd资源不够用时, 可以空一个出来作为新建连接conn fd
};
```

如果fd资源不够用了，导致accept(2)/accept4(2)创建连接失败，比如达到系统上限，怎么办？
Accetor用了这样一种技术：先申请一个空闲的fd（idleFd_），等到发生由于fd资源不够用时，就把这个备用fd暂时用于accept接收连接，然后再马上关闭，以防止不断产生可读事件（连接请求），从而回调相同的失败代码。及早建立连接后并关闭连接，让程序不会频繁响应同一个连接请求。

#### Acceptor构造与析构

Acceptor构造时，创建sockfd（套接字），待后续交给TcpServer来start监听套接字。
空闲fd指向文件"/dev/null"，用来解决服务器fd资源耗尽问题。

```c++
// Acceptor.cc
Acceptor::Acceptor(EventLoop *loop, const InetAddress &listenAddr, bool reuseport)
: loop_(loop),
  acceptSocket_(sockets::createNonblockingOrDie(listenAddr.family())),
  acceptChannel_(loop, acceptSocket_.fd()),
  listening_(false),
  idleFd_(::open("/dev/null", O_RDONLY | O_CLOEXEC)) // 申请空闲fd
{
    assert(idleFd_ >= 0);
    acceptSocket_.setReuseAddr(true);
    acceptSocket_.setReusePort(reuseport);
    acceptSocket_.bindAddress(listenAddr);
    acceptChannel_.setReadCallback(
            std::bind(&Acceptor::handleRead, this));
}

Acceptor::~Acceptor()
{
    acceptChannel_.disableAll(); // disable all event of the channel
    acceptChannel_.remove(); // remove the channel from poller
    ::close(idleFd_);
}
```

#### Acceptor监听

Acceptor包含2类监听：1）监听套接字，即本地ip地址&端口。2）监听通道事件，读事件。

* 为什么不在构造时，就调用listen监听sockfd呢？
  将非必要资源的初始化，延迟到需要时，用户可以通过调用TcpSever::start()来启动。这样，用户可以更灵活控制资源的申请和释放。

```c++
// Acceptor.cc
/**
* 监听本地sock fd, 使能监听Channel读事件
*/
void Acceptor::listen()
{
    loop_->assertInLoopThread();
    listening_ = true;
    acceptSocket_.listen();         // 使能监听本地sock fd(ip, port)
    acceptChannel_.enableReading(); // 使能监听通道读事件
}
```

#### Acceptor接受连接

Acceptor内部有一个Channel成员，当Poller监听到有Tcp连接请求时，就通过Channel的可读事件，在loop线程，来回调Acceptor::handleRead()。从而将conn fd和IP地址传递给上一层TcpServer，用于创建TcpConnection对象管理Tcp连接。

```c++
// Acceptor.cc
/**
* 处理读Channel事件, accept连接
* @note 先accept, 然后将相关资源通过回调交由上一层的TcpServer进行处理(管理)
*/
void Acceptor::handleRead()
{
    loop_->assertInLoopThread();
    InetAddress peerAddr;
    // FIXME loop until no more
    int connfd = acceptSocket_.accept(&peerAddr); // 获取连接fd及对端ip地址
    if (connfd >= 0)
    {
        if (newConnectionCallback_)
        { // 创建新连接回调
            newConnectionCallback_(connfd, peerAddr);
        }
        else
        {
            sockets::close(connfd);
        }
    }
    else
    { // 错误
        LOG_SYSERR << "in Acceptor::handleRead";
        /*
         * Read the section named "The special problem of
         * accept()ing when you can't" in libev's doc.
         * By Marc Lehmann, author of libev.
         *
         * The per-process limit of open file descriptors has been reached.
         */
        if (errno == EMFILE)
        { // 文件描述符资源耗尽错误
            ::close(idleFd_);
            idleFd_ = ::accept(acceptSocket_.fd(), NULL, NULL);
            ::close(idleFd_);
            // reopen /dev/null, it dose not matter whether it succeeds or fails.
            idleFd_ = ::open("/dev/null", O_RDONLY | O_CLOEXEC);
        }
    }
}
```

#### TcpServer类

TcpServer类管理TcpConnection，供用户直接使用，生命周期由用户控制。接口如下，用户只需要设置好callback，然后调用start()即可。

```c++
// TcpServer.h
/**
* Tcp Server, 支持单线程和thread-poll模型.
* 接口类, 因此不要暴露太多细节.
*/
class TcpServer : private noncopyable
{
public:
    typedef std::function<void (EventLoop*)> ThreadInitCallback;
    enum Option
    enum Option
    {
        kNoReusePort, // 不允许重用本地端口
        kReusePort,   // 允许重用本地端口
    };

//    TcpServer(EventLoop* loop, const InetAddress& listenAddr);
    TcpServer(EventLoop* loop,
              const InetAddress& listenAddr,
              const std::string& nameArg,
              Option option = kNoReusePort);
    ~TcpServer(); // force out-line dtor, for std::unique_ptr members.

    /**
     * 如果没有监听, 就启动服务器(监听).
     * 多次调用没有副作用.
     * 线程安全.
     */
    void start();

    /**
     * 设置连接回调.
     * 非线程安全.
     */
    void setConnectionCallback(const ConnectionCallback& cb)
    { connectionCallback_ = cb; }

    /**
     * 设置消息回调.
     * 非线程安全.
     */
    void setMessageCallback(const MessageCallback & cb)
    { messageCallback_ = cb; }

    /**
     * 设置写完成回调.
     * 非线程安全.
     */
    void setWriteCompleteCallback(const WriteCompleteCallback& cb)
    { writeCompleteCallback_ = cb; }
```

注意到并没有连接关闭回调，这是由TcpServer::removeConnection()负责的，进而把工作转交给TcpConnection::connectDestroyed()，用户不可更改设置。

#### TcpServer的构造与析构

TcpServer构造函数主要工作是为成员申请资源，为各回调设置回调函数

```c++
TcpServer::TcpServer(EventLoop* loop,
                     const InetAddress& listenAddr,
                     const std::string& nameArg,
                     Option option)
: loop_(CHECK_NOTNULL(loop)), // 确保loop非空
  ipPort_(listenAddr.toIpPort()),  // 将Ip, port转换为字符串
  name_(nameArg), // 名称
  acceptor_(new Acceptor(loop, listenAddr, option == kReusePort)),
  threadPool_(new EventLoopThreadPool(loop, name_)), // 初始化事件循环线程池
  connectionCallback_(defaultConnectionCallback),    // 连接回调为默认连接回调
  messageCallback_(defaultMessageCallback),          // 消息回调为默认消息回调
  nextConnId_(1)  // 连接id
{
    acceptor_->setNewConnectionCallback(
            std::bind(&TcpServer::newConnection, this, _1, _2)); // 设置新建连接时的回调
}
```

同样是连接回调，TcpServer::newConnection()和connectionCallback_有何区别？
前者是Acceptor发生连接请求事件时，回调，用来新建一个Tcp连接；后者是在TcpServer内部新建连接即调用TcpServer::newConnection()时，回调connectionCallback_。

TcpServer析构工作内容很简单，主要销毁ConnectionMap中所有Tcp连接，而每个Tcp连接用的是一个TcpConnection对象来管理的。

```c++
/**
* 析构TcpServer对象, 销毁ConnectionMap中所有连接
*/
TcpServer::~TcpServer()
{
    loop_->assertInLoopThread();
    LOG_TRACE << "TcpServer::~TcpServer [" << name_ << "] destructing";


    // reset all connection of @c connections_
    for (auto& item : connections_)
    {
        TcpConnectionPtr conn(item.second); // shared_ptr manage TcpConnection
        item.second.reset();
        conn->getLoop()->runInLoop(
                std::bind(&TcpConnection::connectDestroyed, conn));
    }
}
```

TcpServer启动Tcp服务器，主要完成1）线程池的启动；2）Acceptor监听Tcp连接请求。

线程池需要指定其初始数量，当然，这需要在start()之前调用TcpServer::setThreadNum()设置。

```c++
/**
* 启动TcpServer, 初始化线程池, 连接接受器Accept开始监听(Tcp连接请求)
*/
void TcpServer::start()
{
    if (started_.getAndSet(1) == 0)
    {
        threadPool_->start(threadInitCallback_);


        assert(!acceptor_->listening());
        loop_->runInLoop(
                std::bind(&Acceptor::listen, get_pointer(acceptor_)));
    }
}
```

* TcpServer如何获得新连接的conn fd（accept返回值）？
  TcpServer内部用Acceptor，保存用户提供的Connection-Callback和MessageCallback，新建TcpConnection对象（newConn()）的时候直接传递给TcpConnection的构造函数。
* 如何创建TcpConnection对象？
  新连接请求到达时，Acceptor回调newConnection()，通过TcpServer::newConnection创建一个新TcpConnection对象，用于管理一个Tcp连接。
  即，TcpServer::newConnection回调顺序 EventLoop => Channel => Acceptor => TcpServer

下面是用于创建TcpConnection对象的函数TcpServer::newConnectio()

```c++
// TcpServer.cc
/**
* 新建一个TcpConnection对象, 用于连接管理.
* @details 新建的TcpConnection对象会加入内部ConnectionMap.
* @param sockfd accept返回的连接fd (accepted socket fd)
* @param peerAddr 对端ip地址信息
* @note 必须在所属loop线程运行
*/
void TcpServer::newConnection(int sockfd, const InetAddress &peerAddr)
{
    loop_->assertInLoopThread();
    /* 从EventLoop线程池中，取出一个EventLoop对象构造TcpConnection对象，便于均衡各EventLoop负责的连接数　*/
    EventLoop* ioLoop = threadPool_->getNextLoop(); // next event loop from the event loop thread pool

    /* 设置连接对象名称, 包含基础名称+ip地址+端口号+连接Id
     * 因为要作为ConnectionMap的key, 要确保运行时唯一性 */
    char buf[32];
    snprintf(buf, sizeof(buf), "-%s#%d", ipPort_.c_str(), nextConnId_);
    ++nextConnId_;
    std::string connName = name_ + buf;

    LOG_INFO << "TcpServer::newConnection [" << name_
    << "] - new connection [" << connName
    << "] from " << peerAddr.toIpPort();
  
    InetAddress localAddr(sockets::getLocalAddr(sockfd)); // 本地ip地址信息
    // FIXME poll with zero timeout to double confirm the new connection
    // FIXME use make_shared if necessary
    /* 新建TcpConnection对象,　并加入ConnectionMap */
    TcpConnectionPtr conn(new TcpConnection(ioLoop, connName, sockfd, localAddr, peerAddr));
    connections_[connName] = conn;
    /* 为新建TcpConnection对象设置各种回调 */
    conn->setConnectionCallback(connectionCallback_); // 连接回调
    conn->setMessageCallback(messageCallback_);       // 消息回调
    conn->setWriteCompleteCallback(writeCompleteCallback_); // 写完成回调
    conn->setCloseCallback( // 关闭连接回调
            std::bind(&TcpServer::removeConnection, this, _1)); // FIXME: unsafe
    /* 确认连接是否已建立, 并初始化连接建立后的状态 */
    ioLoop->runInLoop(std::bind(&TcpConnection::connectEstablished, conn));
}
```

#### TcpConnection类

TcpConnection类是muduo最核心的类，唯一默认用shared_ptr来管理的类，唯一继承自enable_shared_from_this的类。这是因为其生命周期模糊：可能在连接断开时，还有其他地方持有它的引用，贸然delete会造成空悬指针。只有确保其他地方没有持有该对象的引用的时候，才能安全地销毁对象。

```c++
// TcpConnection.h
/**
* Tcp连接, 为服务器和客户端使用.
* 接口类, 因此不要暴露太多细节.
* 
* @note 继承自std::enable_shared_from_this的类, 可以用getSelf返回用std::shared_ptr管理的this指针
*/
class TcpConnection : noncopyable,
        public std::enable_shared_from_this<TcpConnection> // std::shared_ptr<TcpConnection> getSelf()
{
public:
```

TcpConnection不提供用户使用的函数，所提供的接口主要给TcpServer/TcpClient使用。TcpConnection的状态有4个：kDisconnected, kConnecting, kConnected, kDisconnecting。TcpConnection使用Channel获得socket上IO事件，它会自己处理writable事件，把readable事件通过MessageCallback传递给客户。TcpConnection拥有TCP socket（Socket类），后者析构函数会close(fd)。

```c++
// TcpConnection.h
private:
    enum StateE { kDisconnected, kConnecting, kConnected, kDisconnecting }; // TCP连接状态定义
    void handleRead(Timestamp receiveTime); // 处理读事件
    void handleWrite();                     // 处理写事件
    void handleClose();                     // 处理关闭连接事件
    void handleError();                     // 处理错误事件

    /* loop线程中排队发送消息 */
//    void sendInLoop(std::string&& message); // C++11 // add by martin
    void sendInLoop(const StringPiece& message);
    void sendInLoop(const char* message, size_t len);

    /* loop线程中排队关闭写连接 */
    void shutDownInLoop();
//    void shutDownAndForceCloseInLoop(double seconds); // add by martin
    /* loop线程中排队关闭连接 */
    void forceCloseInLoop();

    /*　设置TcpConnection状态 */
    void setState(StateE s) { state_ = s; }

    /* 将状态转换为字符串 */
    const char* stateToString() const;

    /* loop线程中排队开始监听读事件 */
    void startReadInLoop();
    /* loop线程中排队关闭监听读事件 */
    void stopReadInLoop();

    EventLoop* loop_;     // 所属　EventLoop
    std::string name_;    // 名称
    StateE state_; // FIXME: use atomic varaible　//　状态
    bool reading_; // whether the connection is reading // 连接是否正在监听读事件
    // we don't expose those classes to client.
    std::unique_ptr<Socket> socket_;              // 连接套接字, 用于对连接进行底层操作
    std::unique_ptr<Channel> channel_;            // 通道, 用于绑定要监听的事件
    InetAddress localAddr_;                       // 本地IP地址
    InetAddress peerAddr_;                        // 对端IP地址
    ConnectionCallback connectionCallback_;       // 连接回调
    MessageCallback messageCallback_;             // 收到消息回调
    WriteCompleteCallback writeCompleteCallback_; // 写完成回调
    HighWaterMarkCallback highWaterMarkCallback_; // 高水位回调
    CloseCallback closeCallback_;                 // 关闭连接回调
    size_t highWaterMark_;                        // 高水位阈值
    Buffer inputBuffer_;                          // 输入缓冲区
    Buffer outputBuffer_; // FIXME: use list<Buffer> as output buffer.　//　输出缓冲区
    boost::any contex_;                           // 用户自定义参数
    // FIXME: createTime_, lastReceiveTime_
    //        bytesReceived_, bytesSent_
};
```

TcpConnection表示的是“一次Tcp连接”，不可再生，一旦连接断开，该TcpConnection对象就没用了。TcpConnection没用发起连接的功能，构造函数参数是已经建立好连接的socket fd，初始状态是kConnecting。连接可以是TcpServer或TcpClient发起。

接收到消息时，通过Channel::handleEvent会将可读事件转交给TcpConnection::handleRead处理，而TcpConnection::handleRead又会通过messageCallback_将可读事件转交给TcpServer::messageCallback_，进而传递给用户。

```c++
// TcpConnection.cc
/**
* 从输入缓存inputBuffer_读取数据, 交给回调messageCallback_处理
* @param receiveTime 接收到读事件的时间点
* @details 通常是TcpServer/TcpClient运行回调messageCallback_, 将处理机会传递给用户
*/
void TcpConnection::handleRead(Timestamp receiveTime)
{
    loop_->assertInLoopThread();
    int savedErrno = 0;
    ssize_t n = inputBuffer_.readFd(channel_->fd(), &savedErrno); // 从指定fd读取数据到内部缓冲
    if (n > 0)
    {
        messageCallback_(shared_from_this(), &inputBuffer_, receiveTime);
    }
    else if (n == 0)
    {
        handleClose();
    }
    else
    {
        errno = savedErrno;
        LOG_SYSERR << "TcpConnection::handleRead";
        handleError();
    }
}
```

#### 断开Tcp连接

#### 断开连接方式

muduo中有2种关闭连接的方式：
1）被动关闭：即对端先关闭连接，本地read(2)返回0，触发关闭逻辑，调用handleClose。
2）主动关闭：利用forceClose()或forceCloseWithDelay()成员函数调用handleClose，强制关闭或强制延时关闭连接。

被动关闭流程见下图，图中“X”表示TcpConnection对象通常在此析构。

![](https://img2022.cnblogs.com/blog/741401/202203/741401-20220324082607844-1314411437.png)

#### Channel与断开连接

Channel中有关关闭连接的事件回调CloseCallback，由Channel::handleEvent()调用，从而触发TcpConnection::handleClose()：

调用链路：
Poller::poll()检测到Channel事件就绪 => EventLoop::loop() =>Channel::handleEvent() => Channel::closeCallback_ => TcpConnection::handleClose()

```c++
// Channel.cc
void Channel::handleEvent(Timestamp recevieTime)
{
    std::shared_ptr<void> guard;
    if (tied_)
    {
        guard = tie_.lock();
        if (guard)
        {
            handleEventWithGuard(recevieTime);
        }
    }
    else
    {
        handleEventWithGuard(recevieTime);
    }
}

/**
* 根据不同的激活原因, 调用不同的回调函数
*/
void Channel::handleEventWithGuard(Timestamp receiveTime)
{
    eventHandling_ = true; // 正在处理事件
    LOG_TRACE << reventsToString(); // 打印fd及就绪事件
    if ((revents_ & POLLHUP) && !(revents_ & POLLIN))
    { // fd挂起(套接字已不在连接中), 并且没有数据可读
        if (logHup_)
        { // 打印挂起log
            LOG_WARN << "fd = " << fd_ << " Channel::handle_event() POLLHUP";
        }
        // 调用关闭回调
        if (closeCallback_) closeCallback_();
    }
    ...
}
```

#### TcpConnection与断开连接

包含CloseCallback事件回调，不过是给TcpServer和TcpClient用的，用于通知它们移除所持有的TcpConnectionPtr，而非给普通用户直接用的，普通用户应使用ConnectionCallback。

与断开连接有关的部分：

```c++
// TcpConnection.h
public:
    /* 关闭写半连接 */
    void shutdown(); // NOT thread safe, no simultaneous calling
//    void shutdownAndForceCloseAfter(double seconds); // NOT thread safe, no simultaneous calling

    /* 强制关闭连接 */
    void forceClose();
    /* 强制延时关闭连接 */
    void forceCloseWithDelay(double seconds);

    ...

    /* Internal use only */
    void setCloseCallback(const CloseCallback& cb)
    { closeCallback_ = cb; }

    // called when TcpServer accepts a new connection
    void connectEstablished(); // should be called only once per connection
    // called when TcpServer has removed me from its map
    void connectDestroyed(); // should be called only once per connection

private:
    void handleClose();                     // 处理关闭连接事件
```

* 被动关闭连接
  当收到对端FIN分节时，本地read返回0,，Tcp连接被动关闭，会触发调用本地TcpConnection::handleClose()，其定义如下：

```c++
/**
* 处理Tcp连接关闭
* @details 更新状态为kDisconnected, 清除所有事件通道监听
* @note 必须在所属loop线程中运行.
*/
void TcpConnection::handleClose()
{
    loop_->assertInLoopThread(); // 确保在所属loop线程中运行
    LOG_TRACE << "fd = " << channel_->fd() << " state = " << stateToString();
    assert(state_ == kConnected || state_ == kDisconnecting);
    // we don't close fd, leave it to dtor, so we can find leaks easily.
    setState(kDisconnected); // 更新Tcp连接状态
    channel_->disableAll();     // 停止监听所有通道事件(读写事件)

    TcpConnectionPtr guardThis(shared_from_this());
    connectionCallback_(guardThis); // 连接回调
    // must be the last line
    closeCallback_(guardThis);      //　关闭连接回调
}
```

closeCallback_在TcpServer::newConnection()为新连接新建TcpConnection时，已设为TcpServer::removeConnection()，而removeConnection()最终会调用TcpConnection::connectDestroyed()来销毁连接资源。

```c++
/**
* 主动销毁当前tcp连接, 移除通道事件
* @note 只有处于已连接状态(kConnected)的tcp连接, 才需要先更新状态, 关闭通道事件监听
*/
void TcpConnection::connectDestroyed()
{
    loop_->assertInLoopThread();
    if (state_ == kConnected) // 只有kConnected的连接, 才有必要采取断开连接动作
    {
        setState(kDisconnected);
        channel_->disableAll(); // 关闭通道事件监听

        connectionCallback_(shared_from_this()); // 调用连接回调
    }
    channel_->remove(); // 从EventLoop和Poller中移除监听通道事件
}
```

* 主动关闭连接
  按连接方向，有2类关闭方式：1）强制close连接；2）关闭一个方向的连接（读或写方向）。

```c++
/**
* 强制关闭连接, 只对连接为kConnected或kDisconnecting状态才有效
* @details 为防止意外, 动作应该放到loop末尾去做
*/
void TcpConnection::forceClose()
{
    // FIXME: use compare and swap
    if (state_ == kConnected || state_ == kDisconnecting)
    {
        setState(kDisconnecting);
        loop_->queueInLoop(std::bind(&TcpConnection::forceCloseInLoop, shared_from_this()));
    }
}

/**
* 在所属loop循环中强制关闭连接, 只对连接为kConnected或kDisconnecting状态才有效
*/
void TcpConnection::forceCloseInLoop()
{
    loop_->assertInLoopThread();
    if (state_ == kConnected || state_ == kDisconnecting)
    {
        // as if we received 0 byte in handleRead()
        handleClose();
    }
}
```

可以看到，除了状态更新，对于关闭连接的真正操作，被动、主动关闭连接都是由handleClose来完成的。

假设想延迟一段时间再关闭连接，可以调用forceCloseWithDelay()，区别在于交给EventLoop:runAfter()延时运行，而不是交给EventLoop::queueInLoop()。

```c++
void TcpConnection::forceCloseWithDelay(double seconds)
{
    if (state_ == kConnected || state_ == kDisconnecting)
    {
        setState(kDisconnecting);
        loop_->runAfter(seconds,
                        makeWeakCallback(shared_from_this(),
                                          &TcpConnection::forceClose)); // not forceCloseInLoop to avoid race condition
    }
}
```

关闭连接写方向，相当于库函数shutdown(2)

```c++
/**
* 关闭连接写方向, 只有已连接状态才有效
*/
void TcpConnection::shutdown()
{
    // FIXME: use compare and swap
    if (state_ == kConnected)
    {
        setState(kDisconnecting);
        // FIXME: shared_from_this()?
        loop_->runInLoop(std::bind(&TcpConnection::shutDownInLoop, shared_from_this()));
    }
}
```

#### TcpServer与断开连接

当新建一个tcp连接时，TcpServer会调用newConnection创建一个新TcpConnection对象管理Tcp连接，并将对象加入自己的ConnectionMap进行管理。

而当tcp连接断开时，需要调用removeConnection进行移除工作，而removeConnection会将工作转交给removeConnectionInLoop, 确保在所属loop线程中执行。

```c++
/**
* 转交给removeConnectionInLoop, 在所属loop线程中执行
*/
void TcpServer::removeConnection(const TcpConnectionPtr &conn)
{
    // FIXME: unsafe
    loop_->runInLoop(std::bind(&TcpServer::removeConnectionInLoop, this, conn));
}
```

removeConnectionInLoop要做的工作是将要移除的tcp连接对应TcpConnection对象，从ConnectionMap移除，然后销毁该对象。

```c++
/**
* 在所属loop线程循环中, 排队移除指定tcp连接 conn
* @param conn 指向待移除tcp连接对应TcpConnection对象
*/
void TcpServer::removeConnectionInLoop(const TcpConnectionPtr &conn)
{
    loop_->assertInLoopThread();
    LOG_INFO << "TcpServer::removeConnectionInLoop [" << name_
    << "] - connection " << conn->name();
    size_t n = connections_.erase(conn->name()); // 从ConnectionMap中擦除待移除TcpConnection对象
    (void)n;
    assert(n == 1);
    EventLoop* ioLoop = conn->getLoop();
    ioLoop->queueInLoop(
            std::bind(&TcpConnection::connectDestroyed, conn)); // 在所属loop线程中排队销毁TcpConnection对象
}
```

#### 错误处理

Channel在处理通道事件handleEvent时，如果发生错误（既不是读取数据，也不是写数据完成），

调用链路：
Channel::handleEvent() => 检测到错误，调用Channel::errorCallback_ => TcpConnection::handleError()

```c++
/**
* 处理tcp连接错误, 打印错误log
* @details 从tcp协议栈获取错误信息
*/
void TcpConnection::handleError()
{
    int err = sockets::getSocketError(channel_->fd());
    LOG_ERROR << "TcpConnection::handleError [" << name_
    << "] - SO_ERROR = " << err << " " << strerror_tl(err);
}
```

#### 发送数据

用于发送数据的TcpConnection::send()重载了下面几个版本

```c++
public:
    ...
    /* 发送消息给连接对端 */
//    void send(std::string&& mesage); // C++11
    void send(const void* message, int len);
    void send(const StringPiece& message);
//    void send(Buffer&& message);
    void send(Buffer* message); // this one will swap data

3个send() 重载版本，最终都会转交给sendInLoop(const char*, int)，在所属loop线程中执行发送工作。


// 转发给send(const StringPiece&), 最终转交给sendInLoop(const char*, int)
void TcpConnection::send(const void *message, int len)
{
    send(StringPiece(static_cast<const char*>(message), len));
}

/**
* 转交给 sendInLoop(const char*, int)
* 发送消息给对端, 允许在其他线程调用
* @param message 要发送的消息. StringPiece兼容C/C++风格字符串, 二进制缓存, 提供统一字符串接口
*/
void TcpConnection::send(const StringPiece& message)
{
    if (state_ == kConnected)
    {
        if (loop_->isInLoopThread())
        { // 当前线程是所属loop线程
            sendInLoop(message);
        }
        else
        { // 当前线程并非所属loop线程
            void (TcpConnection::*fp)(const StringPiece& message);
            fp = &TcpConnection::sendInLoop;
            loop_->runInLoop(
                    std::bind(fp,
                              this, // FIXME
                              message.as_string()));
        }
    }
}

// 转交给sendInLoop(const char*, int)
void TcpConnection::sendInLoop(const StringPiece &message)
{
    sendInLoop(message.data(), message.size());
}

// 转交给sendInLoop(const char*, int)
// FIXME efficiency!!!
void TcpConnection::send(Buffer *buf)
{
    if (state_ == kConnected)
    {
        if (loop_->isInLoopThread())
        {
            // send all readable bytes
            sendInLoop(buf->peek(), buf->readableBytes());
            buf->retrieveAll();
        }
    }
}
```

sendInLoop定义如下，主要是向对端发送一次数据，如果发送完一次，就进行一次回调；如果待发送数据超高水位，就进行高水位回调；如果发生错误，就进行错误回调。

```c++
/**
* 在所属loop线程中, 发送data[len]
* @param data 要发送的缓冲区首地址
* @param len　要发送的缓冲区大小(bytes)
* @details 发生write错误, 如果发送缓冲区未满,　对端已发FIN/RST分节 表明tcp连接发生致命错误(faultError为true)
*/
void TcpConnection::sendInLoop(const char *data, size_t len)
{
    loop_->assertInLoopThread();
    ssize_t nwrote = 0;
    size_t remaining = len;
    bool faultError = false;
    if (state_ == kDisconnected) // 如果已经断开连接(kDisconnected), 就无需发送, 打印log(LOG_WARN)
    {
        LOG_WARN << "disconnected, give up writing";
        return;
    }

    // write一次, 往对端发送数据, 后面再看是否发生错误, 是否需要高水位回调
    // if no thing output queue, try writing directly
    if (!channel_->isWriting() && outputBuffer_.readableBytes() == 0)
    { // 如果通道没有使能监听写事件, 并且outputBuffer　没有待发送数据, 就直接通过socket写
        nwrote = sockets::write(channel_->fd(), data, len);
        if (nwrote >= 0)
        {
            remaining = len - nwrote;
            if (remaining == 0 && writeCompleteCallback_)
            {
                loop_->queueInLoop(std::bind(writeCompleteCallback_, shared_from_this()));
            }
        }
        else // nwrote < 0, error
        {
            nwrote = 0;
            if (errno != EWOULDBLOCK) // EWOULDBLOCK: 发送缓冲区已满, 且fd已设为nonblocking
            { // O_NONBLOCK fd, write block but return EWOULDBLOCK error
                LOG_SYSERR << "TcpConnection::sendInLoop";
                if (errno == EPIPE || errno == ECONNRESET) // FIXME: any others?
                { // EPIPE: reading end is closed; ECONNRESET: connection reset by peer
                    faultError = true;
                }
            }
        }
    }

    // 处理剩余待发送数据
    assert(remaining <= len);
    if (!faultError && remaining > 0) // 没有故障, 并且还有待发送数据, 可能是发送太快, 对方来不及接收
    { // no error and data remaining to be written
        size_t oldLen = outputBuffer_.readableBytes(); // Buffer中待发送数据量

        if (oldLen + remaining >= highWaterMark_ // Buffer及当前要发送的数据量之和 超 高水位(highWaterMark)
        && oldLen < highWaterMark_ // 单独的Buffer中待发送数据量 未超 高水位
        && highWaterMarkCallback_)
        {
            loop_->queueInLoop(std::bind(highWaterMarkCallback_, shared_from_this(), oldLen + remaining));
        }
        // append data to be written to the output buffer
        outputBuffer_.append(static_cast<const char*>(data) + nwrote, remaining);
        // enable write event for channel_
        if (!channel_->isWriting())
        { // 如果没有在监听通道写事件, 就使能通道写事件
            channel_->enableWriting();
        }
    }
}
```

#### TcpConnection回调

#### TcpConnection回调有哪些？

TcpConnection为应用程序提供了几个用于处理连接及数据的回调接口，主要包括 ConnectionCallback（连接建立回调）、MessageCallback（接收到消息回调）、WriteCompleteCallback（写完成回调）、HighWaterMarkCallback（高水位回调）、CloseCallback（连接关闭回调）。

通过以下几个成员实现存储、判断：

```c++
    ConnectionCallback connectionCallback_;       // 连接回调
    MessageCallback messageCallback_;             // 收到消息回调
    WriteCompleteCallback writeCompleteCallback_; // 写完成回调
    HighWaterMarkCallback highWaterMarkCallback_; // 高水位回调
    CloseCallback closeCallback_;                 // 关闭连接回调

    size_t highWaterMark_;                        // 高水位阈值
    Buffer inputBuffer_;                          // 应用层接收缓冲区
    Buffer outputBuffer_; // FIXME: use list<Buffer> as output buffer.　//　应用层发送缓冲区
    boost::any contex_;                           // 绑定一个用户自定义类型的上下文参数
```

connectionCallback_存放用户注册的连接回调， 会在以下几种情形回调：
1）Tcp连接确认建立时：TcpServer/TcpClient在Tcp连接建立时，通过newConenction新建TcpConnection对象时，回调TcpConnection::connectEstablished，进而回调connectionCallback_。
2）Tcp连接确认销毁时：TcpServer/TcpClient在移除Tcp连接时，通过removeConnectionInLoop销毁连接，回调connectionCallback_。
3）强制关闭Tcp连接时：TcpConnection强制销毁时，调用forceClose()/forceClose()，或者从sockfd read到数据量为0（对端关闭连接），进而调用handleClose，回调connectionCallback_。

messageCallback_ 存放用户注册的接收到消息回调，会在Channel调用handleRead处理读事件时，调用cpConnection::handleRead，从sockfd read到数据量>0，进而回调messageCallback_。

writeCompleteCallback_ 存放用户注册的写完成回调，会在数据发送完毕回调函数，所有的用户数据都已拷贝，outputBuffer_被清空也会回调该函数。

highWaterMarkCallback_ 存放用户注册的高水位回调，用户send数据时，会在应用发送缓存堆积数据过多（>highWaterMark_）时调用。

#### 回调注册

注册回调接口：

```c++
    void setConnectionCallback(const ConnectionCallback& cb)
    { connectionCallback_ = cb; }

    void setMessageCallback(const MessageCallback& cb)
    { messageCallback_ = cb; }

    void setWriteCompleteCallback(const WriteCompleteCallback& cb)
    { writeCompleteCallback_ = cb; }

    void setHighWaterMarkCallback(const HighWaterMarkCallback& cb, size_t highWaterMark)
    { highWaterMarkCallback_ = cb; highWaterMark_ = highWaterMark; }
```

#### 何时调用connectionCallback_？

调用connectionCallback_ 分为多种情况。connectionCallback_ 通常不应该为空，如果用户没有特殊设置，应该设为默认的defaultConnectionCallback。

1）Tcp连接建立时

```c++
/**
* TcpServer/TcpClient 调用newTcpConnection创建TcpConnection对象后, 用来做一些连接以建立的事后工作
*/
void TcpConnection::connectEstablished()
{
    loop_->assertInLoopThread();
    assert(state_ == kConnecting);
    setState(kConnected);
    channel_->tie(shared_from_this()); // tie this TcpConnection object to channel_
    channel_->enableReading(); // 使能监听读事件, 连接上有读事件发生时, 如收到对端数据, 会触发channel_::handleRead


    // FIXME: add by Martin
    assert(connectionCallback_ != nullptr); // connectionCallback_通常不允许为空, TcpServer/TcpClient可以设置为缺省值defaultConnectionCallback


    connectionCallback_(shared_from_this()); // 回调connectionCallback_
}
```

2）Tcp连接销毁时，通常由TcpServer/TcpClient主动移除通道事件触发

```c++
/**
* TcpServer/TcpClient移除通道事件时, 调用该函数销毁tcp连接
* @note 只有处于已连接状态(kConnected)的tcp连接, 才需要先更新状态, 关闭通道事件监听
*/
void TcpConnection::connectDestroyed()
{
    loop_->assertInLoopThread();
    if (state_ == kConnected) // 只有kConnected的连接, 才有必要采取断开连接动作
    {
        setState(kDisconnected);
        channel_->disableAll(); // 关闭通道事件监听


        connectionCallback_(shared_from_this()); // 调用连接回调
    }
    channel_->remove(); // 从EventLoop和Poller中移除监听通道事件
}
```

3）对端关闭连接（从sockfd read返回0），本地调用handleClose被动关闭

```c++
/**
* 处理Tcp连接关闭
* @details 更新状态为kDisconnected, 清除所有事件通道监听.
* @note 必须在所属loop线程中运行.
*/
void TcpConnection::handleClose()
{
    loop_->assertInLoopThread(); // 确保在所属loop线程中运行
    LOG_TRACE << "fd = " << channel_->fd() << " state = " << stateToString();
    assert(state_ == kConnected || state_ == kDisconnecting);
    // we don't close fd, leave it to dtor, so we can find leaks easily.
    setState(kDisconnected); // 更新Tcp连接状态
    channel_->disableAll();     // 停止监听所有通道事件(读写事件)

    TcpConnectionPtr guardThis(shared_from_this());
    connectionCallback_(guardThis); // 连接回调
    // must be the last line
    closeCallback_(guardThis);      //　关闭连接回调
}

void TcpConnection::handleRead(Timestamp receiveTime)
{
    loop_->assertInLoopThread();
    int savedErrno = 0;
    ssize_t n = inputBuffer_.readFd(channel_->fd(), &savedErrno); // 从指定fd读取数据到内部缓冲
    if (n > 0)
    {
        messageCallback_(shared_from_this(), &inputBuffer_, receiveTime);
    }
    else if (n == 0)
    {
        handleClose();
    }
    ...
}
```

4）本地主动强制关闭（forceClose）Tcp连接，当然，connectionCallback_回调跟3）一样，还是在handleClose中发生的

```c++
/**
* 在所属loop循环中强制关闭连接, 只对连接为kConnected或kDisconnecting状态才有效
*/
void TcpConnection::forceCloseInLoop()
{
    loop_->assertInLoopThread();
    if (state_ == kConnected || state_ == kDisconnecting)
    {
        // as if we received 0 byte in handleRead()
        handleClose();
    }
}

/**
* 强制关闭连接, 只对连接为kConnected或kDisconnecting状态才有效
* @details 为防止意外, 动作应该放到loop末尾去做
*/
void TcpConnection::forceClose()
{
    // FIXME: use compare and swap
    if (state_ == kConnected || state_ == kDisconnecting)
    {
        setState(kDisconnecting);
        loop_->queueInLoop(std::bind(&TcpConnection::forceCloseInLoop, shared_from_this()));
    }
}
```

#### 何时调用messageCallback_？

前面在handleRead()中已经看到，当从sockfd read返回>0时，就会回调messageCallback_，交给应用层处理从对端接收来的数据（存放到inputBuffer_中）。

```c++
void TcpConnection::handleRead(Timestamp receiveTime)
{
    loop_->assertInLoopThread();
    int savedErrno = 0;
    ssize_t n = inputBuffer_.readFd(channel_->fd(), &savedErrno); // 从指定fd读取数据到内部缓冲
    if (n > 0)
    {
        messageCallback_(shared_from_this(), &inputBuffer_, receiveTime);
    }
    ...
}
```

#### 何时调用writeCompleteCallback_，highWaterMarkCallback_？

TcpConnection::send()发送数据时，会转为在IO线程中调用TcpConnection::sendInLoop。将要发送的数据写到内核缓冲区，然后回调writeCompleteCallback_；如果一次没有写完，剩余数据量较大，超过highWaterMark_（高水位），就回调highWaterMarkCallback_。

调用顺序：
send() => sendInLoop() => writeCompleteCallback_(), highWaterMarkCallback_() => handleWrite()

```c++
void TcpConnection::sendInLoop(const char *data, size_t len)
{
    string s;
    std::reverse(s.begin(), s.end());
    loop_->assertInLoopThread();
    ssize_t nwrote = 0;
    size_t remaining = len;
    bool faultError = false;
    if (state_ == kDisconnected) // 如果已经断开连接(kDisconnected), 就无需发送, 打印log(LOG_WARN)
    {
        LOG_WARN << "disconnected, give up writing";
        return;
    }

    // write一次, 往对端发送数据, 后面再看是否发生错误, 是否需要高水位回调
    // if no thing output queue, try writing directly
    if (!channel_->isWriting() && outputBuffer_.readableBytes() == 0)
    { // 如果通道没有使能监听写事件, 并且outputBuffer　没有待发送数据, 就直接通过socket写
        nwrote = sockets::write(channel_->fd(), data, len);
        if (nwrote >= 0)
        {
            remaining = len - nwrote;
            // 写完了data[len]到到内核缓冲区, 就回调writeCompleteCallback_
            if (remaining == 0 && writeCompleteCallback_)
            {
                loop_->queueInLoop(std::bind(writeCompleteCallback_, shared_from_this()));
            }
        }
        else // nwrote < 0, error
        {
            nwrote = 0;
            if (errno != EWOULDBLOCK) // EWOULDBLOCK: 发送缓冲区已满, 且fd已设为nonblocking
            { // O_NONBLOCK fd, write block but return EWOULDBLOCK error
                LOG_SYSERR << "TcpConnection::sendInLoop";
                if (errno == EPIPE || errno == ECONNRESET) // FIXME: any others?
                { // EPIPE: reading end is closed; ECONNRESET: connection reset by peer
                    faultError = true;
                }
            }
        }
    }

    // 处理剩余待发送数据
    // 没有错误, 并且还有没有写完的数据, 说明内涵发送缓冲区满, 要将未写完的数据添加到output buffer中
    assert(remaining <= len);
    if (!faultError && remaining > 0) // 没有故障, 并且还有待发送数据, 可能是发送太快, 对方来不及接收
    { // no error and data remaining to be written
        size_t oldLen = outputBuffer_.readableBytes(); // Buffer中待发送数据(readable空间数据)

        // 如果readable空间数据 + 未写完数据, 超过highWaterMark_(高水位), 回调highWaterMarkCallback_
        if (oldLen + remaining >= highWaterMark_
        && oldLen < highWaterMark_
        && highWaterMarkCallback_)
        {
            loop_->queueInLoop(std::bind(highWaterMarkCallback_, shared_from_this(), oldLen + remaining));
        }
        // append data to be written to the output buffer
        outputBuffer_.append(static_cast<const char*>(data) + nwrote, remaining);
        // enable write event for channel_
        if (!channel_->isWriting())
        { // 如果没有在监听通道写事件, 就使能通道写事件
            channel_->enableWriting();
        }
    }
}

/**
* 内核发送缓冲区有空间了, 回调该函数.
* @details 在send()之后触发. send()通常由用户主动调用, handleWrite由epoll_wait/poll监听,
* 由EventLoop对应IO线程回调.
*/
void TcpConnection::handleWrite()
{
    loop_->assertInLoopThread();
    if (channel_->isWriting())
    { // 只有通道在监听写事件时, 处理write事件才有意义
        // 继续往内核缓冲区写readable空间的待发送数据
        ssize_t n = sockets::write(channel_->fd(),
                                   outputBuffer_.peek(),
                                   outputBuffer_.readableBytes());
        if (n > 0)
        {
            outputBuffer_.retrieve(n);
            if (outputBuffer_.readableBytes() == 0)
            { // 发送缓冲区已清空
                channel_->disableWriting();
                if (writeCompleteCallback_)
                { // 应用层发送缓冲区已被清空, 就回调writeCompleteCallback_
                    loop_->queueInLoop(std::bind(writeCompleteCallback_, shared_from_this()));
                }
                if (state_ == kDisconnecting)
                { // 发送缓冲区已清空, 并且连接状态是kDisconnecting, 要关闭连接
                    shutDownInLoop(); // 关闭连接 写方向
                }
            }
        }
        else
        {
            LOG_SYSERR << "TcpConnection::handleWrite";
        }
    }
}
```

##### 上下文数据传递

input buffer、output buffer是Tcp连接中，伴随着一次或几次请求、应答的数据，是与对端进行沟通的数据。如果想要一下数据，伴随着整个Tcp连接的声明周期，该怎么办？
可以使用TcpConnection::contex_，在连接建立时（回调onConnection），创建boost::any对象（可变类型），然后让TcpConnection::contex_指向该对象。这样，在整个TcpConnection生命周期内，都可以直接通过contex_访问该对象；直到沟通完成，或者连接关闭时，reset contex_。

```c++
public:
    ...
    void setContext(const boost::any& contex)
    { contex_ = contex; }

    const boost::any& getContext() const
    { return contex_; }

    boost::any* getMutableContext()
    { return &contex_; }
    ...
private:
    boost::any contex_;                           // 用户自定义参数
    ...
```

比如，对于一个HttpSever，可以中连接回调onConnection中存放一个HttpContext对象；在收到对端消息，回调onMessage时，可以获得该HttpContext对象；在确认解析完HTTP请求后，可以reset 重置context上下文。

```c++
void HttpServer::onConnection(const TcpConnectionPtr &conn)
{
    if (conn->connected()) // 确认已处于连接建立状态
    {
        conn->setContext(HttpContext()); // 让contex_ 指向一个HttpContext对象
    }
}

void HttpServer::onMessage(const TcpConnectionPtr &conn, Buffer *buf, Timestamp receivedTime)
{
    // 从TcpConnection获取contex_ 上下文，然后将内容转型为所需的HttpContext类型
    HttpContext* context = boost::any_cast<HttpContext>(conn->getMutableContext()); 

    if (!context->parseRequest(buf, receivedTime))
    { // parse request failure
        conn->send("HTTP/1.1 400 Bad Request\r\n\r\n");
        conn->shutdown();
    }

    if (context->gotAll())
    {
        onRequest(conn, context->request());
        context->reset();
    }
}
```

#### HTTP

客户端发送请求，通过muduo库之后服务端收到的数据存放于Buffer中，之后解析成HttpRequest请求对象，再创建一个HttpResponse响应对象并格式化成Buffer返回给客户端。服务端解析客户端请求的Buffer使用HttpContext类。

1、HttpRequest 类
发送端构造一个HttpRequest，调用成员函数设置请求头、请求体等。调用成员函数请求头字段，使用 const char* start, const char* end 来传递一个字符串。主要是因为基于TCP的请求数据都保存在Buffer中，通过解析Buffer中数据进行传递HTTP报文信息。

class HttpRequest : public muduo::copyable
{
 public:
  enum Method  { kInvalid, kGet, kPost, kHead, kPut, kDelete };  //设计支持的请求类型
  enum Version { kUnknown, kHttp10, kHttp11 };  // HTTP版本

  HttpRequest() : method_(kInvalid), version_(kUnknown) {}  //默认构造函数

  void setVersion(Version v) { version_ = v; }      // 版本
  Version getVersion() const { return version_; }

  bool setMethod(const char* start, const char* end)  // 根据字符串设定请求方法
  {
    assert(method_ == kInvalid);
    string m(start, end);
    if      (m == "GET")  {  method_ = kGet;  }
    else if (m == "POST") {  method_ = kPost; }
    else if (m == "HEAD") {  method_ = kHead; }
    else if (m == "PUT")  {  method_ = kPut;  }
    else if (m == "DELETE"){ method_ = kDelete; }
    else                   { method_ = kInvalid;}
    return method_ != kInvalid;
  }
  Method method() const { return method_; }

  const char* methodString() const  // 请类型字符串
  {
    const char* result = "UNKNOWN";
    switch(method_)
    {
      case kGet:    result = "GET";    break;
      case kPost:   result = "POST";   break;
      case kHead:   result = "HEAD";   break;
      case kPut:    result = "PUT";    break;
      case kDelete: result = "DELETE"; break;
      default: break;
    }
    return result;
  }
  // 请求行的URL
  void setPath(const char* start, const char* end) { path_.assign(start, end); }
  const string& path() const { return path_; }

  void setQuery(const char* start, const char* end) { query_.assign(start, end);  }
  const string& query() const { return query_; }

  void setReceiveTime(Timestamp t) { receiveTime_ = t; }
  Timestamp receiveTime() const  { return receiveTime_; }

  // 请求头的添加键值对
  void addHeader(const char* start, const char* colon, const char* end)
  {
    string field(start, colon);  // 要求冒号前无空格
    ++colon;
    while (colon < end && isspace(*colon)) {  // 过滤冒号后的空格
       ++colon;
    }
    string value(colon, end);
    while (!value.empty() && isspace(value[value.size()-1])){
      value.resize(value.size()-1);
    }
    headers_[field] = value;
  }
  // 请求头部查找键的值
  string getHeader(const string& field) const
  {
    string result;
    std::map<string, string>::const_iterator it = headers_.find(field);
    if (it != headers_.end()){
      result = it->second;
    }
    return result;
  }

  const std::map<string, string>& headers() const { return headers_; }

  void swap(HttpRequest& that)  // 交换
  {
    std::swap(method_, that.method_);
    std::swap(version_, that.version_);
    path_.swap(that.path_);
    query_.swap(that.query_);
    receiveTime_.swap(that.receiveTime_);
    headers_.swap(that.headers_);
  }

 private:
  Method method_;						// 请求行 - 请求方法
  Version version_;						// 请求行 - HTTP版本
  string path_;							// 请求行 - URL
  string query_;						// 请求体
  Timestamp receiveTime_;				// 请求事件
  std::map<string, string> headers_;	// 请求头部
};

添加请求头键值对的字符串中，冒号前不能有空格，例如"key:value"或"key: value"是正确的，"key :value"就是错误的。

请求行中的URL中可能带有请求参数，以问号"?"分割，后面的请求参数使用键值对方式，并作为请求体保存。

2、HttpResponse 类
主要用于构造一个HttpResponse，调用成员函数设置响应头部、响应体，调用appendToBuffer()格式化到Buffer中，回复给客户端。

class HttpResponse : public muduo::copyable
{
 public:
  enum HttpStatusCode{
    kUnknown,
    k200Ok = 200,					// 正常
    k301MovedPermanently = 301,	    // 资源不可访问，重定向
    k400BadRequest = 400,			// 请求错误（域名不存在、请求不正确）
    k404NotFound = 404,				// 通常是URL不正确（或者因为服务不再提供）
  };

  explicit HttpResponse(bool close) : statusCode_(kUnknown), closeConnection_(close){}

  void setStatusCode(HttpStatusCode code) { statusCode_ = code; }

  void setStatusMessage(const string& message) { statusMessage_ = message; }

  void setCloseConnection(bool on) { closeConnection_ = on; }

  bool closeConnection() const { return closeConnection_; }

  void setContentType(const string& contentType) { addHeader("Content-Type", contentType); }

  // FIXME: replace string with StringPiece
  void addHeader(const string& key, const string& value) { headers_[key] = value; }

  void setBody(const string& body) { body_ = body; }

  void appendToBuffer(Buffer* output) const;  // 将整个HttpRespose对象按照协议输出到Buffer中

 private:
  std::map<string, string> headers_;   	// 响应头部，键值对
  HttpStatusCode statusCode_;		   	// 响应行 - 状态码
  // FIXME: add http version
  string statusMessage_;				// 响应行 - 状态码文字描述
  bool closeConnection_;				// 是否关闭连接
  string body_;  						// 响应体
};

函数HttpResponse::appendToBuffer(Buffer* output)默认使用HTTP1.1版本，按照HTTP协议对HttpResponse对象进行格式化输出到Buffer中。

void HttpResponse::appendToBuffer(Buffer* output) const
{
  char buf[32];
  // 响应行
  snprintf(buf, sizeof buf, "HTTP/1.1 %d ", statusCode_);
  output->append(buf);
  output->append(statusMessage_);
  output->append("\r\n");

  // 响应头部
  if (closeConnection_){
    output->append("Connection: close\r\n");
  } else{
    snprintf(buf, sizeof buf, "Content-Length: %zd\r\n", body_.size());
    output->append(buf);
    output->append("Connection: Keep-Alive\r\n");
  }

  for (const auto& header : headers_){
    output->append(header.first);
    output->append(": ");
    output->append(header.second);
    output->append("\r\n");
  }

  output->append("\r\n");  // 空行
  output->append(body_);   // 响应体
}

注意添加请求头时，需要删除键值对的字符串左侧和右侧的空字符，保证解析正常。因为解析请求头时，对一行字符串用冒号“:”进行分割解析。

3、HttpContext 类
服务端接收客户请求通过HttpContext解析，解析后数据封装到HttpRequest中。

class HttpContext : public muduo::copyable
{
 public:
  enum HttpRequestParseState
  {
    kExpectRequestLine,		// 请求行
    kExpectHeaders,			// 请求头
    kExpectBody,		    // 请求体
    kGotAll,
  };
  // 构造函数，默认从请求行开始解析
  HttpContext() : state_(kExpectRequestLine) { }

  // default copy-ctor, dtor and assignment are fine

  // return false if any error
  bool parseRequest(Buffer* buf, Timestamp receiveTime); // 解析请求Buffer

  bool gotAll() const { return state_ == kGotAll; }

  void reset()  // 为了复用HttpContext
  {
    state_ = kExpectRequestLine;
    HttpRequest dummy;
    request_.swap(dummy);
  }

  const HttpRequest& request() const { return request_; }
  HttpRequest& request() { return request_; }

 private:
  bool processRequestLine(const char* begin, const char* end);

  HttpRequestParseState state_; // 解析状态
  HttpRequest request_;
};

主要关注如何解析Buffer中的文本（请求行、请求头）部分、请求体（可能是二进制或其他文本格式）。一个正常的请求，一般至少是有请求行的，默认解析状态为kExpectRequestLine。

3.1、请求解析 parseRequest()
函数原型为bool HttpContext::parseRequest(Buffer* buf, Timestamp receiveTime)，传入需要解析的Buffer对象和接收时间，根据期望解析的部分进行处理。

bool HttpContext::parseRequest(Buffer* buf, Timestamp receiveTime)
{
  bool ok = true;
  bool hasMore = true;
  while (hasMore)
  {
    if (state_ == kExpectRequestLine)  // 解析请求行
    { // 查找Buffer中第一次出现“\r\n”的位置
      const char* crlf = buf->findCRLF();
    if (crlf)
      { // 若找到“\r\n”，说明至少有一行数据，进行实际解析
        // buf->peek()为数据开始部分，crlf为结束部分
        ok = processRequestLine(buf->peek(), crlf);
        if (ok){ // 解析成功
          request_.setReceiveTime(receiveTime);
          buf->retrieveUntil(crlf + 2); // buffer->peek()向后移2字节，到下一行
          state_ = kExpectHeaders;
        }
        else { hasMore = false;}
      }
      else {  hasMore = false; }
    }
    else if (state_ == kExpectHeaders)  // 解析请求头
    {
      const char* crlf = buf->findCRLF(); // 找到“\r\n”位置
      if (crlf)
      {
        const char* colon = std::find(buf->peek(), crlf, ':'); // 定位分隔符
        if (colon != crlf){
          request_.addHeader(buf->peek(), colon, crlf); // 键值对解析
        }
        else{
          // empty line, end of header
          // FIXME:
          state_ = kGotAll;
          hasMore = false;
        }
        buf->retrieveUntil(crlf + 2); // 后移2个字符
      }
      else{ hasMore = false; }
    }
    else if (state_ == kExpectBody)  // 解析请求体，未实现
    {
      // FIXME:
    }
  }
  return ok;
}

请求体解析，可以根据请求体协议的定义，两个CRLF后（最后一个请求体的CRLF，空行的CRLF）的数据就是请求体部分。修改部分代码如下

    else if (state_ == kExpectHeaders)
    {
      const char* crlf = buf->findCRLF();
      if (crlf){
        const char* colon = std::find(buf->peek(), crlf, ':');
        if (colon != crlf){
          request_.addHeader(buf->peek(), colon, crlf);
        }
        else{
          // empty line, end of header
          state_ = kExpectBody;  //继续查找后续数据
        }
        buf->retrieveUntil(crlf + 2);
      }
      else{
        hasMore = false;
      }
    }
    else if (state_ == kExpectBody){
      if(buf->readableBytes()){  //如果还有数据，就是请求体
        request_.setQuery(buf->peek(), buf->beginWrite());
      }
      state_ = kGotAll;
      hasMore = false;
    }

3.1、请求行的解析 processRequestLine()
请求行有固定格式Method URL Version CRLF，URL中可能带有请求参数。根据空格符进行分割成三段字符。URL可能带有请求参数，使用"?”分割解析。

bool HttpContext::processRequestLine(const char* begin, const char* end)
{
  bool succeed = false;
  const char* start = begin;
  const char* space = std::find(start, end, ' ');
  // 第一个空格前的字符串，请求方法
  if (space != end && request_.setMethod(start, space))
  {
    start = space+1;
    space = std::find(start, end, ' ');
    if (space != end)
    {  // 第二个空格前的字符串，URL
      const char* question = std::find(start, space, '?');
      if (question != space){  // 如果有"?"，分割成path和请求参数
        request_.setPath(start, question);
        request_.setQuery(question, space);
      }
      else{
        request_.setPath(start, space); // 仅path
      }
      start = space+1;
      // 最后一部分，解析HTTP协议
      succeed = end-start == 8 && std::equal(start, end-1, "HTTP/1.");
      if (succeed)
      {
        if (*(end-1) == '1'){  // 1.1版本
          request_.setVersion(HttpRequest::kHttp11);
        }
        else if (*(end-1) == '0'){  // 1.0版本
          request_.setVersion(HttpRequest::kHttp10);
        }
        else{
          succeed = false;
        }
      }
    }
  }
  return succeed;
}

## 系统测试

### 测试环境

### 服务器单元测试

### 服务器集成测试

### 系统性能测试

### 测试结果分析

## 总结

### 全文总结

### 未来工作展望

## 致谢

## 参考文献

[1] Nancy J. Yeager; Robert E. McGrath (1996). *Web Server Technology*. ISBN "ISBN (identifier)" 1-55860-376-X. Archived from the original on 20 January 2023. Retrieved 22 January 2021.

[2] [字节跳动在 Go 网络库上的实践](https://www.cloudwego.io/zh/blog/2020/05/24/%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8%E5%9C%A8-go-%E7%BD%91%E7%BB%9C%E5%BA%93%E4%B8%8A%E7%9A%84%E5%AE%9E%E8%B7%B5/)

[3] [Netty Project. Netty Project Community. [2019-01-31].](https://web.archive.org/web/20190130065314/https://netty.io/)

[4] Linux 多线程服务端编程（使用muduo C++网络库）.陈硕 .电子工业出版社 .2021.04

[5] Silberschatz, Abraham; Galvin, Peter Baer; Gagne, Greg. Operating System Concepts. Hoboken, NJ: John Wiley & Sons. 2008. ISBN 978-0-470-12872-5

[6] [RFC 793](https://datatracker.ietf.org/doc/html/rfc793)

[7] [什么是事件驱动的架构？](https://www.ibm.com/cn-zh/topics/event-driven-architecture)

[8] [如何深刻理解Reactor和Proactor？ - 小林coding的回答 - 知乎](https://www.zhihu.com/question/26943938/answer/1856426252)

[9] [Efficient IO with io_uring](https://kernel.dk/io_uring.pdf)

[10] [INTERACTION WITH OTHER PROGRAMS, LIBRARIES OR THE ENVIRONMENT](http://pod.tst.eu/http://cvs.schmorp.de/libev/ev.pod#THREADS_AND_COROUTINES)

[11] [Java线程池实现原理及其在美团业务中的实践](https://tech.meituan.com/2020/04/02/java-pooling-pratice-in-meituan.html)

[12] [eventfd(2) — Linux manual page](https://man7.org/linux/man-pages/man2/eventfd.2.html)

[13] [select(2) — Linux manual page](https://man7.org/linux/man-pages/man2/select.2.html)

[14] [poll(2) — Linux manual page](https://man7.org/linux/man-pages/man2/poll.2.html)

[15] [epoll(7) — Linux manual page](https://man7.org/linux/man-pages/man7/epoll.7.html)

[16] Linux 高性能服务器编程 .游双 .机械工业出版社 .2013.06

[17] [timerfd_create(2) — Linux manual page](https://man7.org/linux/man-pages/man2/timerfd_create.2.html)

[18] 周怡，孟实，林雷主编,计算机网络基础实验指导,浙江工商大学出版社,2018.09,第150页

[19] UNIX 网络编程 . W.理查德·史蒂文斯（W.，Richard，Stevens）.杨继张（译者） .清华大学出版社 .2006.01

[20] [epoll(7) — Linux manual page](https://man7.org/linux/man-pages/man7/epoll.7.html)

[21] 'Efficient Variable Automatic Buffers', Matthew Wilson, C/C++ User's Journal, December 2003
